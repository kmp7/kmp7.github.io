<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>kmp+</title>
    <style>
        /* Основные стили */
        :root {
            --primary-color: #325980;
            --secondary-color: #4CAF50;
            --background-color: #f5f5f5;
            --content-bg: #ffffff;
            --text-color: #333333;
            --header-text-color: #ffffff;
            --menu-bg: #ffffff;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            --border-radius: 8px;
			  --accent11: #4caf50;
		--accent12: #4cafff;
		--accent13: #ffaf50;
		--accent14: #821978;
        }

        /* Темная тема */
        [data-theme="dark"] {
            --primary-color: #3e76ad;
            --secondary-color: #388e3c;
            --background-color: #000000;
            --content-bg: #1e1e1e;
            --text-color: #e0e0e0;
            --header-text-color: #ffffff;
            --menu-bg: #000000;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            transition: background-color 0.3s, color 0.3s;
        }

        body {
            font-family: 'Roboto', 'Arial', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--background-color);
            padding-top: 2px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        header {
            background-color: var(--primary-color);
            color: var(--header-text-color);
            padding: 20px 0;
            text-align: center;
            border-radius: var(--border-radius);
            margin-bottom: 2px;
        }

        h1 {
            font-size: 2.2rem;
            margin-bottom: 10px;
        }

        h2 {
            color: var(--primary-color);
            margin: 25px 0 15px;
            padding-bottom: 8px;
            border-bottom: 2px solid var(--secondary-color);
        }

        h3 {
            color: var(--primary-color);
            margin: 20px 0 10px;
        }

        p {
            margin-bottom: 15px;
        }

        /* Меню навигации */
        .menu {
            background-color: var(--menu-bg);
            padding: 15px 20px;
            border-radius: var(--border-radius);
            margin-bottom: 30px;
            box-shadow: var(--menu-shadow);
            display: flex;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .menu-buttons {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }
		
		
		.menu-buttons {
    display: flex;
    flex-wrap: wrap;
    gap: 10px;
    justify-content: center; /* Изменено на center */
}

        .menu-btn {
            background-color: var(--primary-color);
            color: white;
            border: none;
            padding: 8px 15px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: background-color 0.3s;
        }

        .menu-btn:hover {
            background-color: var(--secondary-color);
        }

        .theme-toggle {
            background: none;
            border: 10px solid transparent;
            font-size: 1.5rem;
            cursor: pointer;
            color: var(--primary-color);
        }

        /* Секции контента */
        .section {
            background-color: var(--content-bg);
            border-radius: var(--border-radius);
            padding: 25px;
            margin-bottom: 30px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        /* Выделение важного */
        .important {
            background-color: rgba(76, 175, 80, 0.1);
            border-left: 4px solid var(--secondary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        .note {
            background-color: rgba(50, 89, 128, 0.1);
            border-left: 4px solid var(--primary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        /* Таблицы */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            padding: 12px 15px;
            border: 1px solid #ddd;
            text-align: left;
        }

        th {
            background-color: var(--primary-color);
            color: white;
        }

        tr:nth-child(even) {
            background-color: rgba(0, 0, 0, 0.03);
        }

        /* Списки */
        ul, ol {
            padding-left: 25px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
        }

        /* Адаптивный дизайн */
        @media (max-width: 768px) {
            h1 {
                font-size: 1.8rem;
            }
            
            h2 {
                font-size: 1.5rem;
            }
            
            .menu {
                flex-direction: column;
                gap: 15px;
            }
            
            .menu-buttons {
                width: 100%;
                justify-content: center;
            }
            
            .theme-toggle {
                margin-top: 10px;
            }
            
            .section {
                padding: 15px;
            }
        }

        /* Анимации */
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .fade-in {
            animation: fadeIn 0.5s ease-in;
        }

        footer {
            text-align: center;
            padding: 20px 0;
            margin-top: 40px;
            font-size: 0.9rem;
        }
		
					.kmp11, .example {
      background: rgba(76, 175, 80, 0.1);
      border-left: 4px solid var(--accent11);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp12, .example {
      background: rgba(95, 182, 237, 0.1);
      border-left: 4px solid var(--accent12);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
 
		.kmp13, .example {
      background: rgba(205, 170, 110, 0.1);
      border-left: 4px solid var(--accent13);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp14, .example {
      background: rgba(205, 110, 200, 0.1);
      border-left: 4px solid var(--accent14);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
	
	
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Современная компьютерная лингвистика</h1>
            <p>как новая модель (карта) реальности (территории) языка</p>
        </header>

        <nav class="menu">
            <div class="menu-buttons">
                <button class="menu-btn" onclick="scrollToSection('1')">R vs. M</button>
                <button class="menu-btn" onclick="scrollToSection('2')">Tradition</button>
                <button class="menu-btn" onclick="scrollToSection('3')">AI First</button>
                <button class="menu-btn" onclick="scrollToSection('4')">CCPS</button>
				<button class="menu-btn" onclick="scrollToSection('5')">LLM</button>
				<button class="menu-btn" onclick="scrollToSection('6')">Addit</button>
				<button class="menu-btn" onclick="scrollToSection('7')">Summary</button>
                 </div>
            <button class="theme-toggle" id="themeToggle" title="Переключить тему">☀️</button>
        </nav>

<!-- ============================== РАЗДЕЛ 1 ============================== -->
<section id="prehistory" class="section">
  <h2 class="section-title">1. Предыстория: докомпьютерная эпоха</h2>

  <div class="pre-ideas">
    <h3 class="pre-ideas-title">1.1. Идея универсального языка и автоматического перевода</h3>
    <div class="pre-ideas-card">
      <p>Задолго до появления компьютеров люди стремились преодолеть языковой барьер. Библейский миф о <strong>Вавилонской башне</strong> зафиксировал в культуре саму проблему: множественность языков как препятствие для понимания. На протяжении столетий мыслители предлагали решения — от искусственных языков до механических устройств для перевода.</p>

      <p><strong>Ключевые идеи и персоналии:</strong></p>
      <ul>
        <li><strong>Раймунд Луллий</strong> (ок. 1232–1316) — каталонский философ, создатель <em>Ars Magna</em> («Великого искусства»): системы вращающихся концентрических кругов с символами, позволяющей комбинаторно порождать высказывания. Это одна из первых попыток <em>формализовать</em> порождение осмысленных выражений механическим путём.</li>
        <li><strong>Рене Декарт</strong> (1596–1650) — в письме к Мерсенну (1629) высказал идею <em>универсального языка</em>, в котором простые идеи обозначались бы элементарными символами, а сложные — их комбинациями, что сделало бы перевод тривиальным.</li>
        <li><strong>Готфрид Вильгельм Лейбниц</strong> (1646–1716) — развил идею <em>characteristica universalis</em>: формального языка, способного точно выражать любые понятия, и <em>calculus ratiocinator</em> — «вычислительной машины» для логических рассуждений на этом языке. Мечта Лейбница — по сути, прообраз подхода «интерлингва» в машинном переводе.</li>
        <li><strong>Людвик Заменгоф</strong> (1859–1917) — создатель <em>эсперанто</em> (1887), наиболее успешного искусственного международного языка. Эсперанто — практическая попытка решить проблему «Вавилонской башни» не через автоматизацию, а через лингвистический дизайн.</li>
      </ul>
    </div>
  </div>

  <div class="pre-mech">
    <h3 class="pre-mech-title">1.2. Первые механические и формальные концепции перевода</h3>
    <div class="pre-mech-card">
      <p>В XX веке идеи автоматического перевода начали оформляться как инженерные проекты:</p>
      <ul>
        <li><strong>Патенты 1930-х годов.</strong> Независимо друг от друга француз <strong>Жорж Арцруни</strong> (1933) и советский инженер <strong>Пётр Смирнов-Троянский</strong> (1933) получили патенты на устройства для механизированного перевода. Арцруни предложил «механический мозг» на основе бумажной ленты, а Смирнов-Троянский — систему, в которой оператор анализировал грамматику входного текста, машина заменяла слова по словарю, а второй оператор синтезировал текст на выходном языке. Примечательно, что Смирнов-Троянский фактически предвосхитил <em>трёхфазную архитектуру</em> (анализ → трансфер → синтез), которая легла в основу RBMT спустя десятилетия.</li>
        <li><strong>Меморандум Уоррена Уивера</strong> (1949). Американский математик и администратор науки Уоррен Уивер написал знаменитый меморандум «Translation», в котором предложил подойти к задаче перевода методами <em>криптографии</em> и <em>теории информации</em>. Ключевая метафора Уивера: «Когда я смотрю на текст на русском языке, я говорю себе: на самом деле это написано по-английски, но закодировано странными символами. Я сейчас приступлю к расшифровке». Этот меморандум считается отправной точкой для исследований машинного перевода как научной и инженерной дисциплины.</li>
      </ul>
    </div>
  </div>

  <div class="pre-legacy">
    <h3 class="pre-legacy-title">1.3. Наследие докомпьютерной эпохи</h3>
    <div class="pre-legacy-card">
      <p>Докомпьютерные идеи сформировали два противоположных полюса ожиданий, между которыми развивается машинный перевод до сих пор:</p>
      <ul>
        <li><strong>Полюс «точности»</strong> — стремление к безошибочному, детерминированному преобразованию (Лейбниц, формальные языки).</li>
        <li><strong>Полюс «универсальности»</strong> — стремление к переводу между любыми языками, в любой области (Декарт, эсперанто, Уивер).</li>
      </ul>
      <p>Также были заложены метафоры, повлиявшие на проектирование систем: перевод как <em>дешифровка</em> (→ статистические модели), перевод через <em>язык-посредник</em> (→ интерлингва), перевод как <em>механическая замена</em> (→ прямые словарные системы).</p>
    </div>
  </div>

  <div class="table-kmp">
    <table class="table">
      <thead>
        <tr>
          <th>Дата / Период</th>
          <th>Персоналия / Событие</th>
          <th>Ключевая идея</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>ок.&nbsp;1305</td>
          <td>Раймунд Луллий — <em>Ars Magna</em></td>
          <td>Комбинаторное порождение высказываний</td>
        </tr>
        <tr>
          <td>1629</td>
          <td>Рене Декарт — письмо к Мерсенну</td>
          <td>Универсальный язык простых идей</td>
        </tr>
        <tr>
          <td>1679</td>
          <td>Г.&nbsp;В.&nbsp;Лейбниц — <em>characteristica universalis</em></td>
          <td>Формальный язык + исчисление рассуждений</td>
        </tr>
        <tr>
          <td>1887</td>
          <td>Л.&nbsp;Заменгоф — эсперанто</td>
          <td>Международный искусственный язык</td>
        </tr>
        <tr>
          <td>1933</td>
          <td>Ж.&nbsp;Арцруни, П.&nbsp;Смирнов-Троянский</td>
          <td>Патенты на механизированный перевод</td>
        </tr>
        <tr>
          <td>1949</td>
          <td>У.&nbsp;Уивер — меморандум «Translation»</td>
          <td>Перевод как задача дешифровки</td>
        </tr>
      </tbody>
    </table>
  </div>

  <div class="kmp12"><strong>Важно:</strong> Меморандум Уивера (1949) — общепринятая точка отсчёта истории машинного перевода как научной дисциплины. Именно после него начались систематические исследования и государственное финансирование.</div>

  <a target="_blank" href="https://aclanthology.org/1955.earlymt-1.1/" class="link-kmp1">Hutchins J. Machine Translation: A Brief History (ACL Anthology)</a>
</section>


<!-- ============================== РАЗДЕЛ 2 ============================== -->
<section id="rbmt" class="section">
  <h2 class="section-title">2. Rule-Based Machine Translation (RBMT): правило-ориентированные системы</h2>

  <div class="rbmt-principles">
    <h3 class="rbmt-principles-title">2.1. Основные принципы</h3>
    <div class="rbmt-principles-card">
      <p>RBMT — парадигма машинного перевода, в которой перевод осуществляется на основе <strong>лингвистических правил</strong>, <strong>двуязычных словарей</strong> и <strong>формальных грамматик</strong>, вручную написанных экспертами-лингвистами. Система анализирует входной текст по правилам, формирует промежуточное представление и генерирует текст на целевом языке.</p>
      <p>Основное допущение RBMT: язык подчиняется формализуемым правилам, и качественный перевод можно получить, описав эти правила достаточно полно.</p>
    </div>
  </div>

  <div class="rbmt-approaches">
    <h3 class="rbmt-approaches-title">2.2. Три подхода в рамках RBMT</h3>
    <div class="rbmt-approaches-card">
      <p>Классическая типология RBMT описывается <strong>треугольником Вокуа</strong> (Bernard Vauquois, 1968), который показывает соотношение между глубиной анализа и простотой трансфера:</p>
      <ul>
        <li><strong>Прямой перевод (Direct Translation).</strong> Минимальный анализ: система заменяет слова исходного текста на слова целевого языка по словарю с минимальными морфологическими корректировками. Пример: ранние системы 1950-х годов. Проблема: крайне низкое качество на сколько-нибудь сложных текстах.</li>
        <li><strong>Трансферный подход (Transfer-Based).</strong> Три фазы: (1) анализ исходного текста → его структурное представление; (2) трансфер — преобразование структур исходного языка в структуры целевого; (3) генерация — синтез текста на целевом языке. Требует набора <em>трансферных правил</em> для каждой пары языков. Пример: SYSTRAN (ранние версии), METAL.</li>
        <li><strong>Интерлингвальный подход (Interlingua-Based).</strong> Самый глубокий анализ: исходный текст преобразуется в <em>универсальное языконезависимое представление</em> (интерлингву), из которого генерируется текст на любом целевом языке. Теоретически, для N языков нужно только N анализаторов и N генераторов (вместо N×(N−1) трансферных модулей). Проблема: построение полноценной интерлингвы — нерешённая задача, так как языки концептуализируют мир по-разному.</li>
      </ul>
      <p><strong>Пояснение:</strong> Треугольник Вокуа иллюстрирует фундаментальный компромисс: чем глубже анализ (от прямой замены к интерлингве), тем сложнее анализ и синтез, но тем проще трансфер между языками.</p>
    </div>
  </div>

  <div class="rbmt-history">
    <h3 class="rbmt-history-title">2.3. Ключевые вехи</h3>
    <div class="rbmt-history-card">
      <ul>
        <li><strong>1954 — Джорджтаунский эксперимент.</strong> Совместная демонстрация Джорджтаунского университета и IBM: компьютер IBM 701 перевёл 60 тщательно отобранных русских предложений на английский, используя 250 слов словаря и 6 грамматических правил. Хотя система была примитивной и подготовленной под демонстрацию, результат произвёл огромное впечатление на публику и политиков. Последовала волна государственного финансирования (в первую очередь — в контексте Холодной войны и необходимости перевода советской научно-технической документации).</li>
        <li><strong>1960 — Критика Бар-Хиллела.</strong> Израильский логик и лингвист Иехошуа Бар-Хиллел опубликовал доклад, в котором доказывал, что <em>полностью автоматический высококачественный перевод</em> (Fully Automatic High-Quality Translation — FAHQT) невозможен в обозримом будущем, поскольку для разрешения многозначности необходимы <em>знания о мире</em>, а не только лингвистические правила. Бар-Хиллел рекомендовал сосредоточиться на системах с участием человека.</li>
        <li><strong>1966 — Отчёт ALPAC.</strong> Комитет ALPAC (Automatic Language Processing Advisory Committee) при Национальной академии наук США заключил, что машинный перевод не оправдывает затраченных на него средств, а результаты далеки от практической пользы. Финансирование MT в США было <em>резко сокращено</em> на более чем десятилетие. Это событие известно как «зима машинного перевода».</li>
        <li><strong>1968 — SYSTRAN.</strong> Петер Тома основал компанию Systran, которая создала одну из самых долгоживущих RBMT-систем. SYSTRAN использовалась ВВС США для перевода с русского языка, а позже — Европейской комиссией. В дальнейшем система была гибридизирована (добавлены статистические и нейронные компоненты).</li>
        <li><strong>1976–1980-е — METEO и Eurotra.</strong> Канадская система METEO успешно переводила метеосводки (англ. → фр.) — пример RBMT, идеально работающей в <em>узком домене</em>. Проект Eurotra Европейского сообщества (1982–1992) — амбициозная попытка создать мультиязычную трансферную систему для всех официальных языков ЕС.</li>
      </ul>
    </div>
  </div>

  <div class="rbmt-eval">
    <h3 class="rbmt-eval-title">2.4. Сильные стороны и ограничения</h3>
    <div class="rbmt-eval-card">
      <p><strong>Сильные стороны:</strong></p>
      <ul>
        <li><strong>Контролируемость и объяснимость.</strong> Каждое решение системы определяется конкретным правилом — ошибку можно отследить и исправить.</li>
        <li><strong>Предсказуемость.</strong> Одинаковый вход всегда даёт одинаковый выход (детерминированность).</li>
        <li><strong>Высокое качество в узких доменах</strong> с ограниченной лексикой и простым синтаксисом (например, метеосводки, технические инструкции).</li>
        <li><strong>Не нужны большие корпуса</strong> параллельных текстов.</li>
      </ul>
      <p><strong>Ограничения:</strong></p>
      <ul>
        <li><strong>Тяжёлая масштабируемость.</strong> Добавление нового языка или домена требует написания тысяч правил вручную.</li>
        <li><strong>Дорогое создание и поддержание.</strong> Нужны команды квалифицированных лингвистов для каждой языковой пары.</li>
        <li><strong>Хрупкость.</strong> Любой вход, не предусмотренный правилами, ведёт к сбою (негладкая деградация).</li>
        <li><strong>Проблема многозначности.</strong> Без «знаний о мире» правила плохо справляются с лексической и структурной неоднозначностью.</li>
        <li><strong>Негибкость стиля.</strong> Системы не адаптируются к стилю, регистру и прагматике текста.</li>
      </ul>
    </div>
  </div>

  <div class="kmp11"><strong>Примечание:</strong> Отчёт ALPAC (1966) часто воспринимается как «приговор» раннему MT, но критики указывают, что комитет оценивал завышенные ожидания, а не реальный потенциал технологии. Тем не менее последствия отчёта — сокращение финансирования — затормозили развитие MT в США на 10–15 лет.</div>

  <div class="kmp14"><strong>Пояснение:</strong> Термин «RBMT» иногда употребляется как синоним «символического МТ» (Symbolic MT) — в противоположность «эмпирическим» подходам (SMT, NMT), основанным на обучении из данных.</div>

  <a target="_blank" href="https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment" class="link-kmp1">Georgetown–IBM experiment (Wikipedia)</a>
  <a target="_blank" href="https://en.wikipedia.org/wiki/ALPAC" class="link-kmp1">ALPAC Report (Wikipedia)</a>
</section>


<!-- ============================== РАЗДЕЛ 3 ============================== -->
<section id="smt" class="section">
  <h2 class="section-title">3. Statistical Machine Translation (SMT): статистические системы</h2>

  <div class="smt-principles">
    <h3 class="smt-principles-title">3.1. Основные принципы</h3>
    <div class="smt-principles-card">
      <p>SMT — парадигма, в которой перевод рассматривается как <strong>вероятностная задача</strong>: система ищет наиболее вероятный перевод на основе статистических закономерностей, извлечённых из больших объёмов параллельных текстов (корпусов «исходный текст — перевод»).</p>
      <p><strong>Формальная постановка задачи</strong> (модель зашумлённого канала, вслед за метафорой Уивера):</p>
      <p>Для исходного предложения <em>f</em> (на языке-источнике) система ищет перевод <em>e</em> (на целевом языке), максимизирующий:</p>
      <p style="text-align:center;"><em>ê = argmax<sub>e</sub> P(e | f) = argmax<sub>e</sub> P(f | e) × P(e)</em></p>
      <ul>
        <li><strong>P(f | e)</strong> — <em>модель перевода</em> (translation model): вероятность того, что предложение <em>f</em> является «зашифрованной» версией предложения <em>e</em>. Обучается на параллельных корпусах.</li>
        <li><strong>P(e)</strong> — <em>языковая модель</em> (language model): вероятность предложения <em>e</em> на целевом языке. Обучается на одноязычных корпусах и отвечает за грамматическую и стилистическую естественность перевода.</li>
      </ul>
      <p><strong>Пояснение:</strong> Разделение на модель перевода и языковую модель — ключевое решение SMT. Языковая модель «подтягивает» перевод к естественности, а модель перевода обеспечивает соответствие смыслу оригинала.</p>
    </div>
  </div>

  <div class="smt-evolution">
    <h3 class="smt-evolution-title">3.2. Эволюция: от слов к фразам</h3>
    <div class="smt-evolution-card">
      <p><strong>Пословный SMT (Word-Based SMT).</strong> Первые модели (IBM Models 1–5, Brown et al., 1990–1993) работали на уровне <em>отдельных слов</em>: для каждого слова исходного предложения вычислялась вероятность его перевода в каждое слово целевого предложения (word alignment). Модели обучались с использованием алгоритма EM (Expectation-Maximization) — без явной разметки выравнивания.</p>
      <p><strong>Фразовый SMT (Phrase-Based SMT).</strong> Прорыв 2000-х годов: единицей перевода стала не отдельная пара слов, а <em>фраза</em> (непрерывная последовательность слов). Фразовые таблицы извлекались из данных выравнивания. Это позволило учитывать локальный контекст и идиоматику, значительно улучшив качество.</p>
      <p><strong>Иерархический / синтаксический SMT.</strong> Дальнейшее развитие: использование иерархических фразовых правил (Chiang, 2005) и синтаксических деревьев для лучшего моделирования перестановок слов (особенно важно для пар языков с разным порядком слов, например, английский — японский).</p>
    </div>
  </div>

  <div class="smt-systems">
    <h3 class="smt-systems-title">3.3. Ключевые системы, корпуса и метрики</h3>
    <div class="smt-systems-card">
      <ul>
        <li><strong>Moses</strong> (Koehn et al., 2007) — открытая платформа для фразового SMT, ставшая стандартом для исследований. Позволяла обучить систему перевода из параллельного корпуса «из коробки».</li>
        <li><strong>Google Translate</strong> (запущен в 2006 году на базе SMT) — первая по-настоящему массовая система машинного перевода, покрывающая десятки языков.</li>
        <li><strong>Europarl</strong> — параллельный корпус, построенный на стенограммах заседаний Европарламента (11 языков, ~60 млн слов на язык). Один из ключевых ресурсов для обучения SMT.</li>
        <li><strong>BLEU</strong> (Bilingual Evaluation Understudy, Papineni et al., 2002) — автоматическая метрика оценки качества перевода, основанная на подсчёте совпадающих n-грамм между машинным переводом и эталонным переводом. BLEU стала <em>стандартной метрикой</em> в эпоху SMT (и за её пределами), несмотря на известные ограничения (нечувствительность к смыслу, порядку аргументов и т.д.).</li>
      </ul>
    </div>
  </div>

  <div class="smt-eval">
    <h3 class="smt-eval-title">3.4. Сильные стороны и ограничения</h3>
    <div class="smt-eval-card">
      <p><strong>Сильные стороны:</strong></p>
      <ul>
        <li><strong>Автоматическое обучение из данных.</strong> Не нужны команды лингвистов — система извлекает закономерности из корпуса.</li>
        <li><strong>Масштабируемость.</strong> Добавление нового языка требует корпуса, а не грамматики.</li>
        <li><strong>Значительный прирост качества</strong> для ресурсных языков (языков с большим объёмом параллельных данных).</li>
        <li><strong>Гибкость к домену:</strong> переобучение на специализированном корпусе адаптирует систему.</li>
      </ul>
      <p><strong>Ограничения:</strong></p>
      <ul>
        <li><strong>Зависимость от качества и объёма корпусов.</strong> Для малоресурсных языков и доменов качество резко падает.</li>
        <li><strong>Ограниченный контекст.</strong> Фразовые модели учитывают локальный контекст (несколько слов), но не могут моделировать долгосрочные зависимости и дискурсивную связность.</li>
        <li><strong>Негладкая флуентность.</strong> Переводы часто звучат «склеенными» из фрагментов, с нарушениями согласования.</li>
        <li><strong>Инженерная сложность.</strong> Pipelines SMT включают множество компонентов (выравнивание, извлечение фраз, рекомбинация, тюнинг весов), каждый из которых вносит ошибки.</li>
      </ul>
    </div>
  </div>

  <div class="table-kmp">
    <table class="table">
      <thead>
        <tr>
          <th>Дата</th>
          <th>Событие</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>1990–1993</td>
          <td>IBM Models 1–5 (Brown et al.) — основа пословного SMT</td>
        </tr>
        <tr>
          <td>2002</td>
          <td>Метрика BLEU (Papineni et al.)</td>
        </tr>
        <tr>
          <td>2003</td>
          <td>Phrase-Based SMT (Koehn, Och, Marcu)</td>
        </tr>
        <tr>
          <td>2005</td>
          <td>Hierarchical Phrase-Based MT (Chiang)</td>
        </tr>
        <tr>
          <td>2006</td>
          <td>Запуск Google Translate (на базе SMT)</td>
        </tr>
        <tr>
          <td>2007</td>
          <td>Moses — открытая платформа SMT (Koehn et al.)</td>
        </tr>
      </tbody>
    </table>
  </div>

  <div class="kmp12"><strong>Важно:</strong> Главное концептуальное отличие SMT от RBMT — переход от <em>ручного</em> описания языка к <em>автоматическому</em> извлечению закономерностей из данных. Это фундаментальный сдвиг в методологии: от рационализма — к эмпиризму.</div>

  <a target="_blank" href="https://aclanthology.org/J93-2003/" class="link-kmp1">Brown et al. (1993) — The Mathematics of Statistical MT</a>
  <a target="_blank" href="https://aclanthology.org/P02-1040/" class="link-kmp1">Papineni et al. (2002) — BLEU: a Method for Automatic Evaluation of MT</a>
</section>


<!-- ============================== РАЗДЕЛ 4 ============================== -->
<section id="nmt" class="section">
  <h2 class="section-title">4. Neural Machine Translation (NMT): нейросетевые системы</h2>

  <div class="nmt-principles">
    <h3 class="nmt-principles-title">4.1. Основные принципы</h3>
    <div class="nmt-principles-card">
      <p>NMT — парадигма, в которой перевод выполняется <strong>единой нейронной сетью</strong>, обученной <em>end-to-end</em> (от исходного предложения к целевому) на параллельных корпусах. В отличие от SMT, где перевод — это pipeline из отдельных компонентов, NMT обучает <em>одну дифференцируемую модель</em>, которая одновременно учится понимать исходный язык, находить соответствия и генерировать текст на целевом языке.</p>
      <p><strong>Архитектура encoder-decoder (кодировщик–декодировщик):</strong></p>
      <ul>
        <li><strong>Encoder (кодировщик)</strong> — читает исходное предложение и преобразует его в <em>вектор фиксированной длины</em> (или последовательность векторов), кодирующий смысл предложения в <em>непрерывном пространстве</em>.</li>
        <li><strong>Decoder (декодировщик)</strong> — на основе этого представления генерирует перевод, слово за словом (авторегрессивно: каждое следующее слово предсказывается с учётом всех уже сгенерированных).</li>
      </ul>
    </div>
  </div>

  <div class="nmt-arch">
    <h3 class="nmt-arch-title">4.2. Ключевые архитектурные идеи</h3>
    <div class="nmt-arch-card">
      <ul>
        <li><strong>RNN / LSTM / GRU.</strong> Первые NMT-модели использовали рекуррентные нейронные сети (Sutskever et al., 2014; Cho et al., 2014). LSTM (Long Short-Term Memory) и GRU (Gated Recurrent Unit) решали проблему «затухающего градиента», позволяя сети запоминать информацию на более длинных дистанциях. Однако рекуррентные модели обрабатывают входную последовательность <em>последовательно</em> (слово за словом), что ограничивает скорость обучения и длину контекста.</li>
        <li><strong>Механизм внимания (Attention).</strong> Bahdanau et al. (2014) предложили механизм, позволяющий декодировщику на каждом шаге генерации «обращать внимание» на разные части исходного предложения. Вместо сжатия всего предложения в один вектор, decoder получает <em>взвешенную сумму</em> представлений всех слов кодировщика. Это революционное решение резко улучшило качество, особенно для длинных предложений.</li>
        <li><strong>BPE-сегментация (Byte Pair Encoding).</strong> Sennrich et al. (2016) предложили алгоритм, разбивающий слова на <em>подсловные единицы</em> (subword units). Это решило проблему открытого словаря: редкие и незнакомые слова разбиваются на известные подсловные элементы, а не отбрасываются. BPE стал стандартом в NMT и используется по сей день.</li>
      </ul>
    </div>
  </div>

  <div class="nmt-systems">
    <h3 class="nmt-systems-title">4.3. Прорывы и системы</h3>
    <div class="nmt-systems-card">
      <ul>
        <li><strong>Google Neural Machine Translation (GNMT)</strong> — Wu et al. (2016). Google перевёл свой сервис Google Translate с фразового SMT на NMT. Переход привёл к <em>скачкообразному</em> улучшению качества: по ряду оценок, NMT сократила разрыв между машинным и человеческим переводом на 55–85 % (в зависимости от языковой пары). Это стало одним из самых заметных «демонстрационных моментов» NMT для широкой аудитории.</li>
        <li><strong>Multilingual NMT.</strong> Идея единой модели для перевода между множеством языков (Johnson et al., 2017): одна сеть обучается на данных для многих языковых пар одновременно, что позволяет осуществлять <em>zero-shot перевод</em> — перевод между парами языков, для которых нет параллельных данных (например, корейский→французский, если модель обучена на корейский→английский и английский→французский).</li>
      </ul>
    </div>
  </div>

  <div class="nmt-eval">
    <h3 class="nmt-eval-title">4.4. Сильные стороны и ограничения</h3>
    <div class="nmt-eval-card">
      <p><strong>Сильные стороны:</strong></p>
      <ul>
        <li><strong>Флуентность.</strong> NMT генерирует значительно более естественный и связный текст, чем SMT.</li>
        <li><strong>Учёт контекста.</strong> Механизм внимания позволяет учитывать зависимости в пределах предложения.</li>
        <li><strong>End-to-end обучение.</strong> Единая модель оптимизируется целиком, без «швов» между компонентами.</li>
        <li><strong>Обобщающая способность.</strong> Нейронные представления позволяют лучше обрабатывать незнакомые конструкции.</li>
      </ul>
      <p><strong>Ограничения:</strong></p>
      <ul>
        <li><strong>Требовательность к данным и вычислительным ресурсам.</strong> Для достижения хорошего качества нужны миллионы параллельных предложений и GPU-обучение.</li>
        <li><strong>Склонность к «галлюцинациям».</strong> Модель может порождать флуентный, но <em>фактически неверный</em> или не связанный с оригиналом текст, особенно при недостатке данных или нетипичном вводе.</li>
        <li><strong>Непрозрачность (black box).</strong> Сложно объяснить, <em>почему</em> модель выбрала тот или иной вариант перевода.</li>
        <li><strong>Проблема за пределами предложения.</strong> Стандартные NMT-модели переводят предложения изолированно, не учитывая документный контекст.</li>
      </ul>
    </div>
  </div>

  <div class="table-kmp">
    <table class="table">
      <thead>
        <tr>
          <th>Дата</th>
          <th>Событие</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>2014</td>
          <td>Seq2Seq encoder-decoder (Sutskever et al.; Cho et al.)</td>
        </tr>
        <tr>
          <td>2014–2015</td>
          <td>Attention mechanism (Bahdanau et al.)</td>
        </tr>
        <tr>
          <td>2016</td>
          <td>BPE-сегментация для NMT (Sennrich et al.)</td>
        </tr>
        <tr>
          <td>2016</td>
          <td>Google NMT — переход Google Translate на NMT (Wu et al.)</td>
        </tr>
        <tr>
          <td>2017</td>
          <td>Multilingual NMT / Zero-shot Translation (Johnson et al.)</td>
        </tr>
      </tbody>
    </table>
  </div>

  <div class="kmp12"><strong>Важно:</strong> Переход от SMT к NMT — это не просто «улучшение качества», а смена <em>типа представления</em> языка: от дискретных символических структур (слова, фразы, правила) к непрерывным распределённым представлениям (векторам). Это имеет глубокие последствия для теории перевода.</div>

  <div class="kmp14"><strong>Пояснение:</strong> «Галлюцинация» в контексте NMT — это генерация текста, который грамматически и стилистически корректен, но содержит информацию, отсутствующую в оригинале, или искажает его смысл. Например, модель может «выдумать» числа, имена или факты.</div>

  <a target="_blank" href="https://arxiv.org/abs/1409.3215" class="link-kmp1">Sutskever et al. (2014) — Sequence to Sequence Learning with Neural Networks</a>
  <a target="_blank" href="https://arxiv.org/abs/1409.0473" class="link-kmp1">Bahdanau et al. (2014) — Neural Machine Translation by Jointly Learning to Align and Translate</a>
</section>


<!-- ============================== РАЗДЕЛ 5 ============================== -->
<section id="transformer-llm" class="section">
  <h2 class="section-title">5. Transformer и эра больших языковых моделей (LLM)</h2>

  <div class="tr-arch">
    <h3 class="tr-arch-title">5.1. Архитектура Transformer</h3>
    <div class="tr-arch-card">
      <p>В 2017 году группа исследователей Google опубликовала статью <strong>«Attention Is All You Need»</strong> (Vaswani et al.), предложив архитектуру <strong>Transformer</strong>, которая полностью отказалась от рекуррентности (RNN) и свёрток (CNN) в пользу механизма <em>self-attention</em> (самовнимания).</p>
      <p><strong>Ключевые компоненты Transformer:</strong></p>
      <ul>
        <li><strong>Self-Attention (самовнимание).</strong> Каждый элемент последовательности «смотрит» на все остальные элементы, вычисляя степень их релевантности. Это позволяет моделировать зависимости на <em>любой дистанции</em> за один шаг (в отличие от RNN, где информация должна «проползти» через всю цепочку).</li>
        <li><strong>Multi-Head Attention.</strong> Несколько параллельных «головок» внимания, каждая из которых улавливает разные типы зависимостей (синтаксические, семантические, референциальные и др.).</li>
        <li><strong>Позиционное кодирование (Positional Encoding).</strong> Поскольку Transformer не обрабатывает последовательность «по порядку», информация о позиции слов в предложении добавляется явно через специальные векторы.</li>
        <li><strong>Параллелизация.</strong> Отсутствие рекуррентности позволяет обрабатывать все позиции входной последовательности одновременно, что <em>радикально ускоряет</em> обучение на GPU/TPU.</li>
      </ul>
      <p><strong>Пояснение:</strong> Transformer сохраняет архитектуру encoder-decoder, но заменяет RNN-слои стеком слоёв self-attention и feed-forward. Это дало прорыв и в качестве, и в скорости обучения.</p>
    </div>
  </div>

  <div class="tr-mt-models">
    <h3 class="tr-mt-models-title">5.2. Специализированные модели перевода на базе Transformer</h3>
    <div class="tr-mt-models-card">
      <p>После 2017 года Transformer стал стандартной архитектурой для систем машинного перевода. Появился ряд моделей, специально обученных для перевода:</p>
      <ul>
        <li><strong>Fairseq / MarianNMT</strong> — открытые библиотеки для обучения Transformer-моделей перевода.</li>
        <li><strong>mBART</strong> (Liu et al., 2020) — мультиязычная модель, предобученная на одноязычных корпусах 25 языков методом деноизинга, а затем дообученная для перевода. Позволяет получить хорошее качество даже для малоресурсных языковых пар.</li>
        <li><strong>mT5</strong> (Xue et al., 2021) — мультиязычная версия модели T5 (Text-to-Text Transfer Transformer), обученная на 101 языке.</li>
        <li><strong>NLLB (No Language Left Behind)</strong> (Meta, 2022) — модель, обученная для перевода между 200 языками, включая малоресурсные и исчезающие.</li>
        <li><strong>DeepL</strong> (запущен в 2017) — коммерческая система, использующая Transformer-архитектуру, известная высоким качеством перевода для европейских языков.</li>
      </ul>
    </div>
  </div>

  <div class="tr-llm">
    <h3 class="tr-llm-title">5.3. Большие языковые модели (LLM) и перевод</h3>
    <div class="tr-llm-card">
      <p>Параллельно со специализированными MT-моделями развивались <strong>большие языковые модели (Large Language Models)</strong> — модели общего назначения, обученные на огромных объёмах текста для предсказания следующего токена. Оказалось, что такие модели обладают <em>эмерджентными</em> переводческими способностями — способностями, которые не были явно заданы при проектировании, а возникли как побочный продукт масштабирования.</p>
      <p><strong>Ключевые модели и вехи:</strong></p>
      <ul>
        <li><strong>GPT-3</strong> (Brown et al., 2020) — продемонстрировал способность к few-shot переводу (перевод по нескольким примерам в промпте, без дополнительного обучения).</li>
        <li><strong>GPT-4, Claude, Gemini</strong> и другие мультимодальные LLM (2023–2024) — показывают качество перевода, конкурентоспособное со специализированными системами для ресурсных языков, а также способность к <em>стилистической адаптации</em>, <em>перефразированию</em> и <em>объяснению</em> своих переводческих решений.</li>
        <li><strong>Промптинг (prompting)</strong> — основной способ управления переводом в LLM: инструкция в свободной форме (например, «Переведи на формальный немецкий, сохраняя юмор оригинала»).</li>
      </ul>
      <p><strong>Режимы перевода в LLM:</strong></p>
      <ul>
        <li><strong>Zero-shot</strong> — перевод без примеров: модель получает только инструкцию и текст.</li>
        <li><strong>Few-shot</strong> — перевод с несколькими примерами-образцами в промпте.</li>
        <li><strong>Fine-tuned</strong> — LLM, дополнительно обученная на параллельных данных для конкретного домена или языковой пары.</li>
      </ul>
    </div>
  </div>

  <div class="tr-comparison">
    <h3 class="tr-comparison-title">5.4. Сравнительная таблица парадигм машинного перевода</h3>
    <div class="tr-comparison-card">
      <p>Обобщённое сопоставление четырёх основных парадигм:</p>
    </div>
  </div>

  <div class="table-kmp">
    <table class="table">
      <thead>
        <tr>
          <th>Характеристика</th>
          <th>RBMT</th>
          <th>SMT</th>
          <th>NMT (RNN/LSTM)</th>
          <th>Transformer / LLM</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>Источник знаний</strong></td>
          <td>Правила и словари (лингвисты)</td>
          <td>Параллельные корпуса (статистика)</td>
          <td>Параллельные корпуса (нейросеть)</td>
          <td>Масштабные корпуса (пре-тренинг + параллельные данные)</td>
        </tr>
        <tr>
          <td><strong>Единица перевода</strong></td>
          <td>Слово / синтаксическая конструкция</td>
          <td>Фраза (n-грамма)</td>
          <td>Подслово / скрытое состояние</td>
          <td>Подслово / контекстуальный вектор</td>
        </tr>
        <tr>
          <td><strong>Представление смысла</strong></td>
          <td>Символическое (деревья, правила)</td>
          <td>Дискретные вероятностные таблицы</td>
          <td>Непрерывные векторы (embeddings)</td>
          <td>Глубокие контекстуальные векторы</td>
        </tr>
        <tr>
          <td><strong>Учёт контекста</strong></td>
          <td>Локальный (предложение)</td>
          <td>Локальный (фраза + языковая модель n-грамм)</td>
          <td>Предложение (attention)</td>
          <td>Документ, диалог, инструкция (длинное окно контекста)</td>
        </tr>
        <tr>
          <td><strong>Флуентность</strong></td>
          <td>Низкая–средняя</td>
          <td>Средняя</td>
          <td>Высокая</td>
          <td>Очень высокая</td>
        </tr>
        <tr>
          <td><strong>Объяснимость</strong></td>
          <td>Высокая</td>
          <td>Средняя</td>
          <td>Низкая</td>
          <td>Низкая (но возможна через промптинг)</td>
        </tr>
        <tr>
          <td><strong>Потребность в данных</strong></td>
          <td>Эксперты, а не данные</td>
          <td>Параллельные корпуса (миллионы предложений)</td>
          <td>Параллельные корпуса (миллионы предложений) + GPU</td>
          <td>Гигантские одноязычные + параллельные корпуса, TPU/GPU-кластеры</td>
        </tr>
        <tr>
          <td><strong>Риск «галлюцинаций»</strong></td>
          <td>Минимальный</td>
          <td>Низкий</td>
          <td>Средний</td>
          <td>Значительный</td>
        </tr>
        <tr>
          <td><strong>Пик эпохи</strong></td>
          <td>1960-е–1990-е</td>
          <td>2000-е–2015</td>
          <td>2015–2018</td>
          <td>2018 – наст. время</td>
        </tr>
      </tbody>
    </table>
  </div>

  <div class="kmp11"><strong>Примечание:</strong> Границы между эпохами условны: системы и подходы сосуществуют. Современные коммерческие системы часто являются <em>гибридными</em> — например, используют терминологические словари (элемент RBMT) в сочетании с нейросетевой моделью.</div>

  <div class="kmp12"><strong>Важно:</strong> Эмерджентные способности LLM к переводу — один из самых неожиданных результатов масштабирования нейронных моделей. Модели, обученные «просто предсказывать следующее слово», оказываются способны к межъязыковому переносу без явного обучения на параллельных данных.</div>

  <a target="_blank" href="https://arxiv.org/abs/1706.03762" class="link-kmp1">Vaswani et al. (2017) — Attention Is All You Need</a>
  <a target="_blank" href="https://arxiv.org/abs/2207.04672" class="link-kmp1">NLLB Team (2022) — No Language Left Behind</a>
</section>


<!-- ============================== РАЗДЕЛ 6 ============================== -->
<section id="extended-translation" class="section">
  <h2 class="section-title">6. Расширенное понимание перевода в эпоху LLM</h2>

  <div class="ext-jakobson">
    <h3 class="ext-jakobson-title">6.1. Типология перевода по Р.&nbsp;Якобсону</h3>
    <div class="ext-jakobson-card">
      <p>Ещё в 1959 году лингвист <strong>Роман Якобсон</strong> в работе «О лингвистических аспектах перевода» предложил различать три типа перевода:</p>
      <ul>
        <li><strong>Внутриязыковой перевод (intralingual translation)</strong> — перефразирование, пересказ, упрощение текста <em>в пределах одного языка</em>. Пример: адаптация научного текста для широкой аудитории.</li>
        <li><strong>Межъязыковой перевод (interlingual translation)</strong> — собственно перевод с одного естественного языка на другой. Это «перевод» в традиционном понимании.</li>
        <li><strong>Межсемиотический перевод (intersemiotic translation / transmutation)</strong> — перенос смысла между разными <em>знаковыми системами</em>. Примеры: экранизация романа, описание картины словами, озвучивание текста.</li>
      </ul>
      <p><strong>Пояснение:</strong> Типология Якобсона оказывается удивительно актуальной в эпоху LLM, поскольку современные мультимодальные модели способны выполнять <em>все три типа</em> перевода — и часто комбинируют их в рамках одной задачи.</p>
    </div>
  </div>

  <div class="ext-semiotic">
    <h3 class="ext-semiotic-title">6.2. Перевод как межсемиотический перенос смысла</h3>
    <div class="ext-semiotic-card">
      <p>В эпоху LLM понятие «перевода» выходит за рамки строгого межъязыкового соответствия и становится <strong>межсемиотическим</strong> — включающим перенос смысла между модальностями и знаковыми системами:</p>
      <ul>
        <li><strong>Текст → Текст (на том же языке):</strong> перефразирование, адаптация стиля (формальный → разговорный), суммаризация, упрощение.</li>
        <li><strong>Текст → Текст (на другом языке):</strong> «классический» перевод, но с возможностью стилистической и культурной адаптации (локализация).</li>
        <li><strong>Текст → Речь (TTS):</strong> синтез речи с учётом просодии, эмоций, акцента.</li>
        <li><strong>Речь → Текст (ASR + перевод):</strong> каскадный или end-to-end устный перевод.</li>
        <li><strong>Текст → Изображение:</strong> генерация изображений по текстовому описанию (DALL·E, Midjourney, Stable Diffusion).</li>
        <li><strong>Изображение → Текст:</strong> описание, объяснение, OCR + перевод.</li>
        <li><strong>Текст → Видео, Видео → Текст</strong> и другие модальные переходы.</li>
      </ul>
      <p>Современные LLM оперируют <strong>распределёнными представлениями смысла</strong> и <strong>мультимодальными сигналами</strong>. Реальные преобразования, которые они выполняют, часто комбинируют:</p>
      <ul>
        <li><strong>Семантическую трансформацию</strong> — перенос смысла высказывания.</li>
        <li><strong>Стилистическую адаптацию</strong> — подстройку под регистр, аудиторию, жанр.</li>
        <li><strong>Модальную транскодировку</strong> — перевод между знаковыми системами (текст, звук, изображение).</li>
      </ul>
      <p>Границы между «переводом», «перефразом», «адаптацией» и «генерацией» становятся <em>размытыми и условными</em> — они задаются целями пользователя и задачей, а не внутренней механикой модели.</p>
    </div>
  </div>

  <div class="ext-metrics">
    <h3 class="ext-metrics-title">6.3. Метрики качества и методы контроля</h3>
    <div class="ext-metrics-card">
      <p>Расширенное понимание перевода требует расширенного набора <strong>метрик оценки качества</strong>:</p>
      <ul>
        <li><strong>Флуентность (fluency)</strong> — насколько естественно звучит результат на целевом языке / в целевой модальности.</li>
        <li><strong>Сохранение смысла (adequacy / meaning preservation)</strong> — насколько полно и точно передан смысл оригинала.</li>
        <li><strong>Верность фактам (faithfulness / factual accuracy)</strong> — отсутствие «галлюцинаций», добавленной или искажённой информации.</li>
        <li><strong>Соответствие модальности</strong> — для межсемиотического перевода: насколько результат соответствует ожиданиям целевой знаковой системы (например, реалистичность изображения, разборчивость речи).</li>
        <li><strong>Соответствие цели пользователя</strong> — прагматическая адекватность: решает ли результат задачу, для которой был запрошен перевод.</li>
      </ul>
      <p><strong>Методы контроля перевода в LLM:</strong></p>
      <ul>
        <li><strong>Промптинг (prompting)</strong> — формулирование инструкций, определяющих стиль, регистр, целевую аудиторию, степень точности.</li>
        <li><strong>Ограничение (constraining)</strong> — задание терминологических словарей, глоссариев, запрещённых формулировок.</li>
        <li><strong>Пост-редактирование (post-editing)</strong> — проверка и правка результата человеком-переводчиком.</li>
        <li><strong>Автоматическая оценка</strong> — использование метрик (BLEU, COMET, BLEURT, BERTScore и др.) для автоматического мониторинга качества.</li>
      </ul>
    </div>
  </div>

  <div class="table-kmp">
    <table class="table">
      <thead>
        <tr>
          <th>Тип перевода (по Якобсону)</th>
          <th>Примеры задач для LLM</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Внутриязыковой</td>
          <td>Перефразирование, суммаризация, упрощение, смена стиля</td>
        </tr>
        <tr>
          <td>Межъязыковой</td>
          <td>Перевод текста, локализация, адаптация</td>
        </tr>
        <tr>
          <td>Межсемиотический</td>
          <td>Text-to-Image, Text-to-Speech, Image Captioning, описание видео</td>
        </tr>
      </tbody>
    </table>
  </div>

  <div class="kmp12"><strong>Важно:</strong> Исследовать перевод в эпоху LLM стоит именно как более широкое явление — <em>межсемиотический перенос смысла и намерений</em>. Классическое переводоведение, сфокусированное на межъязыковом переводе текстов, охватывает лишь часть того, что делают современные мультимодальные модели.</div>

  <div class="kmp14"><strong>Пояснение:</strong> «Локализация» — процесс адаптации текста не только на другой язык, но и к другой <em>культуре</em> (адаптация дат, единиц измерения, юмора, визуального ряда, юридических отсылок и т. д.). LLM способны частично автоматизировать локализацию благодаря своим знаниям о культурных контекстах.</div>
</section>


<!-- ============================== РАЗДЕЛ 7 ============================== -->
<section id="llm-paradigm" class="section">
  <h2 class="section-title">7. Перевод в LLM-парадигме: природа процесса</h2>

  <div class="llm-cont">
    <h3 class="llm-cont-title">7.1. Континуальные представления vs. символические правила</h3>
    <div class="llm-cont-card">
      <p>В LLM-парадигме произошёл фундаментальный сдвиг в способе представления языковых знаний:</p>

      <p><strong>Символическая парадигма (RBMT):</strong> знания о языке хранятся в виде <em>дискретных правил</em> и <em>словарных статей</em>. Слово — это символ, грамматика — набор формальных правил преобразования символов. Перевод — детерминированная операция: одинаковый вход всегда даёт одинаковый выход.</p>

      <p><strong>Вероятностная парадигма (SMT):</strong> знания извлекаются из данных в виде <em>таблиц вероятностей</em>. Слова и фразы — по-прежнему дискретные единицы, но связи между ними — вероятностные. Перевод — выбор наиболее вероятного варианта.</p>

      <p><strong>Нейросетевая / LLM-парадигма (NMT → Transformer → LLM):</strong> знания о языке хранятся <em>распределённо</em> — в миллиардах числовых параметров (весов) нейронной сети. Каждое слово представлено не как символ и не как запись в таблице, а как <em>точка в многомерном непрерывном пространстве</em> (вектор, embedding). Смысл слова определяется его <em>положением относительно</em> всех остальных слов в этом пространстве. Более того, представление слова <em>зависит от контекста</em>: одно и то же слово получает разные векторы в разных предложениях (контекстуальные embeddings).</p>

      <p><strong>Следствие:</strong> в LLM нет «словарной статьи» для слова и нет «правила» для конструкции. Есть <em>непрерывное пространство смыслов</em>, в котором перевод — это перемещение от одной области пространства (исходный язык, стиль, модальность) к другой (целевой язык, стиль, модальность).</p>
    </div>
  </div>

  <div class="llm-stoch">
    <h3 class="llm-stoch-title">7.2. Стохастическая природа генерации</h3>
    <div class="llm-stoch-card">
      <p>Перевод в LLM — это <strong>стохастический</strong> (вероятностный) процесс, а не детерминированное преобразование:</p>
      <ul>
        <li>На каждом шаге генерации модель вычисляет <em>распределение вероятностей</em> над всеми возможными следующими токенами (подсловными единицами).</li>
        <li>Выбор конкретного токена зависит от <em>стратегии декодирования</em>: жадный поиск (greedy), лучевой поиск (beam search), семплирование (sampling) с параметром «температуры».</li>
        <li>При одном и том же входе модель может порождать <em>разные переводы</em> (при использовании стохастического декодирования). Это принципиально отличается от RBMT, где результат полностью детерминирован.</li>
      </ul>
      <p><strong>Практические следствия стохастичности:</strong></p>
      <ul>
        <li><strong>Вариативность</strong> — можно получить несколько вариантов перевода и выбрать лучший.</li>
        <li><strong>Креативность</strong> — модель способна предлагать неочевидные, но удачные решения.</li>
        <li><strong>Непредсказуемость и галлюцинации</strong> — тот же механизм, который порождает креативность, порождает и ошибки: модель может «выбрать» вероятный, но фактически неверный вариант.</li>
      </ul>
    </div>
  </div>

  <div class="llm-implications">
    <h3 class="llm-implications-title">7.3. Импликации для переводоведения и практики</h3>
    <div class="llm-implications-card">
      <p>LLM-парадигма ставит перед переводоведением и переводческой практикой ряд новых вопросов:</p>
      <ul>
        <li><strong>Что такое «правильный перевод»?</strong> Если модель порождает несколько флуентных и адекватных вариантов, какой из них «правильный»? Понятие единственного правильного перевода становится ещё более проблематичным, чем в традиционном переводоведении.</li>
        <li><strong>Кто несёт ответственность за ошибку?</strong> Непрозрачность моделей и их склонность к галлюцинациям поднимают вопросы <em>этики и юридической ответственности</em> машинного перевода.</li>
        <li><strong>Роль переводчика-человека.</strong> Смещается от «автора перевода» к «редактору, контролёру и промпт-инженеру» — специалисту, который формулирует задачу для модели, оценивает результат и несёт финальную ответственность.</li>
        <li><strong>Новые компетенции лингвиста.</strong> Понимание того, как работают LLM (на концептуальном уровне), становится необходимой частью профессиональной подготовки.</li>
      </ul>
    </div>
  </div>

  <div class="table-kmp">
    <table class="table">
      <thead>
        <tr>
          <th>Аспект</th>
          <th>Классический MT (RBMT/SMT)</th>
          <th>LLM-парадигма</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Представление знаний</td>
          <td>Дискретное (правила, таблицы вероятностей)</td>
          <td>Непрерывное, распределённое (векторы)</td>
        </tr>
        <tr>
          <td>Контекст</td>
          <td>Локальный (предложение, фраза)</td>
          <td>Глобальный (документ, диалог, инструкция)</td>
        </tr>
        <tr>
          <td>Детерминированность</td>
          <td>Высокая (RBMT) / средняя (SMT)</td>
          <td>Низкая — стохастическая генерация</td>
        </tr>
        <tr>
          <td>Объяснимость</td>
          <td>Высокая (RBMT) / средняя (SMT)</td>
          <td>Низкая (black box)</td>
        </tr>
        <tr>
          <td>Границы «перевода»</td>
          <td>Межъязыковой перевод текста</td>
          <td>Межсемиотический перенос смысла</td>
        </tr>
        <tr>
          <td>Роль человека</td>
          <td>Создатель правил / составитель корпуса</td>
          <td>Промпт-инженер, редактор, контролёр</td>
        </tr>
      </tbody>
    </table>
  </div>

  <div class="kmp12"><strong>Важно:</strong> Перевод в LLM-парадигме — это <em>статистически обусловленная языковая продукция</em>, а не строго детерминированное преобразование. Модель не «знает» перевод — она <em>порождает</em> наиболее вероятное продолжение в контексте задачи перевода, опираясь на закономерности, усвоенные из обучающих данных.</div>

  <div class="kmp14"><strong>Пояснение:</strong> Сдвиг от символических правил и явных вероятностных моделей к непрерывным векторным представлениям означает, что смысл выражается <em>распределённо</em>: нет отдельной «ячейки памяти» для каждого слова или правила. Контекст (локальный и глобальный) и общая языковая индукция модели совместно определяют выбор перевода. Результаты — более флуентные, но <em>вероятностно обусловленные</em>, что неизбежно включает риск «галлюцинаций» и фактических неточностей.</div>

  <div class="kmp11"><strong>Примечание:</strong> Понимание стохастической природы LLM критически важно для профессионального лингвиста и переводчика.</div>
</section>



<footer class="footer">
<div class="container">
<p>© 2026 | kmp | CC BY-NC-SA 4.0<br>
Разработано для студентов БрГУ имени А.С. Пушкина</p>
</div>
</footer>
<div style="position: fixed; bottom: 10px; color: #777777; right: 30px; opacity: 0.3; font-size: 14px;">kmp+</div>
    <script>
        // Функция для плавной прокрутки к разделу
        function scrollToSection(sectionId) {
            const section = document.getElementById(sectionId);
            const menuHeight = document.querySelector('.menu').offsetHeight;
            
            window.scrollTo({
                top: section.offsetTop - menuHeight - 20,
                behavior: 'smooth'
            });
        }

        // Функция для переключения темы
        document.getElementById('themeToggle').addEventListener('click', function() {
            const currentTheme = document.documentElement.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            
            document.documentElement.setAttribute('data-theme', newTheme);
            this.textContent = newTheme === 'dark' ? '🌙' : '☀️';
        });

        // Анимация появления секций при прокрутке
        document.addEventListener('DOMContentLoaded', function() {
            const sections = document.querySelectorAll('.section');
            
            const observerOptions = {
                root: null,
                rootMargin: '0px',
                threshold: 0.1
            };
            
            const observer = new IntersectionObserver(function(entries, observer) {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('fade-in');
                        observer.unobserve(entry.target);
                    }
                });
            }, observerOptions);
            
            sections.forEach(section => {
                section.classList.remove('fade-in');
                observer.observe(section);
            });
        });
    </script>
</body>
</html>