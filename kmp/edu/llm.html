<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>kmp+</title>
    <style>
        /* Основные стили */
        :root {
            --primary-color: #325980;
            --secondary-color: #4CAF50;
            --background-color: #f5f5f5;
            --content-bg: #ffffff;
            --text-color: #333333;
            --header-text-color: #ffffff;
            --menu-bg: #ffffff;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            --border-radius: 8px;
			  --accent11: #4caf50;
		--accent12: #4cafff;
		--accent13: #ffaf50;
		--accent14: #821978;
        }

        /* Темная тема */
        [data-theme="dark"] {
            --primary-color: #3e76ad;
            --secondary-color: #388e3c;
            --background-color: #000000;
            --content-bg: #1e1e1e;
            --text-color: #e0e0e0;
            --header-text-color: #ffffff;
            --menu-bg: #000000;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            transition: background-color 0.3s, color 0.3s;
        }

        body {
            font-family: 'Roboto', 'Arial', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--background-color);
            padding-top: 2px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        header {
            background-color: var(--primary-color);
            color: var(--header-text-color);
            padding: 20px 0;
            text-align: center;
            border-radius: var(--border-radius);
            margin-bottom: 2px;
        }

        h1 {
            font-size: 2.2rem;
            margin-bottom: 10px;
        }

        h2 {
            color: var(--primary-color);
            margin: 25px 0 15px;
            padding-bottom: 8px;
            border-bottom: 2px solid var(--secondary-color);
        }

        h3 {
            color: var(--primary-color);
            margin: 20px 0 10px;
        }

        p {
            margin-bottom: 15px;
        }

        /* Меню навигации */
        .menu {
            background-color: var(--menu-bg);
            padding: 15px 20px;
            border-radius: var(--border-radius);
            margin-bottom: 30px;
            box-shadow: var(--menu-shadow);
            display: flex;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .menu-buttons {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }
		
		
		.menu-buttons {
    display: flex;
    flex-wrap: wrap;
    gap: 10px;
    justify-content: center; /* Изменено на center */
}

        .menu-btn {
            background-color: var(--primary-color);
            color: white;
            border: none;
            padding: 8px 15px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: background-color 0.3s;
        }

        .menu-btn:hover {
            background-color: var(--secondary-color);
        }

        .theme-toggle {
            background: none;
            border: 10px solid transparent;
            font-size: 1.5rem;
            cursor: pointer;
            color: var(--primary-color);
        }

        /* Секции контента */
        .section {
            background-color: var(--content-bg);
            border-radius: var(--border-radius);
            padding: 25px;
            margin-bottom: 30px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        /* Выделение важного */
        .important {
            background-color: rgba(76, 175, 80, 0.1);
            border-left: 4px solid var(--secondary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        .note {
            background-color: rgba(50, 89, 128, 0.1);
            border-left: 4px solid var(--primary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        /* Таблицы */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            padding: 12px 15px;
            border: 1px solid #ddd;
            text-align: left;
        }

        th {
            background-color: var(--primary-color);
            color: white;
        }

        tr:nth-child(even) {
            background-color: rgba(0, 0, 0, 0.03);
        }

        /* Списки */
        ul, ol {
            padding-left: 25px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
        }

        /* Адаптивный дизайн */
        @media (max-width: 768px) {
            h1 {
                font-size: 1.8rem;
            }
            
            h2 {
                font-size: 1.5rem;
            }
            
            .menu {
                flex-direction: column;
                gap: 15px;
            }
            
            .menu-buttons {
                width: 100%;
                justify-content: center;
            }
            
            .theme-toggle {
                margin-top: 10px;
            }
            
            .section {
                padding: 15px;
            }
        }

        /* Анимации */
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .fade-in {
            animation: fadeIn 0.5s ease-in;
        }

        footer {
            text-align: center;
            padding: 20px 0;
            margin-top: 40px;
            font-size: 0.9rem;
        }
		
					.kmp11, .example {
      background: rgba(76, 175, 80, 0.1);
      border-left: 4px solid var(--accent11);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp12, .example {
      background: rgba(95, 182, 237, 0.1);
      border-left: 4px solid var(--accent12);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
 
		.kmp13, .example {
      background: rgba(205, 170, 110, 0.1);
      border-left: 4px solid var(--accent13);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp14, .example {
      background: rgba(205, 110, 200, 0.1);
      border-left: 4px solid var(--accent14);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>LLM </h1>
            <p>Большие языковые модели (Large Language Models)</p>
        </header>

        <nav class="menu">
            <div class="menu-buttons">
                <button class="menu-btn" onclick="scrollToSection('section1')">GAI</button>
				<button class="menu-btn" onclick="scrollToSection('section2')">LM</button>
                <button class="menu-btn" onclick="scrollToSection('section3')">Emergence</button>
                <button class="menu-btn" onclick="scrollToSection('section4')">Big Data</button>
                <button class="menu-btn" onclick="scrollToSection('section5')">Stochasticity</button>
                <button class="menu-btn" onclick="scrollToSection('section6')">Vectorization</button>
				<button class="menu-btn" onclick="scrollToSection('section7')">Embeddings</button>
                <button class="menu-btn" onclick="scrollToSection('section8')">Neural Nets</button>
            </div>
            <button class="theme-toggle" id="themeToggle" title="Переключить тему">☀️</button>
        </nav>
	
		<section id="section1" class="section">
    <h2 class="section-title">1. Принципы работы и модели генеративного ИИ (GAI)</h2>
    <div class="001">
        <h3 class="001-title">Основы генеративного ИИ</h3>
        <div class="001-card">
            <h4>Что такое генеративный ИИ</h4>
            <p>Генеративный искусственный интеллект (Generative Artificial Intelligence, GAI) — это класс алгоритмов машинного обучения, способных создавать новый контент, включая текст, изображения, аудио и видео. В отличие от традиционных систем ИИ, которые анализируют и классифицируют существующие данные, генеративные модели могут производить оригинальный контент, имитируя человеческое творчество.</p>
            <p>Генеративный ИИ работает на основе обучения на больших массивах данных, выявляя закономерности и статистические связи, которые затем используются для генерации нового содержания, соответствующего этим закономерностям.</p>
            <p><strong>Ключевые характеристики генеративного ИИ:</strong></p>
            <ul>
                <li>Способность создавать новый контент, а не только анализировать существующий</li>
                <li>Обучение на основе больших массивов данных без явной разметки</li>
                <li>Использование вероятностных методов для генерации разнообразных выходных данных</li>
                <li>Возможность адаптации к различным задачам и доменам</li>
            </ul>
            <p><strong>Пояснение:</strong> Генеративные модели ИИ "учатся" на примерах и затем могут создавать новые экземпляры, похожие на обучающие данные, но не идентичные им.</p>
        </div>
    </div>
    <div class="002">
        <h3 class="002-title">Типы генеративных моделей</h3>
        <div class="002-card">
            <h4>Основные архитектуры генеративных моделей</h4>
            <p>В современном ИИ существует несколько ключевых типов генеративных моделей, каждая со своими особенностями и областями применения. Для языковых задач наиболее важными стали трансформерные архитектуры.</p>
            <p>Генеративные модели различаются по архитектуре, методам обучения и типам данных, с которыми они работают.</p>
            <p><strong>Основные типы генеративных моделей:</strong></p>
            <ul>
                <li>Трансформеры (используются в LLM, таких как GPT, Claude, LLaMA)</li>
                <li>Генеративно-состязательные сети (GAN)</li>
                <li>Вариационные автоэнкодеры (VAE)</li>
                <li>Диффузионные модели</li>
            </ul>
            <p><strong>Пояснение:</strong> Для обработки естественного языка наиболее эффективными оказались трансформерные архитектуры, которые лежат в основе современных языковых моделей.</p>
        </div>
    </div>
    <div class="kmp14"><strong>Важно:</strong> Генеративный ИИ не обладает пониманием в человеческом смысле этого слова. Он работает на основе статистических закономерностей, выявленных в обучающих данных, и не имеет истинного осознания контекста или значения генерируемого контента.</div>
</section>

<section id="section2" class="section">
    <h2 class="section-title">2. Language Models</h2>
    <div class="001">
        <h3 class="001-title">Понятие языковой модели</h3>
        <div class="001-card">
            <h4>Определение и назначение языковых моделей</h4>
            <p>Языковая модель (Language Model) — это алгоритм машинного обучения, обученный предсказывать вероятность последовательности слов или символов в тексте. Основная задача языковой модели — оценить вероятность появления определенного слова или фразы в заданном контексте.</p>
            <p>Языковые модели изучают статистические закономерности в языке на основе корпусов текстов и используют эти знания для генерации текста, который статистически похож на человеческую речь.</p>
            <p><strong>Основные функции языковых моделей:</strong></p>
            <ul>
                <li>Предсказание следующего слова или символа в последовательности</li>
                <li>Генерация связного текста на основе начального запроса</li>
                <li>Оценка вероятности различных вариантов продолжения текста</li>
                <li>Понимание контекста и семантических связей между словами</li>
            </ul>
            <p><strong>Пояснение:</strong> Языковые модели можно рассматривать как системы, которые "выучили" статистические закономерности языка и могут использовать эти знания для генерации текста или оценки вероятности различных языковых конструкций.</p>
        </div>
    </div>
    <div class="002">
        <h3 class="002-title">Эволюция языковых моделей</h3>
        <div class="002-card">
            <h4>От простых к сложным языковым моделям</h4>
            <p>Языковые модели прошли значительный путь развития от простых статистических моделей до сложных нейросетевых архитектур. Эта эволюция отражает прогресс в понимании и моделировании естественного языка.</p>
            <p>Современные крупные языковые модели (LLM) представляют собой результат многолетних исследований и технологических прорывов в области обработки естественного языка.</p>
            <p><strong>Ключевые этапы эволюции языковых моделей:</strong></p>
            <ul>
                <li>N-граммные модели (статистический подход)</li>
                <li>Рекуррентные нейронные сети (RNN, LSTM, GRU)</li>
                <li>Модели на основе архитектуры трансформер (с 2017 года)</li>
                <li>Крупные языковые модели (LLM) с миллиардами параметров</li>
            </ul>
            <p><strong>Пояснение:</strong> Каждый новый тип языковых моделей решал определенные ограничения предыдущих подходов, что привело к постепенному улучшению качества генерации текста и понимания языка.</p>
        </div>
    </div>
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Тип языковой модели</th>
                    <th>Особенности и применение</th>
                </tr>
            </thead>
            <tr>
                <td>N-граммные модели</td>
                <td>Простые статистические модели, учитывающие вероятности последовательностей из N слов. Ограниченный контекст, проблемы с разреженностью данных.</td>
            </tr>
            <tr>
                <td>Рекуррентные нейронные сети</td>
                <td>Учитывают более длинный контекст, но страдают от проблемы затухающего градиента при обработке длинных последовательностей.</td>
            </tr>
            <tr>
                <td>Трансформерные модели</td>
                <td>Используют механизм внимания (attention), эффективно обрабатывают параллельно длинные последовательности, лучше улавливают дальние зависимости.</td>
            </tr>
            <tr>
                <td>Крупные языковые модели (LLM)</td>
                <td>Масштабные трансформерные модели с миллиардами параметров, обученные на огромных корпусах текста, демонстрируют эмерджентные способности.</td>
            </tr>
        </table>
    </div>
    <div class="kmp13"><strong>Пример:</strong> Если языковой модели дать начало предложения "Студенты изучают иностранные", она может предсказать, что следующим словом с высокой вероятностью будет "языки", основываясь на статистических закономерностях, выявленных в обучающих данных.</div>
</section>

<section id="section3" class="section">
    <h2 class="section-title">3. Масштабирование и эмерджентность языковых моделей</h2>
    <div class="001">
        <h3 class="001-title">Масштабирование языковых моделей</h3>
        <div class="001-card">
            <h4>Значение размера модели и объема данных</h4>
            <p>Масштабирование языковых моделей относится к увеличению их размера (количества параметров), объема обучающих данных и вычислительных ресурсов. Исследования показали, что увеличение масштаба модели часто приводит к значительному улучшению производительности и появлению новых возможностей.</p>
            <p>Современные LLM содержат десятки и сотни миллиардов параметров и обучаются на триллионах токенов текста, что требует огромных вычислительных ресурсов.</p>
            <p><strong>Ключевые аспекты масштабирования:</strong></p>
            <ul>
                <li>Увеличение количества параметров модели (от миллионов до сотен миллиардов)</li>
                <li>Расширение объема и разнообразия обучающих данных</li>
                <li>Оптимизация вычислительной инфраструктуры для обучения крупных моделей</li>
                <li>Разработка методов эффективного обучения и инференса</li>
            </ul>
            <p><strong>Пояснение:</strong> Закон масштабирования в языковых моделях показывает, что производительность модели улучшается предсказуемым образом при увеличении размера модели, объема данных и вычислительных ресурсов.</p>
        </div>
    </div>
    <div class="002">
        <h3 class="002-title">Эмерджентные способности</h3>
        <div class="002-card">
            <h4>Неожиданные возможности крупных языковых моделей</h4>
            <p>Эмерджентность в контексте языковых моделей относится к появлению новых способностей и функций, которые не были явно запрограммированы или ожидаемы, а возникают спонтанно при достижении определенного масштаба модели.</p>
            <p>Эмерджентные способности часто проявляются как качественные скачки в производительности при преодолении определенных пороговых значений размера модели или объема обучающих данных.</p>
            <p><strong>Примеры эмерджентных способностей LLM:</strong></p>
            <ul>
                <li>Решение сложных логических и математических задач</li>
                <li>Понимание и следование многошаговым инструкциям</li>
                <li>Способность к "рассуждению" и пошаговому решению проблем</li>
                <li>Перенос знаний между различными доменами и задачами</li>
                <li>Метаобучение и адаптация к новым задачам с минимальными примерами</li>
            </ul>
            <p><strong>Пояснение:</strong> Эмерджентные способности часто появляются неожиданно при масштабировании модели, что делает их особенно интересными для исследования, поскольку они не были явно заложены в архитектуру или процесс обучения.</p>
        </div>
    </div>
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Эмерджентная способность</th>
                    <th>Описание</th>
                </tr>
            </thead>
            <tr>
                <td>Решение задач в несколько шагов</td>
                <td>Способность разбивать сложные проблемы на подзадачи и последовательно их решать, появляется при достижении определенного масштаба модели.</td>
            </tr>
            <tr>
                <td>Обучение в контексте (in-context learning)</td>
                <td>Способность адаптироваться к новым задачам на основе нескольких примеров, предоставленных в запросе, без изменения параметров модели.</td>
            </tr>
            <tr>
                <td>Цепочки рассуждений (chain-of-thought)</td>
                <td>Способность генерировать промежуточные шаги рассуждения, что значительно улучшает решение сложных задач.</td>
            </tr>
            <tr>
                <td>Понимание неявных инструкций</td>
                <td>Способность интерпретировать и следовать инструкциям, даже если они выражены неявно или требуют контекстуального понимания.</td>
            </tr>
        </table>
    </div>
    <div class="kmp11"><strong>Примечание:</strong> Хотя эмерджентные способности LLM впечатляют, они все еще имеют существенные ограничения и не достигают уровня человеческого понимания и рассуждения. Модели могут давать уверенные, но неверные ответы и не обладают истинным пониманием концепций, с которыми работают.</div>
    <div class="kmp14"><strong>Важно:</strong> Масштабирование моделей требует значительных вычислительных ресурсов и энергозатрат, что поднимает вопросы экологической устойчивости и доступности технологии для широкого круга исследователей и организаций.</div>
</section>

<section id="section4" class="section">
    <h2 class="section-title">4. Big Data</h2>
    <div class="001">
        <h3 class="001-title">Концепция больших данных</h3>
        <div class="001-card">
            <h4>Что такое Big Data и их роль в развитии LLM</h4>
            <p>Большие данные (Big Data) — это наборы данных, объем, скорость обновления и разнообразие которых превышают возможности традиционных систем обработки данных. В контексте языковых моделей, большие данные представляют собой огромные корпусы текстов из различных источников, используемые для обучения моделей.</p>
            <p>Развитие LLM неразрывно связано с доступностью и качеством больших данных, поскольку именно на этих данных модели учатся понимать и генерировать человеческий язык.</p>
            <p><strong>Характеристики больших данных (5V):</strong></p>
            <ul>
                <li>Volume (объем) — огромные массивы информации</li>
                <li>Velocity (скорость) — быстрое накопление и обновление данных</li>
                <li>Variety (разнообразие) — различные типы и форматы данных</li>
                <li>Veracity (достоверность) — проблемы качества и надежности данных</li>
                <li>Value (ценность) — извлечение полезной информации из данных</li>
            </ul>
            <p><strong>Пояснение:</strong> Для обучения современных LLM используются сотни гигабайт или даже терабайты текстовых данных, собранных из интернета, книг, научных статей и других источников.</p>
        </div>
    </div>
    <div class="002">
        <h3 class="002-title">Источники и подготовка данных для LLM</h3>
        <div class="002-card">
            <h4>Сбор и обработка данных для обучения языковых моделей</h4>
            <p>Качество и разнообразие обучающих данных напрямую влияют на способности и ограничения языковых моделей. Процесс подготовки данных включает сбор, фильтрацию, очистку и структурирование текстовых корпусов.</p>
            <p>Современные LLM обучаются на данных из множества источников, что позволяет им приобретать широкий спектр знаний и языковых навыков.</p>
            <p><strong>Основные источники данных для LLM:</strong></p>
            <ul>
                <li>Веб-страницы (Common Crawl и другие веб-архивы)</li>
                <li>Книги и литературные произведения (Books1, Books2, Gutenberg Project)</li>
                <li>Научные статьи и публикации (arXiv, PubMed)</li>
                <li>Социальные медиа и форумы (Reddit, Twitter)</li>
                <li>Энциклопедии (Wikipedia)</li>
                <li>Код и техническая документация (GitHub, Stack Overflow)</li>
            </ul>
            <p><strong>Пояснение:</strong> Перед использованием для обучения, данные проходят многоэтапную обработку, включая дедупликацию (удаление повторов), фильтрацию нежелательного контента, токенизацию и другие преобразования.</p>
        </div>
    </div>
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Этап обработки данных</th>
                    <th>Описание</th>
                </tr>
            </thead>
            <tr>
                <td>Сбор данных</td>
                <td>Агрегация текстов из различных источников с использованием веб-краулеров, API и других методов.</td>
            </tr>
            <tr>
                <td>Фильтрация и очистка</td>
                <td>Удаление низкокачественного, токсичного или нерелевантного контента, исправление ошибок.</td>
            </tr>
            <tr>
                <td>Дедупликация</td>
                <td>Выявление и удаление дублирующихся или почти идентичных текстов для предотвращения переобучения.</td>
            </tr>
            <tr>
                <td>Токенизация</td>
                <td>Разбиение текста на токены (слова, части слов или символы), которые будут использоваться моделью.</td>
            </tr>
            <tr>
                <td>Формирование обучающих примеров</td>
                <td>Создание последовательностей токенов определенной длины для обучения модели предсказывать следующие токены.</td>
            </tr>
        </table>
    </div>
    <div class="kmp12"><strong>Внимание:</strong> Качество и разнообразие обучающих данных напрямую влияют на возможные предубеждения и ограничения языковых моделей. Модели могут воспроизводить и усиливать существующие в данных стереотипы, предубеждения и неточности.</div>
    <div class="kmp13"><strong>Пример:</strong> Модель GPT-3 была обучена на корпусе данных объемом около 570 ГБ текста, что эквивалентно примерно 400 миллиардам токенов. Эти данные включали тексты из Common Crawl (60%), книги (16%), Wikipedia (3%) и другие источники.</div>
</section>

<section id="section5" class="section">
    <h2 class="section-title">5. Стохастические модели языка</h2>
    <div class="001">
        <h3 class="001-title">Вероятностный подход к моделированию языка</h3>
        <div class="001-card">
            <h4>Основы стохастического моделирования языка</h4>
            <p>Стохастические модели языка основаны на вероятностном подходе к моделированию последовательностей слов или токенов. Они рассматривают язык как случайный процесс, где каждое следующее слово имеет определенную вероятность появления в зависимости от предыдущего контекста.</p>
            <p>В основе стохастических моделей лежит идея о том, что вероятность последовательности слов можно разложить на произведение условных вероятностей каждого слова, учитывая предыдущие слова.</p>
            <p><strong>Ключевые концепции стохастических моделей:</strong></p>
            <ul>
                <li>Условная вероятность слов в последовательности</li>
                <li>Марковское предположение (зависимость только от ограниченного контекста)</li>
                <li>Максимизация правдоподобия при обучении</li>
                <li>Сглаживание и обработка редких или неизвестных слов</li>
            </ul>
            <p><strong>Пояснение:</strong> Формально, стохастическая модель языка оценивает вероятность P(w₁, w₂, ..., wₙ) последовательности слов, обычно разлагая её на произведение условных вероятностей P(wᵢ|w₁, w₂, ..., wᵢ₋₁) для каждого слова.</p>
        </div>
    </div>
    <div class="002">
        <h3 class="002-title">От N-грамм к нейронным языковым моделям</h3>
        <div class="002-card">
            <h4>Эволюция стохастических моделей языка</h4>
            <p>Стохастические модели языка прошли значительную эволюцию от простых N-граммных моделей до сложных нейронных архитектур. Каждый новый подход расширял возможности моделей по учету контекста и улавливанию сложных языковых закономерностей.</p>
            <p>Современные нейронные языковые модели сохраняют стохастическую природу, но используют гораздо более сложные способы моделирования вероятностных распределений.</p>
            <p><strong>Эволюция стохастических моделей языка:</strong></p>
            <ul>
                <li>N-граммные модели (учитывают фиксированное количество предыдущих слов)</li>
                <li>Модели на основе скрытых марковских цепей</li>
                <li>Нейронные языковые модели с распределенными представлениями слов</li>
                <li>Рекуррентные нейронные сети для моделирования последовательностей</li>
                <li>Трансформерные архитектуры с механизмом внимания</li>
            </ul>
            <p><strong>Пояснение:</strong> Переход от классических N-граммных моделей к нейронным позволил преодолеть проблему разреженности данных и моделировать более длинные зависимости в тексте.</p>
        </div>
    </div>
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Тип стохастической модели</th>
                    <th>Особенности и ограничения</th>
                </tr>
            </thead>
            <tr>
                <td>N-граммные модели</td>
                <td>Учитывают только N-1 предыдущих слов, страдают от разреженности данных, требуют сглаживания для обработки редких последовательностей.</td>
            </tr>
            <tr>
                <td>Нейронные языковые модели</td>
                <td>Используют распределенные представления слов, лучше обобщают на редкие последовательности, могут учитывать более длинный контекст.</td>
            </tr>
            <tr>
                <td>Рекуррентные нейронные сети</td>
                <td>Теоретически могут учитывать неограниченный контекст, но на практике страдают от проблемы затухающего градиента при длинных последовательностях.</td>
            </tr>
            <tr>
                <td>Трансформерные модели</td>
                <td>Используют механизм внимания для прямого моделирования зависимостей между любыми позициями в последовательности, эффективно обрабатывают длинный контекст.</td>
            </tr>
        </table>
    </div>
    <div class="kmp13"><strong>Пример:</strong> В биграммной модели (N=2) вероятность предложения "Студенты изучают иностранные языки" будет рассчитана как произведение P(Студенты) × P(изучают|Студенты) × P(иностранные|изучают) × P(языки|иностранные), где каждая условная вероятность оценивается на основе частоты встречаемости соответствующих пар слов в обучающих данных.</div>
    <div class="kmp14"><strong>Важно:</strong> Даже самые современные нейронные языковые модели сохраняют стохастическую природу — они генерируют текст, выбирая каждый следующий токен на основе вероятностного распределения, что объясняет вариативность и иногда непредсказуемость их выходных данных.</div>
</section>

<section id="section6" class="section">
    <h2 class="section-title">6. Векторные представления</h2>
    <div class="001">
        <h3 class="001-title">Векторизация слов и текстов</h3>
        <div class="001-card">
            <h4>Представление языковых единиц в векторном пространстве</h4>
            <p>Векторные представления (embeddings) — это способ преобразования слов, фраз или более крупных текстовых единиц в числовые векторы фиксированной размерности. Такие представления позволяют моделям работать с текстом в математическом пространстве, где семантически близкие понятия располагаются рядом.</p>
            <p>Векторные представления являются фундаментальной концепцией в современной обработке естественного языка, позволяя преодолеть разрыв между символьной природой языка и числовыми вычислениями, необходимыми для машинного обучения.</p>
            <p><strong>Ключевые свойства векторных представлений:</strong></p>
            <ul>
                <li>Отражение семантических и синтаксических отношений между словами</li>
                <li>Возможность выполнения математических операций над словами (например, king - man + woman ≈ queen)</li>
                <li>Представление слов в пространстве с меньшей размерностью по сравнению с one-hot кодированием</li>
                <li>Способность к обобщению на редкие или даже неизвестные слова</li>
            </ul>
            <p><strong>Пояснение:</strong> Векторные представления преобразуют дискретные символы (слова) в непрерывные векторы, что позволяет измерять "близость" слов и выполнять над ними математические операции.</p>
        </div>
    </div>
    <div class="002">
        <h3 class="002-title">Методы создания векторных представлений</h3>
        <div class="002-card">
            <h4>Эволюция подходов к векторизации текста</h4>
            <p>Существует множество методов создания векторных представлений, от простых статистических подходов до сложных нейросетевых моделей. Каждый метод имеет свои особенности, преимущества и ограничения.</p>
            <p>Современные LLM используют контекстуальные векторные представления, которые учитывают окружающий контекст и могут представлять одно и то же слово по-разному в зависимости от его употребления.</p>
            <p><strong>Основные методы создания векторных представлений:</strong></p>
            <ul>
                <li>Частотные методы (TF-IDF, Bag of Words)</li>
                <li>Матрицы совместной встречаемости и их разложение (LSA, GloVe)</li>
                <li>Предиктивные модели (Word2Vec, FastText)</li>
                <li>Контекстуальные представления (ELMo, BERT, GPT)</li>
                <li>Мультимодальные представления (CLIP, DALL-E)</li>
            </ul>
            <p><strong>Пояснение:</strong> Эволюция методов векторизации шла в направлении все более точного учета контекста и семантики, от статических представлений слов к динамическим контекстуальным представлениям.</p>
        </div>
    </div>
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Метод векторизации</th>
                    <th>Описание и особенности</th>
                </tr>
            </thead>
            <tr>
                <td>One-hot encoding</td>
                <td>Простейший метод, где каждое слово представлено вектором размерности словаря с единицей в позиции слова и нулями в остальных позициях. Не отражает семантических отношений.</td>
            </tr>
            <tr>
                <td>Word2Vec</td>
                <td>Предиктивная модель, обучающаяся предсказывать слово по контексту (CBOW) или контекст по слову (Skip-gram). Создает статические векторные представления слов.</td>
            </tr>
            <tr>
                <td>GloVe</td>
                <td>Метод, основанный на матрице совместной встречаемости слов и глобальной статистике корпуса. Сочетает преимущества предиктивных и счетных методов.</td>
            </tr>
            <tr>
                <td>BERT embeddings</td>
                <td>Контекстуальные представления, где вектор слова зависит от его окружения в предложении. Учитывает полисемию и различные употребления слов.</td>
            </tr>
        </table>
    </div>
    <div class="kmp13"><strong>Пример:</strong> В пространстве векторных представлений Word2Vec слова "король" и "королева" будут расположены близко друг к другу, так же как "мужчина" и "женщина". При этом вектор "король" - "мужчина" + "женщина" будет близок к вектору "королева", что отражает семантические отношения между этими понятиями.</div>
    <div class="kmp14"><strong>Важно:</strong> Качество векторных представлений напрямую влияет на эффективность языковых моделей. Хорошие векторные представления должны улавливать семантические нюансы, учитывать контекст и адаптироваться к различным доменам и задачам.</div>
</section>

<section id="section7" class="section">
    <h2 class="section-title">7. Embeddings</h2>
    <div class="001">
        <h3 class="001-title">Понятие и роль эмбеддингов в LLM</h3>
        <div class="001-card">
            <h4>Что такое эмбеддинги и как они используются</h4>
            <p>Эмбеддинги (embeddings) — это специальный тип векторных представлений, которые преобразуют дискретные объекты (слова, предложения, документы) в непрерывные векторы в многомерном пространстве. В контексте LLM, эмбеддинги играют ключевую роль на всех этапах работы модели.</p>
            <p>Эмбеддинги позволяют моделям "понимать" семантические отношения между языковыми единицами и работать с текстом в математическом пространстве.</p>
            <p><strong>Функции эмбеддингов в LLM:</strong></p>
            <ul>
                <li>Преобразование токенов в векторы на входе модели</li>
                <li>Представление внутренних состояний и активаций нейронной сети</li>
                <li>Кодирование семантической информации в компактной форме</li>
                <li>Обеспечение переноса знаний между задачами и доменами</li>
            </ul>
            <p><strong>Пояснение:</strong> Эмбеддинги можно рассматривать как "координаты" слов или других языковых единиц в семантическом пространстве, где расстояния и направления имеют лингвистический смысл.</p>
        </div>
    </div>
    <div class="002">
        <h3 class="002-title">Типы и применение эмбеддингов</h3>
        <div class="002-card">
            <h4>Разнообразие эмбеддингов и их использование</h4>
            <p>Существуют различные типы эмбеддингов, которые отличаются по уровню представления (слова, предложения, документы), способу обучения и контекстуальности. Каждый тип имеет свои преимущества и области применения.</p>
            <p>Современные LLM используют и генерируют различные типы эмбеддингов для разных задач и уровней обработки текста.</p>
            <p><strong>Основные типы эмбеддингов:</strong></p>
            <ul>
                <li>Словесные эмбеддинги (представления отдельных слов)</li>
                <li>Контекстуальные эмбеддинги (зависят от окружающего контекста)</li>
                <li>Предложенческие эмбеддинги (представления целых предложений)</li>
                <li>Документные эмбеддинги (представления больших текстовых фрагментов)</li>
                <li>Мультимодальные эмбеддинги (объединяют текст с другими модальностями)</li>
            </ul>
            <p><strong>Пояснение:</strong> В отличие от статических словесных эмбеддингов (как Word2Vec), контекстуальные эмбеддинги в современных LLM динамически меняются в зависимости от контекста, в котором употребляется слово.</p>
        </div>
    </div>
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Применение эмбеддингов</th>
                    <th>Описание</th>
                </tr>
            </thead>
            <tr>
                <td>Семантический поиск</td>
                <td>Поиск документов или фрагментов текста по смыслу, а не по ключевым словам, с использованием близости векторов в пространстве эмбеддингов.</td>
            </tr>
            <tr>
                <td>Кластеризация и классификация</td>
                <td>Группировка и категоризация текстов на основе их векторных представлений, что позволяет выявлять тематические кластеры и паттерны.</td>
            </tr>
            <tr>
                <td>Рекомендательные системы</td>
                <td>Подбор релевантного контента на основе семантической близости векторных представлений текстов или пользовательских предпочтений.</td>
            </tr>
            <tr>
                <td>Retrieval-Augmented Generation (RAG)</td>
                <td>Улучшение генерации текста путем поиска и использования релевантной информации из внешних источников на основе векторной близости.</td>
            </tr>
        </table>
    </div>
    <div class="kmp13"><strong>Пример:</strong> Система семантического поиска может преобразовать запрос "как влияет изменение климата на биоразнообразие" в вектор и найти документы с близкими векторными представлениями, даже если они не содержат точных слов из запроса, но семантически связаны с темой.</div>
    <div class="kmp11"><strong>Примечание:</strong> Размерность эмбеддингов в современных LLM обычно составляет от нескольких сотен до нескольких тысяч, что позволяет кодировать богатую семантическую информацию, оставаясь вычислительно эффективными.</div>
</section>

<section id="section8" class="section">
    <h2 class="section-title">8. Нейронные сети</h2>
    <div class="001">
        <h3 class="001-title">Основы нейронных сетей</h3>
        <div class="001-card">
            <h4>Принципы работы искусственных нейронных сетей</h4>
            <p>Искусственные нейронные сети (ИНС) — это вычислительные модели, вдохновленные структурой и функционированием биологических нейронных сетей. Они состоят из взаимосвязанных узлов (нейронов), организованных в слои, и способны обучаться на данных, адаптируя свои параметры.</p>
            <p>Нейронные сети лежат в основе современных LLM и других систем искусственного интеллекта, обеспечивая их способность к обучению и обобщению.</p>
            <p><strong>Ключевые компоненты нейронных сетей:</strong></p>
            <ul>
                <li>Нейроны (узлы), выполняющие вычисления</li>
                <li>Веса связей между нейронами, которые настраиваются в процессе обучения</li>
                <li>Функции активации, вносящие нелинейность</li>
                <li>Слои нейронов (входной, скрытые, выходной)</li>
                <li>Алгоритмы обучения (обратное распространение ошибки)</li>
            </ul>
            <p><strong>Пояснение:</strong> Нейронные сети обучаются путем корректировки весов связей между нейронами на основе ошибки между предсказанными и фактическими значениями, постепенно минимизируя эту ошибку.</p>
        </div>
    </div>
    <div class="002">
        <h3 class="002-title">Нейронные сети в обработке естественного языка</h3>
        <div class="002-card">
            <h4>Применение нейронных сетей для языковых задач</h4>
            <p>В области обработки естественного языка (NLP) нейронные сети произвели революцию, значительно превзойдя традиционные статистические методы. Различные архитектуры нейронных сетей нашли применение в разных языковых задачах.</p>
            <p>Эволюция нейронных сетей в NLP шла от простых feed-forward сетей к более сложным рекуррентным и трансформерным архитектурам, способным эффективно обрабатывать последовательные данные.</p>
            <p><strong>Ключевые типы нейронных сетей в NLP:</strong></p>
            <ul>
                <li>Полносвязные (feed-forward) нейронные сети</li>
                <li>Рекуррентные нейронные сети (RNN, LSTM, GRU)</li>
                <li>Сверточные нейронные сети (CNN) для обработки текста</li>
                <li>Трансформеры и сети с механизмом внимания</li>
                <li>Автоэнкодеры и вариационные автоэнкодеры</li>
            </ul>
            <p><strong>Пояснение:</strong> Каждый тип нейронных сетей имеет свои преимущества: рекуррентные сети хорошо обрабатывают последовательности, сверточные эффективны для выделения локальных паттернов, а трансформеры могут параллельно обрабатывать длинные последовательности.</p>
        </div>
    </div>
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Тип нейронной сети</th>
                    <th>Применение в NLP</th>
                </tr>
            </thead>
            <tr>
                <td>Полносвязные сети</td>
                <td>Классификация текстов, простые языковые модели, обработка векторных представлений слов.</td>
            </tr>
            <tr>
                <td>Рекуррентные сети (RNN/LSTM/GRU)</td>
                <td>Моделирование последовательностей, машинный перевод, распознавание именованных сущностей, генерация текста.</td>
            </tr>
            <tr>
                <td>Сверточные сети (CNN)</td>
                <td>Классификация текстов, выделение ключевых фраз, анализ тональности, извлечение признаков из текста.</td>
            </tr>
            <tr>
                <td>Трансформеры</td>
                <td>Современные языковые модели (GPT, BERT), машинный перевод, вопросно-ответные системы, генерация текста.</td>
            </tr>
        </table>
    </div>
	</section>
            </div>
<footer class="footer">
<div class="container">
<p>© 2025 | kmp | CC BY-NC-SA 4.0<br>
Разработано для студентов БрГУ имени А.С. Пушкина</p>
</div>
</footer>
<div style="position: fixed; bottom: 10px; color: #777777; right: 30px; opacity: 0.3; font-size: 14px;">kmp+</div>
    <script>
        // Функция для плавной прокрутки к разделу
        function scrollToSection(sectionId) {
            const section = document.getElementById(sectionId);
            const menuHeight = document.querySelector('.menu').offsetHeight;
            
            window.scrollTo({
                top: section.offsetTop - menuHeight - 20,
                behavior: 'smooth'
            });
        }

        // Функция для переключения темы
        document.getElementById('themeToggle').addEventListener('click', function() {
            const currentTheme = document.documentElement.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            
            document.documentElement.setAttribute('data-theme', newTheme);
            this.textContent = newTheme === 'dark' ? '🌙' : '☀️';
        });

        // Анимация появления секций при прокрутке
        document.addEventListener('DOMContentLoaded', function() {
            const sections = document.querySelectorAll('.section');
            
            const observerOptions = {
                root: null,
                rootMargin: '0px',
                threshold: 0.1
            };
            
            const observer = new IntersectionObserver(function(entries, observer) {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('fade-in');
                        observer.unobserve(entry.target);
                    }
                });
            }, observerOptions);
            
            sections.forEach(section => {
                section.classList.remove('fade-in');
                observer.observe(section);
            });
        });
    </script>
</body>
</html>