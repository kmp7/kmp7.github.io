<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>kmp+</title>
    <style>
        /* Основные стили */
        :root {
            --primary-color: #325980;
            --secondary-color: #4CAF50;
            --background-color: #f5f5f5;
            --content-bg: #ffffff;
            --text-color: #333333;
            --header-text-color: #ffffff;
            --menu-bg: #ffffff;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            --border-radius: 8px;
			  --accent11: #4caf50;
		--accent12: #4cafff;
		--accent13: #ffaf50;
		--accent14: #821978;
        }

        /* Темная тема */
        [data-theme="dark"] {
            --primary-color: #3e76ad;
            --secondary-color: #388e3c;
            --background-color: #000000;
            --content-bg: #1e1e1e;
            --text-color: #e0e0e0;
            --header-text-color: #ffffff;
            --menu-bg: #000000;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            transition: background-color 0.3s, color 0.3s;
        }

        body {
            font-family: 'Roboto', 'Arial', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--background-color);
            padding-top: 2px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        header {
            background-color: var(--primary-color);
            color: var(--header-text-color);
            padding: 20px 0;
            text-align: center;
            border-radius: var(--border-radius);
            margin-bottom: 2px;
        }

        h1 {
            font-size: 2.2rem;
            margin-bottom: 10px;
        }

        h2 {
            color: var(--primary-color);
            margin: 25px 0 15px;
            padding-bottom: 8px;
            border-bottom: 2px solid var(--secondary-color);
        }

        h3 {
            color: var(--primary-color);
            margin: 20px 0 10px;
        }

        p {
            margin-bottom: 15px;
        }

        /* Меню навигации */
        .menu {
            background-color: var(--menu-bg);
            padding: 15px 20px;
            border-radius: var(--border-radius);
            margin-bottom: 30px;
            box-shadow: var(--menu-shadow);
            display: flex;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .menu-buttons {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }
		
		
		.menu-buttons {
    display: flex;
    flex-wrap: wrap;
    gap: 10px;
    justify-content: center; /* Изменено на center */
}

        .menu-btn {
            background-color: var(--primary-color);
            color: white;
            border: none;
            padding: 8px 15px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: background-color 0.3s;
        }

        .menu-btn:hover {
            background-color: var(--secondary-color);
        }

        .theme-toggle {
            background: none;
            border: 10px solid transparent;
            font-size: 1.5rem;
            cursor: pointer;
            color: var(--primary-color);
        }

        /* Секции контента */
        .section {
            background-color: var(--content-bg);
            border-radius: var(--border-radius);
            padding: 25px;
            margin-bottom: 30px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        /* Выделение важного */
        .important {
            background-color: rgba(76, 175, 80, 0.1);
            border-left: 4px solid var(--secondary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        .note {
            background-color: rgba(50, 89, 128, 0.1);
            border-left: 4px solid var(--primary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        /* Таблицы */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            padding: 12px 15px;
            border: 1px solid #ddd;
            text-align: left;
        }

        th {
            background-color: var(--primary-color);
            color: white;
        }

        tr:nth-child(even) {
            background-color: rgba(0, 0, 0, 0.03);
        }

        /* Списки */
        ul, ol {
            padding-left: 25px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
        }

        /* Адаптивный дизайн */
        @media (max-width: 768px) {
            h1 {
                font-size: 1.8rem;
            }
            
            h2 {
                font-size: 1.5rem;
            }
            
            .menu {
                flex-direction: column;
                gap: 15px;
            }
            
            .menu-buttons {
                width: 100%;
                justify-content: center;
            }
            
            .theme-toggle {
                margin-top: 10px;
            }
            
            .section {
                padding: 15px;
            }
        }

        /* Анимации */
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .fade-in {
            animation: fadeIn 0.5s ease-in;
        }

        footer {
            text-align: center;
            padding: 20px 0;
            margin-top: 40px;
            font-size: 0.9rem;
        }
		
					.kmp11, .example {
      background: rgba(76, 175, 80, 0.1);
      border-left: 4px solid var(--accent11);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp12, .example {
      background: rgba(95, 182, 237, 0.1);
      border-left: 4px solid var(--accent12);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
 
		.kmp13, .example {
      background: rgba(205, 170, 110, 0.1);
      border-left: 4px solid var(--accent13);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp14, .example {
      background: rgba(205, 110, 200, 0.1);
      border-left: 4px solid var(--accent14);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
	
	
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Современная компьютерная лингвистика</h1>
            <p>как новая модель (карта) реальности (территории) языка</p>
        </header>

        <nav class="menu">
            <div class="menu-buttons">
                <button class="menu-btn" onclick="scrollToSection('1')">R vs. M</button>
                <button class="menu-btn" onclick="scrollToSection('2')">Tradition</button>
                <button class="menu-btn" onclick="scrollToSection('3')">AI First</button>
                <button class="menu-btn" onclick="scrollToSection('4')">CCPS</button>
				<button class="menu-btn" onclick="scrollToSection('5')">LLM</button>
				<button class="menu-btn" onclick="scrollToSection('6')">Addit</button>
				<button class="menu-btn" onclick="scrollToSection('7')">Summary</button>
                 </div>
            <button class="theme-toggle" id="themeToggle" title="Переключить тему">☀️</button>
        </nav>


<section id="1" class="section">
    <h2 class="section-title">Карта и территория: модели — не язык</h2>
            <div class="001">
        <h3 class="001-title">Принцип Коржибски и афоризм Бокса</h3>
               <div class="001-card">
            <p>В 1931 году польско-американский учёный Альфред Коржибски сформулировал принцип, ставший одним из фундаментальных в эпистемологии: <strong>Карта — не территория (The map is not the territory)</strong>. Любая модель, схема, описание реальности — это лишь упрощённое представление, а не сама реальность. Карта полезна для навигации, но она неизбежно что-то опускает, что-то искажает, а что-то подчёркивает в ущерб другому.</p>
            <p>Полвека спустя британский статистик Джордж Бокс дополнил эту мысль знаменитым афоризмом: <em>«Все модели неверны, но некоторые полезны»</em> (All models are wrong, but some are useful). Бокс имел в виду статистические модели, но его слова оказались пророческими для всей науки о языке.</p>
            <p>Эти два принципа составляют эпистемологическую рамку нашего курса. Мы изучаем не «истинное устройство языка» — мы изучаем <strong>модели языка</strong>, созданные людьми и машинами, оцениваем их полезность в разных контекстах и учимся выбирать подходящую карту для конкретной задачи.</p>
           <p><strong>Ключевые понятия:</strong></p>
                <ul>
                    <li><strong>Территория</strong> — реальный живой язык во всей его сложности, изменчивости и контекстуальности: то, как люди действительно говорят, пишут, думают и понимают друг друга.</li>
                    <li><strong>Карта</strong> — любая модель языка: грамматика, словарь, лингвистическая теория, формальная грамматика, нейросетевая языковая модель (LLM).</li>
                    <li><strong>Полезность модели</strong> — степень, в которой модель помогает решать конкретные задачи: описывать, предсказывать, генерировать, обучать, переводить.</li>
                </ul>
                <p><strong>Пояснение:</strong> Когда мы говорим, что модель «неверна», мы не обесцениваем её. Мы признаём фундаментальное ограничение любого моделирования: полная модель языка была бы самим языком. Задача лингвиста — не искать «единственно верную» модель, а владеть репертуаром моделей и понимать границы применимости каждой из них.</p>
        </div>
    </div>
    <div class="kmp12"><strong>Важно:</strong> Принцип «карта — не территория» не означает, что карты бесполезны. Напротив, без карт навигация невозможна. Но путать карту с территорией опасно: это ведёт к догматизму, прескриптивизму и слепоте к реальным языковым явлениям, не укладывающимся в привычную схему.</div>
</section>

<section id="2" class="section">
    <h2 class="section-title">Традиционная лингвистика: великие карты и их границы</h2>
            <div class="001">
        <h3 class="001-title">Достижения традиционной лингвистики</h3>
               <div class="001-card">
            <p>Традиционная лингвистика — от древнеиндийского грамматиста Панини до структурализма Соссюра, генеративной грамматики Хомского и функционализма — создала мощный арсенал моделей, которые на протяжении столетий служили (и продолжают служить) важнейшим инструментом понимания языка.</p>
            <p>Эти модели дали человечеству системы письменности, грамматические описания тысяч языков, типологические классификации, теории синтаксиса и семантики, методы исторической реконструкции, словари, учебники и стандарты грамотности. Без них не было бы ни школьного образования, ни литературных норм, ни самой возможности осмысленно говорить о языке.</p>
           <p><strong>Ключевые «карты» традиционной лингвистики:</strong></p>
                <ul>
                    <li><strong>Фонология:</strong> модель звуковой системы языка — фонемы, аллофоны, фонологические правила.</li>
                    <li><strong>Морфология:</strong> модель строения слов — морфемы, словообразование, словоизменение, парадигмы.</li>
                    <li><strong>Синтаксис:</strong> модель строения предложений — части речи, члены предложения, синтаксические деревья.</li>
                    <li><strong>Семантика:</strong> модель значения — компонентный анализ, семантические поля, тезаурусы.</li>
                    <li><strong>Прагматика:</strong> модель использования языка в контексте — речевые акты, импликатуры, дискурс.</li>
                </ul>
                <p><strong>Пояснение:</strong> Каждая из этих «карт» выделяет один аспект языковой территории и описывает его с помощью дискретных категорий, правил и исключений. Это их сила — и одновременно источник ограничений.</p>
        </div>
    </div>
            <div class="001">
        <h3 class="001-title">Ловушка прескриптивизма: карта подменяет территорию</h3>
               <div class="001-card">
            <p>Прескриптивизм — это подход, при котором лингвистическая модель (карта) начинает восприниматься как норма, которой должна подчиняться реальность языка (территория). Вместо того чтобы описывать, как люди говорят, прескриптивист предписывает, как они <em>должны</em> говорить.</p>
            <p>В определённых контекстах прескриптивизм необходим и полезен. Школьное образование невозможно без нормативной грамматики. Юридический, медицинский, деловой дискурсы требуют стандартизации. Литературная норма обеспечивает взаимопонимание между носителями разных диалектов.</p>
            <p>Однако когда прескриптивизм выходит за пределы своей зоны полезности, он становится ловушкой. Лингвист, убеждённый, что его модель и есть язык, перестаёт видеть реальные языковые явления: продуктивные неологизмы объявляются «ошибками», живые грамматические изменения — «порчей языка», а миллионы носителей — «неграмотными».</p>
           <p><strong>Примеры подмены территории картой:</strong></p>
                <ul>
                    <li>Утверждение, что «кофе» может быть только мужского рода, хотя большинство носителей русского языка используют средний род, следуя продуктивной модели (ср. «какао», «метро»).</li>
                    <li>Осуждение конструкции «их» в значении притяжательного местоимения («их дом»), которая естественна и однозначна.</li>
                    <li>Запрет на начало предложения с «но» или «и» в английском языке — правило, которое не соблюдали ни Шекспир, ни Библия короля Якова.</li>
                    <li>Объявление двойного отрицания «ошибкой» в английском, хотя оно нормативно в десятках других языков, включая русский, французский, испанский.</li>
                </ul>
                <p><strong>Пояснение:</strong> Прескриптивизм полезен как инструмент стандартизации в конкретных контекстах (образование, делопроизводство). Он вреден, когда претендует на описание «истинного» языка, объявляя живую языковую практику миллионов людей «неправильной». Дескриптивизм — описание реального употребления — ближе к территории, но тоже остаётся картой.</p>
        </div>
    </div>
            <div class="001">
        <h3 class="001-title">Фундаментальные ограничения традиционных моделей</h3>
               <div class="001-card">
            <p>Традиционные лингвистические модели строятся на нескольких базовых допущениях, которые упрощают реальность языка. Эти упрощения были необходимы в доцифровую эпоху, но сегодня мы можем увидеть их как ограничения — и преодолеть их с помощью новых подходов.</p>
           <p><strong>Основные упрощения традиционных моделей:</strong></p>
                <ul>
                    <li><strong>Дискретность:</strong> язык описывается через чёткие категории (существительное / глагол, подлежащее / сказуемое, правильно / неправильно). В реальности границы размыты: «бег» — это существительное или глагол? «Светает» — есть ли тут подлежащее? «Норм» — это наречие, частица или предикатив?</li>
                    <li><strong>Контекстонезависимость:</strong> значения слов описываются «в словаре», как если бы они существовали вне контекста. В реальности значение слова конструируется в контексте каждого конкретного употребления.</li>
                    <li><strong>Детерминизм:</strong> правила грамматики формулируются как жёсткие: «если X, то Y». В реальности язык вероятностен: «если X, то чаще Y, но иногда Z, а в контексте W — почти всегда Q».</li>
                    <li><strong>Статичность:</strong> модели фиксируют язык в определённый момент (синхрония). Реальный язык непрерывно меняется, и границы между «нормой» и «отклонением» подвижны.</li>
                    <li><strong>Интроспекция:</strong> данные часто получаются через самонаблюдение лингвиста или опрос небольшого числа информантов. Реальный узус миллионов носителей может радикально отличаться от интуиции эксперта.</li>
                </ul>
        </div>
    </div>
<div class="table-kmp">
<table class="table">
<thead><tr><th>Свойство традиционной модели</th><th>Реальность языка (территория)</th></tr></thead>
 <tr><td>Дискретные категории</td><td>Континуальные, градиентные явления</td></tr>
 <tr><td>Фиксированные значения слов</td><td>Контекстуально-зависимые значения</td></tr>
 <tr><td>Детерминированные правила</td><td>Вероятностные тенденции</td></tr>
 <tr><td>Синхронный срез</td><td>Непрерывная эволюция</td></tr>
 <tr><td>Данные интроспекции</td><td>Массовый узус миллиардов высказываний</td></tr>
 <tr><td>Бинарная грамматичность (да/нет)</td><td>Градиент приемлемости</td></tr>
</table>
</div>
    <div class="kmp11"><strong>Примечание:</strong> Ограничения традиционных моделей — это не недостаток лингвистов прошлого, а следствие технологических и эпистемологических рамок их эпохи. У них не было возможности обработать миллиарды текстов. Их модели были лучшими картами, которые можно было создать вручную. Сегодня у нас есть новые инструменты — и мы можем строить карты нового типа, не отбрасывая старые.</div>
</section>

<section id="" class="section">
    <h2 class="section-title">Новая парадигма моделирования языка</h2>
            <div class="001">
        <h3 class="001-title">AI First</h3>
               <div class="001-card">
            <p>Парадигма <strong>AI First</strong> — это подход, при котором искусственный интеллект (и прежде всего большие языковые модели — LLM) рассматривается не как вспомогательный инструмент, а как <em>первичная среда</em> исследования, моделирования и взаимодействия с языком.</p>
            <p>Это не означает, что AI заменяет лингвиста. Это означает, что лингвист XXI века работает <em>с</em> AI и <em>через</em> AI — точно так же, как астроном работает с телескопом, а биолог — с микроскопом. LLM — это новый инструмент наблюдения и моделирования языковой территории, обладающий беспрецедентными возможностями.</p>
            <p>В парадигме AI First меняется не только инструментарий, но и сама постановка вопросов. Вместо «какому правилу подчиняется это явление?» мы спрашиваем: «какова вероятность этого явления в данном контексте?» Вместо «правильно ли это?» — «насколько это типично, и для каких контекстов?»</p>
           <p><strong>Ключевые принципы парадигмы AI First в лингвистике:</strong></p>
                <ul>
                    <li><strong>Данные прежде теории:</strong> сначала наблюдаем массивы реального языка, затем выявляем закономерности, а не подгоняем данные под заранее выбранную теорию.</li>
                    <li><strong>Вероятность прежде правила:</strong> языковые явления описываются через распределения вероятностей, а не через бинарные правила «можно / нельзя».</li>
                    <li><strong>Контекст прежде словаря:</strong> значение слова — это функция контекста, а не фиксированная запись в словаре.</li>
                    <li><strong>Масштаб прежде примера:</strong> модели обучаются на миллиардах высказываний, а не на сотнях примеров, отобранных лингвистом.</li>
                    <li><strong>Генерация как проверка:</strong> способность модели порождать естественный текст — это эмпирическая проверка качества моделирования.</li>
                </ul>
        </div>
    </div>
            <div class="001">
        <h3 class="001-title">LLM: большая языковая модель</h3>
               <div class="001-card">
            <p><strong>LLM (Large Language Model)</strong> — большая языковая модель — это нейросетевая модель, обученная на огромных массивах текстовых данных предсказывать следующий токен (слово, часть слова) в последовательности. При всей кажущейся простоте этой задачи, её решение на масштабе триллионов токенов приводит к возникновению удивительных свойств: модель «выучивает» грамматику, семантику, прагматику, факты о мире, стили общения и логические рассуждения — без явного программирования правил.</p>
            <p>LLM — это статистическая карта языка, построенная не лингвистом, а алгоритмом на основе реальных текстов. Это принципиально новый тип модели: она не содержит явных правил, но демонстрирует поведение, совместимое с правилами (и часто — более тонкое, чем эксплицитные грамматики).</p>
           <p><strong>Архитектурные основы современных LLM:</strong></p>
                <ul>
                    <li><strong>Трансформер (Transformer):</strong> архитектура нейросети, предложенная в 2017 году (Vaswani et al., «Attention Is All You Need»), основанная на механизме внимания (attention), позволяющем модели учитывать связи между любыми позициями в тексте.</li>
                    <li><strong>Токенизация:</strong> разбиение текста на субсловные единицы (токены), что позволяет модели работать с любыми словами, включая незнакомые, составные и многоязычные.</li>
                    <li><strong>Предобучение (pretraining):</strong> обучение на огромном корпусе текстов без разметки — модель учится предсказывать следующий токен.</li>
                    <li><strong>Дообучение (fine-tuning) и выравнивание (alignment):</strong> адаптация модели к конкретным задачам и человеческим предпочтениям с помощью инструкций, обратной связи, RLHF.</li>
                    <li><strong>Эмерджентные способности:</strong> при увеличении масштаба модели появляются способности, которые не были явно запрограммированы — аналогия, рассуждение, перевод, суммаризация.</li>
                </ul>
                <p><strong>Пояснение:</strong> LLM — это не «база знаний» и не «набор правил». Это <em>вероятностная модель языковой компетенции</em>, усвоенная из реальных данных. Она ближе к тому, как дети осваивают язык (через массовое воздействие реального употребления), чем к тому, как лингвист описывает язык (через анализ и формулировку правил). Впрочем, и эта аналогия — тоже модель, тоже неверная, но полезная.</p>
        </div>
    </div>
    <div class="kmp12"><strong>Важно:</strong> LLM — это не «искусственный разум» и не «цифровой лингвист». Это вычислительный артефакт, который моделирует статистические закономерности в текстовых данных. Но именно потому, что язык пронизывает всё человеческое знание, эта статистическая модель оказывается удивительно мощным инструментом — и для лингвистики, и далеко за её пределами.</div>
</section>

<section id="4" class="section">
    <h2 class="section-title">Cвойства LLM-моделей языка: континуальность, контекстуальность, вероятностность, стохастичность</h2>
            <div class="001">
        <h3 class="001-title">Континуальность: от дискретных категорий к непрерывным пространствам</h3>
               <div class="001-card">
            <p>Традиционная лингвистика оперирует <strong>дискретными категориями</strong>: слово — либо существительное, либо глагол; предложение — либо грамматично, либо нет; значение — либо «А», либо «Б». Эта дискретность удобна для описания, но плохо отражает реальность.</p>
            <p>LLM работают в <strong>непрерывных векторных пространствах</strong>. Каждое слово, каждый фрагмент текста представляется вектором — точкой в многомерном пространстве. В этом пространстве нет жёстких границ между категориями: «бег» занимает позицию <em>между</em> типичными существительными и типичными глаголами, а не в одной из двух ячеек. Грамматичность предложения — это не бинарный признак, а положение на шкале.</p>
            <p>Такое представление позволяет улавливать <strong>градиентные</strong> явления, которые традиционная лингвистика вынуждена описывать через громоздкие системы исключений и оговорок. Это не значит, что дискретные категории «неверны» — они полезны как ориентиры на карте. Но территория языка континуальна.</p>
           <p><strong>Примеры континуальности в языке:</strong></p>
                <ul>
                    <li><strong>Части речи:</strong> «Бег», «бегство», «беганье», «бегающий», «бегая», «бежать» — эти формы образуют градиент от «чистого» существительного к «чистому» глаголу, а не два дискретных класса.</li>
                    <li><strong>Грамматичность:</strong> «Мне нравится этот фильм» → «Мне нравится этот кино» → «Мне нравится этот шкаф фильм» — степень «неправильности» нарастает плавно, а не скачком.</li>
                    <li><strong>Синонимия:</strong> «большой» — «крупный» — «огромный» — «гигантский» — «колоссальный» — это не набор эквивалентов, а точки в пространстве с разными расстояниями друг от друга.</li>
                </ul>
                <p><strong>Пояснение:</strong> В LLM континуальность реализуется через эмбеддинги (embeddings) — числовые векторы, представляющие слова, предложения и тексты. Расстояние между векторами отражает семантическую близость. Это математически точная реализация интуиции о том, что языковые явления градиентны.</p>
        </div>
    </div>
            <div class="001">
        <h3 class="001-title">Контекстуальность: значение рождается в контексте</h3>
               <div class="001-card">
            <p>Один из главных уроков, который LLM преподносят лингвистике, — это радикальная <strong>контекстуальность значения</strong>. В традиционной лексикографии слово имеет «значения», перечисленные в словаре: «коса» — 1) причёска, 2) инструмент, 3) песчаная отмель. Но в реальности значение не «выбирается» из списка — оно <em>конструируется</em> каждый раз заново, в каждом конкретном контексте.</p>
            <p>В современных LLM (начиная с модели BERT, 2018) каждое слово получает <strong>контекстуальное представление</strong>: вектор слова «коса» в предложении «Она заплела косу» радикально отличается от вектора того же слова в «Он наточил косу». Это не два фиксированных значения — это непрерывный спектр контекстуально обусловленных представлений.</p>
           <p><strong>Что это меняет для лингвистики:</strong></p>
                <ul>
                    <li>Полисемия — это не «проблема», которую надо решить, а <em>нормальное состояние</em> языка: почти каждое слово меняет своё значение от контекста к контексту.</li>
                    <li>Граница между полисемией (одно слово — несколько значений) и омонимией (разные слова, совпавшие по форме) размывается: это не два класса, а континуум.</li>
                    <li>«Значение слова» в словаре — это полезная абстракция (карта), но не реальность (территория). Реальное значение — всегда значение-в-контексте.</li>
                    <li>Метафора, ирония, языковая игра — не «отклонения» от «буквального значения», а нормальные режимы работы языковой системы, которые LLM моделируют естественным образом.</li>
                </ul>
                <p><strong>Пояснение:</strong> Идея контекстуальности значения не нова — её высказывали Витгенштейн («значение слова — это его употребление»), Фёрс («You shall know a word by the company it keeps»), а также дистрибутивная семантика Харриса. Но только LLM реализовали эту идею вычислительно, на масштабе всех языков и всех контекстов.</p>
        </div>
    </div>
            <div class="001">
        <h3 class="001-title">Вероятностность: язык как распределение, а не набор правил</h3>
               <div class="001-card">
            <p>Традиционная грамматика формулирует правила: «после предлога «в» существительное стоит в винительном или предложном падеже». LLM-подход описывает то же самое через <strong>распределения вероятностей</strong>: после «в» + существительное модель присваивает высокую вероятность формам винительного и предложного падежей, низкую — дательному, и почти нулевую — именительному. Но «почти нулевая» — не ноль: в поэзии, в диалекте, в языковой игре возможно всё.</p>
            <p>Такой подход имеет фундаментальное преимущество: он <strong>не разделяет</strong> «правило» и «исключение», «норму» и «вариант». Всё — вероятности. Частотные паттерны — это и есть «грамматика» в вероятностном смысле. Редкие паттерны — это не «ошибки», а хвост распределения.</p>
           <p><strong>Что это меняет для лингвистики:</strong></p>
                <ul>
                    <li><strong>Грамматичность — это градиент, а не бинарный признак.</strong> Предложение может быть «более грамматичным» или «менее грамматичным», а не просто «правильным» или «неправильным».</li>
                    <li><strong>Вариативность — это норма, а не исключение.</strong> «Звонит» и «звонит» — это не «правильный» и «неправильный» вариант, а два варианта с разными вероятностями в разных социолингвистических контекстах.</li>
                    <li><strong>Продуктивность конструкций имеет количественную меру.</strong> Можно измерить, насколько свободно заполняется «слот» в конструкции, а не просто констатировать «продуктивна / непродуктивна».</li>
                    <li><strong>Языковое изменение — это сдвиг вероятностей.</strong> Когда новая форма появляется и распространяется, её вероятность плавно растёт — и это можно отследить по данным.</li>
                </ul>
        </div>
    </div>
            <div class="001">
        <h3 class="001-title">Стохастичность: творчество через случайность</h3>
               <div class="001-card">
            <p><strong>Стохастичность</strong> (от греч. στοχαστικός — «умеющий угадывать») — это свойство LLM порождать <em>разные</em> ответы на один и тот же запрос. Каждый раз, когда модель генерирует текст, она «бросает взвешенный кубик» — выбирает следующий токен случайно, но с учётом вероятностного распределения.</p>
            <p>Это не «баг» — это фундаментальное свойство, которое отражает важную истину о языке. Реальный человеческий язык тоже стохастичен: один и тот же человек, отвечая на один и тот же вопрос в двух ситуациях, сформулирует ответ по-разному. Стохастичность — это источник вариативности, креативности, стилистического разнообразия.</p>
           <p><strong>Параметр temperature в LLM:</strong></p>
                <ul>
                    <li><strong>Низкая temperature (0.0–0.3):</strong> модель выбирает наиболее вероятные токены — выход предсказуемый, «безопасный», стереотипный. Аналогия: формальное деловое письмо.</li>
                    <li><strong>Средняя temperature (0.5–0.8):</strong> баланс между предсказуемостью и разнообразием. Аналогия: живая разговорная речь.</li>
                    <li><strong>Высокая temperature (0.9–1.5):</strong> модель активно исследует маловероятные продолжения — выход творческий, неожиданный, иногда абсурдный. Аналогия: поэтический эксперимент, языковая игра.</li>
                </ul>
                <p><strong>Пояснение:</strong> Стохастичность LLM позволяет моделировать один из самых загадочных аспектов языка — его <em>творческий потенциал</em>, способность бесконечно порождать новые высказывания, никогда ранее не произносившиеся. Традиционная лингвистика описывала эту способность теоретически (как «порождающую способность» грамматики у Хомского), но не могла её воспроизвести. LLM воспроизводят — стохастически.</p>
        </div>
    </div>
<div class="table-kmp">
<table class="table">
<thead><tr><th>Свойство</th><th>Традиционная модель</th><th>LLM-модель</th></tr></thead>
 <tr><td>Представление категорий</td><td>Дискретные символы, метки, классы</td><td>Непрерывные векторы в многомерном пространстве</td></tr>
 <tr><td>Работа со значением</td><td>Фиксированные значения в словаре</td><td>Контекстуальные эмбеддинги, меняющиеся от контекста к контексту</td></tr>
 <tr><td>Описание закономерностей</td><td>Детерминированные правила и исключения</td><td>Вероятностные распределения</td></tr>
 <tr><td>Порождение текста</td><td>Детерминированное применение правил</td><td>Стохастическая выборка из распределения</td></tr>
 <tr><td>Источник данных</td><td>Интроспекция, примеры, корпуса (тысячи–миллионы слов)</td><td>Массивные корпуса (триллионы токенов)</td></tr>
 <tr><td>Отношение к вариативности</td><td>Норма vs. отклонение</td><td>Разные точки на шкале вероятности</td></tr>
</table>
</div>
    <div class="kmp14"><strong>Пояснение:</strong> Четыре свойства LLM-моделей — континуальность, контекстуальность, вероятностность и стохастичность — не являются техническими деталями. Это эпистемологические сдвиги, меняющие наше понимание природы языка. Каждое из них соответствует реальному свойству языковой территории, которое было трудно или невозможно отразить на старых картах.</div>
</section>

<section id="5" class="section">
    <h2 class="section-title">LLM-лингвистика</h2>
            <div class="001">
        <h3 class="001-title">Фронтир современной компьютерной лингвистикиLLM-лингвистика</h3>
               <div class="001-card">
            <p><strong>LLM-лингвистика</strong> — это формирующееся направление на пересечении лингвистики, компьютерных наук и когнитивистики, которое использует большие языковые модели как инструмент, объект и среду лингвистического исследования.</p>
            <p>LLM-лингвистика работает в трёх режимах:</p>
           <p><strong>Три режима LLM-лингвистики:</strong></p>
                <ul>
                    <li><strong>LLM как инструмент (tool):</strong> использование языковых моделей для решения лингвистических задач — аннотирование корпусов, классификация текстов, извлечение информации, машинный перевод, создание обучающих материалов.</li>
                    <li><strong>LLM как объект (object):</strong> исследование самих языковых моделей как моделей языка — что они «знают» о грамматике? как представляют семантику? какие лингвистические обобщения возникают в них спонтанно? где они ошибаются — и что эти ошибки говорят о языке?</li>
                    <li><strong>LLM как среда (medium):</strong> использование LLM как среды для лингвистических экспериментов — «что будет, если изменить контекст?», «как модель обрабатывает метафору?», «как меняется выход при переключении языка?» Модель становится лабораторией, в которой можно проводить контролируемые эксперименты с языком.</li>
                </ul>
                <p><strong>Пояснение:</strong> LLM-лингвистика не «отменяет» ни фонетику, ни синтаксис, ни семантику, ни прагматику. Она предлагает новую перспективу на все эти области и новые методы их исследования. Точно так же изобретение микроскопа не «отменило» биологию — оно открыло новый уровень наблюдения и породило новые разделы науки.</p>
        </div>
    </div>
            <div class="001">
        <h3 class="001-title">Новые вопросы, которые ставит LLM-лингвистика</h3>
               <div class="001-card">
            <p>LLM-лингвистика не просто даёт новые ответы на старые вопросы. Она ставит <strong>принципиально новые вопросы</strong>, которые невозможно было даже сформулировать в рамках традиционной парадигмы:</p>
           <p><strong>Примеры новых вопросов:</strong></p>
                <ul>
                    <li>Если LLM порождает грамматичный текст без явных грамматических правил — что это говорит о природе грамматики? Может ли грамматика быть <em>эмерджентным</em> свойством статистических закономерностей?</li>
                    <li>Если LLM обучена только на тексте (без звука, изображений, телесного опыта), но демонстрирует признаки «понимания» — что это говорит о связи языка и мышления? О гипотезе Сепира—Уорфа? О дискуссии между эмпиризмом и нативизмом?</li>
                    <li>LLM усваивают «грамматику» из данных без врождённых языковых структур. Подрывает ли это гипотезу Хомского об универсальной грамматике — или подтверждает её (структуры возникают, пусть и другим путём)?</li>
                    <li>Если одна и та же архитектура (трансформер) успешно моделирует тысячи языков — что это говорит о языковых универсалиях?</li>
                    <li>Как LLM обрабатывают метафору, иронию, импликатуру? Что паттерны их «ошибок» говорят о прагматических аспектах языка?</li>
                    <li>Если LLM может порождать текст, неотличимый от человеческого, — как переопределяется понятие «авторство»? «Текст»? «Язык»?</li>
                </ul>
        </div>
    </div>
            <div class="001">
        <h3 class="001-title">Практические приложения LLM-лингвистики</h3>
               <div class="001-card">
            <p>LLM-лингвистика — это не только теоретический фронтир, но и область с мощными практическими приложениями, которые определяют рынок труда для лингвистов сегодня и в обозримом будущем:</p>
           <p><strong>Области применения:</strong></p>
                <ul>
                    <li><strong>Промпт-инженерия и промпт-дизайн:</strong> составление эффективных инструкций для LLM — задача, требующая глубокого понимания прагматики, риторики, жанровых конвенций.</li>
                    <li><strong>Оценка и выравнивание моделей:</strong> лингвисты оценивают качество порождаемого текста, выявляют ошибки, предвзятости, стилистические несоответствия — и помогают улучшать модели.</li>
                    <li><strong>Создание данных для обучения:</strong> подготовка, разметка и курирование датасетов — задача, требующая лингвистической экспертизы.</li>
                    <li><strong>Компьютерная лексикография нового поколения:</strong> создание словарей на основе контекстуальных эмбеддингов, автоматическое выявление новых значений и неологизмов.</li>
                    <li><strong>Машинный перевод и локализация:</strong> пост-редактирование, оценка качества, адаптация к специфическим доменам и культурным контекстам.</li>
                    <li><strong>Документация и ревитализация малых языков:</strong> использование LLM для создания ресурсов для языков с малым числом носителей.</li>
                    <li><strong>Образовательные технологии:</strong> персонализированное обучение языкам, автоматическая проверка и обратная связь, генерация упражнений.</li>
                    <li><strong>Анализ дискурса и медиа:</strong> автоматический анализ тональности, выявление манипуляций, фактчекинг.</li>
                </ul>
        </div>
    </div>
    <div class="kmp12"><strong>Важно:</strong> Лингвист, владеющий LLM-инструментарием, — это не «программист» и не «инженер». Это специалист по языку, который понимает, как работают новые модели, умеет с ними взаимодействовать, оценивать их и направлять. Лингвистическая экспертиза не обесценивается в эпоху LLM — она становится критически необходимой.</div>
</section>

<section id="6" class="section">
    <h2 class="section-title">Принцип дополнительности: больше карт — лучше навигация</h2>
            <div class="001">
        <h3 class="001-title">Не замена, а расширение репертуара</h3>
               <div class="001-card">
            <p>Ключевой тезис нашего курса: LLM-лингвистика <strong>не отменяет</strong> традиционную лингвистику. Она <strong>дополняет</strong> её и <strong>расширяет</strong> наше понимание языка, показывая ограничения старых подходов и предлагая новые инструменты там, где старые не справляются.</p>
            <p>Это принцип <strong>дополнительности карт</strong>: для разных задач нужны разные карты. Топографическая карта бесполезна для навигации в метро, а схема метро не поможет в пешем походе. Но обе карты ценны — каждая в своём контексте.</p>
            <p>Точно так же в лингвистике:</p>
           <p><strong>Контексты, в которых полезны разные модели:</strong></p>
                <ul>
                    <li><strong>Школьное образование:</strong> традиционная нормативная грамматика (прескриптивная модель) остаётся основным инструментом. Детям нужны чёткие правила и категории, а не вероятностные распределения.</li>
                    <li><strong>Историческая лингвистика:</strong> сравнительно-исторический метод, реконструкция праязыков — здесь традиционные модели незаменимы, хотя LLM начинают применяться и в этой области.</li>
                    <li><strong>Лексикография:</strong> традиционные словари полезны как справочники, но LLM-подходы позволяют создавать динамические, контекстуально-чувствительные словари нового поколения.</li>
                    <li><strong>Машинный перевод:</strong> LLM-модели сегодня значительно превосходят подходы, основанные на правилах и лингвистических моделях.</li>
                    <li><strong>Анализ текста:</strong> для литературоведческого анализа может быть полезна традиционная стилистика; для обработки миллионов документов — LLM-подходы.</li>
                    <li><strong>Типология:</strong> классификация языков по структурным параметрам — традиционная модель; выявление скрытых типологических паттернов в многоязычных LLM — новый фронтир.</li>
                </ul>
        </div>
    </div>
            <div class="001">
        <h3 class="001-title">Все модели неверны, включая LLM-модели</h3>
               <div class="001-card">
            <p>Было бы наивно и непоследовательно критиковать традиционную лингвистику за подмену территории картой — и при этом утверждать, что LLM дают «истинную картину» языка. LLM-модели — это тоже карты. Они тоже неверны. Они тоже имеют ограничения и искажения.</p>
           <p><strong>Ограничения LLM как моделей языка:</strong></p>
                <ul>
                    <li><strong>Зависимость от обучающих данных:</strong> LLM моделируют не «язык вообще», а тот язык, который представлен в обучающей выборке. Языки, диалекты, регистры, которые плохо представлены в интернете, будут плохо смоделированы.</li>
                    <li><strong>Отсутствие телесного опыта:</strong> LLM не имеют тела, не воспринимают мир через органы чувств. Их «понимание» пространственных метафор, сенсорных описаний, эмоциональных высказываний — это статистическая аппроксимация, а не переживание. Вопрос о «заземлении» (grounding) остаётся открытым.</li>
                    <li><strong>Галлюцинации:</strong> LLM могут порождать текст, который выглядит убедительно, но содержит фактические ошибки. Это следствие того, что модель оптимизирует правдоподобие текста, а не его истинность.</li>
                    <li><strong>Непрозрачность:</strong> внутренние представления LLM трудно интерпретировать. Мы видим, что модель «знает» грамматику, но не всегда можем объяснить, как именно это знание закодировано.</li>
                    <li><strong>Предвзятости (biases):</strong> LLM воспроизводят предвзятости, содержащиеся в обучающих данных — гендерные, расовые, культурные стереотипы.</li>
                    <li><strong>Статичность обученной модели:</strong> после обучения модель не обновляется автоматически. Она не «знает» о событиях и языковых изменениях, произошедших после даты отсечения обучающих данных.</li>
                </ul>
                <p><strong>Пояснение:</strong> Признание ограничений LLM — не слабость, а сила научного подхода. Мы используем LLM, осознавая их ограничения, — точно так же, как грамотный лингвист использует традиционную грамматику, осознавая её условность. Зрелая эпистемологическая позиция — не фанатизм одного подхода, а рефлексивное использование множества инструментов.</p>
        </div>
    </div>
<div class="table-kmp">
<table class="table">
<thead><tr><th>Критерий</th><th>Традиционная лингвистика</th><th>LLM-лингвистика</th></tr></thead>
 <tr><td>Тип модели</td><td>Символическая, основанная на правилах</td><td>Нейросетевая, основанная на данных</td></tr>
 <tr><td>Интерпретируемость</td><td>Высокая: правила можно прочитать</td><td>Низкая: «чёрный ящик» (активное направление исследований — mechanistic interpretability)</td></tr>
 <tr><td>Масштаб данных</td><td>Малые выборки, экспертная оценка</td><td>Триллионы токенов</td></tr>
 <tr><td>Охват языков</td><td>Описаны тысячи языков (неравномерно)</td><td>Хорошо представлены десятки языков, остальные — слабо</td></tr>
 <tr><td>Обработка вариативности</td><td>Через «нормы» и «исключения»</td><td>Через вероятностные распределения</td></tr>
 <tr><td>Главная сила</td><td>Объяснительная способность, прозрачность</td><td>Предсказательная сила, генеративная способность, масштаб</td></tr>
 <tr><td>Главная слабость</td><td>Упрощение, негибкость, зависимость от эксперта</td><td>Непрозрачность, зависимость от данных, галлюцинации</td></tr>
</table>
</div>
    <div class="kmp11"><strong>Примечание:</strong> Таблица — тоже карта. Она неизбежно упрощает ситуацию. В реальности между «традиционной» и «LLM-лингвистикой» нет чёткой границы: существует спектр подходов — корпусная лингвистика, вычислительная лингвистика, статистическое NLP — которые занимают промежуточные позиции. Но для вводного обзора такая «бинарная карта» полезна.</div>
</section>


<section id="7" class="section">
    <h2 class="section-title">Итоги и выводы</h2>
            <div class="001">
        <h3 class="001-title">Ключевые тезисы:</h3>
               <div class="001-card">
            
                <ul>
                    <li><strong>Тезис 1. Карта — не территория.</strong> Любая лингвистическая теория, грамматика, словарь или нейросетевая модель — это модель (карта) языка, а не сам язык (территория). Путать модель с реальностью — эпистемологическая ошибка.</li>
                    <li><strong>Тезис 2. Все модели неверны, но некоторые полезны.</strong> Совершенная модель языка невозможна — и не нужна. Нужны модели, полезные для конкретных задач в конкретных контекстах.</li>
                    <li><strong>Тезис 3. Традиционная лингвистика создала великие карты.</strong> Фонология, морфология, синтаксис, семантика, типология — эти модели остаются ценными и применимыми во множестве контекстов.</li>
                    <li><strong>Тезис 4. Традиционные карты имеют систематические ограничения.</strong> Дискретность, контекстонезависимость, детерминизм, статичность — эти свойства традиционных моделей упрощают реальность языка.</li>
                    <li><strong>Тезис 5. LLM-подходы создают карты нового типа.</strong> Континуальные, контекстуальные, вероятностные, стохастические — эти свойства LLM-моделей ближе к некоторым аспектам языковой территории, хотя тоже имеют свои ограничения.</li>
                    <li><strong>Тезис 6. LLM-лингвистика — фронтир.</strong> Это направление ставит новые вопросы, открывает новые перспективы и создаёт новые возможности для исследования и практического применения.</li>
                    <li><strong>Тезис 7. Больше карт хороших и разных.</strong> Зрелая позиция лингвиста — не приверженность одной «истинной» теории, а владение репертуаром моделей с пониманием границ применимости каждой. Чем больше карт в нашем распоряжении, тем лучше мы ориентируемся на территории языка.</li>
                </ul>
        </div>
    </div>
            <div class="001">
        <h3 class="001-title">От введения к практике</h3>
               <div class="001-card">
            <p>Этот вводный материал — отправная точка понимания современной комьютерной лингвистики. Углубление понимания предполагает:</p>
            <p>работу с реальными LLM — задавать им лингвистические вопросы, анализировать их ответы, исследовать их «знание» грамматики, семантики, прагматики. Мы будем сравнивать LLM-модели с традиционными описаниями и выяснять, где они совпадают, где расходятся — и что это расхождение означает.</p>
            <p>намерения стать лингвистами нового поколения — специалистами, для которых работа с AI — не экзотика, а повседневная практика, а понимание ограничений любой модели — не скепсис, а профессиональная зрелость.</p>
           <p><strong>Вопросы для самопроверки:</strong></p>
                <ul>
                    <li>Что означает принцип «карта — не территория» применительно к лингвистике? Приведите собственный пример.</li>
                    <li>В чём полезность прескриптивизма и в чём его ограничения?</li>
                    <li>Назовите четыре ключевых свойства LLM-моделей языка и объясните, чем каждое из них отличается от соответствующего свойства традиционных моделей.</li>
                    <li>Что такое контекстуальный эмбеддинг и почему он лучше отражает природу лексического значения, чем словарная статья?</li>
                    <li>Приведите пример ограничения LLM как модели языка. Почему это ограничение не обесценивает модель?</li>
                    <li>Как вы понимаете тезис «больше карт хороших и разных»?</li>
                </ul>
        </div>
    </div>
    <div class="kmp12"><strong>Важно:</strong> Мы начинаем путешествие по территории языка с новыми картами в руках — но с уважением к старым. Цель курса — не выбрать «правильную» карту, а научиться читать разные карты, понимать их возможности и ограничения, и уверенно ориентироваться в быстро меняющемся ландшафте современной лингвистики и AI.</div>
</section>



<footer class="footer">
<div class="container">
<p>© 2026 | kmp | CC BY-NC-SA 4.0<br>
Разработано для студентов БрГУ имени А.С. Пушкина</p>
</div>
</footer>
<div style="position: fixed; bottom: 10px; color: #777777; right: 30px; opacity: 0.3; font-size: 14px;">kmp+</div>
    <script>
        // Функция для плавной прокрутки к разделу
        function scrollToSection(sectionId) {
            const section = document.getElementById(sectionId);
            const menuHeight = document.querySelector('.menu').offsetHeight;
            
            window.scrollTo({
                top: section.offsetTop - menuHeight - 20,
                behavior: 'smooth'
            });
        }

        // Функция для переключения темы
        document.getElementById('themeToggle').addEventListener('click', function() {
            const currentTheme = document.documentElement.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            
            document.documentElement.setAttribute('data-theme', newTheme);
            this.textContent = newTheme === 'dark' ? '🌙' : '☀️';
        });

        // Анимация появления секций при прокрутке
        document.addEventListener('DOMContentLoaded', function() {
            const sections = document.querySelectorAll('.section');
            
            const observerOptions = {
                root: null,
                rootMargin: '0px',
                threshold: 0.1
            };
            
            const observer = new IntersectionObserver(function(entries, observer) {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('fade-in');
                        observer.unobserve(entry.target);
                    }
                });
            }, observerOptions);
            
            sections.forEach(section => {
                section.classList.remove('fade-in');
                observer.observe(section);
            });
        });
    </script>
</body>
</html>