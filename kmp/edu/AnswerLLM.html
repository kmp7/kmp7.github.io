<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>kmp+</title>
    <style>
        /* Основные стили */
        :root {
            --primary-color: #325980;
            --secondary-color: #4CAF50;
            --background-color: #f5f5f5;
            --content-bg: #ffffff;
            --text-color: #333333;
            --header-text-color: #ffffff;
            --menu-bg: #ffffff;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            --border-radius: 8px;
			  --accent11: #4caf50;
		--accent12: #4cafff;
		--accent13: #ffaf50;
		--accent14: #821978;
        }

        /* Темная тема */
        [data-theme="dark"] {
            --primary-color: #3e76ad;
            --secondary-color: #388e3c;
            --background-color: #000000;
            --content-bg: #1e1e1e;
            --text-color: #e0e0e0;
            --header-text-color: #ffffff;
            --menu-bg: #000000;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            transition: background-color 0.3s, color 0.3s;
        }

        body {
            font-family: 'Roboto', 'Arial', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--background-color);
            padding-top: 2px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        header {
            background-color: var(--primary-color);
            color: var(--header-text-color);
            padding: 20px 0;
            text-align: center;
            border-radius: var(--border-radius);
            margin-bottom: 2px;
        }

        h1 {
            font-size: 2.2rem;
            margin-bottom: 10px;
        }

        h2 {
            color: var(--primary-color);
            margin: 25px 0 15px;
            padding-bottom: 8px;
            border-bottom: 2px solid var(--secondary-color);
        }

        h3 {
            color: var(--primary-color);
            margin: 20px 0 10px;
        }

        p {
            margin-bottom: 15px;
        }

        /* Меню навигации */
        .menu {
            background-color: var(--menu-bg);
            padding: 15px 20px;
            border-radius: var(--border-radius);
            margin-bottom: 30px;
            box-shadow: var(--menu-shadow);
            display: flex;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .menu-buttons {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }
		
		
		.menu-buttons {
    display: flex;
    flex-wrap: wrap;
    gap: 10px;
    justify-content: center; /* Изменено на center */
}

        .menu-btn {
            background-color: var(--primary-color);
            color: white;
            border: none;
            padding: 8px 15px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: background-color 0.3s;
        }

        .menu-btn:hover {
            background-color: var(--secondary-color);
        }

        .theme-toggle {
            background: none;
            border: 10px solid transparent;
            font-size: 1.5rem;
            cursor: pointer;
            color: var(--primary-color);
        }

        /* Секции контента */
        .section {
            background-color: var(--content-bg);
            border-radius: var(--border-radius);
            padding: 25px;
            margin-bottom: 30px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        /* Выделение важного */
        .important {
            background-color: rgba(76, 175, 80, 0.1);
            border-left: 4px solid var(--secondary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        .note {
            background-color: rgba(50, 89, 128, 0.1);
            border-left: 4px solid var(--primary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        /* Таблицы */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            padding: 12px 15px;
            border: 1px solid #ddd;
            text-align: left;
        }

        th {
            background-color: var(--primary-color);
            color: white;
        }

        tr:nth-child(even) {
            background-color: rgba(0, 0, 0, 0.03);
        }

        /* Списки */
        ul, ol {
            padding-left: 25px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
        }

        /* Адаптивный дизайн */
        @media (max-width: 768px) {
            h1 {
                font-size: 1.8rem;
            }
            
            h2 {
                font-size: 1.5rem;
            }
            
            .menu {
                flex-direction: column;
                gap: 15px;
            }
            
            .menu-buttons {
                width: 100%;
                justify-content: center;
            }
            
            .theme-toggle {
                margin-top: 10px;
            }
            
            .section {
                padding: 15px;
            }
        }

        /* Анимации */
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .fade-in {
            animation: fadeIn 0.5s ease-in;
        }

        footer {
            text-align: center;
            padding: 20px 0;
            margin-top: 40px;
            font-size: 0.9rem;
        }
		
					.kmp11, .example {
      background: rgba(76, 175, 80, 0.1);
      border-left: 4px solid var(--accent11);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp12, .example {
      background: rgba(95, 182, 237, 0.1);
      border-left: 4px solid var(--accent12);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
 
		.kmp13, .example {
      background: rgba(205, 170, 110, 0.1);
      border-left: 4px solid var(--accent13);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp14, .example {
      background: rgba(205, 110, 200, 0.1);
      border-left: 4px solid var(--accent14);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Искусственный интеллект</h1>
            <p>как цель и ценность образования</p>
        </header>

        <nav class="menu">
            <div class="menu-buttons">
                <button class="menu-btn" onclick="scrollToSection('section0')">Интеллект</button>
				<button class="menu-btn" onclick="scrollToSection('section1')">Концепция ИИ</button>
				<button class="menu-btn" onclick="scrollToSection('section2')">Грамматика ИИ</button>
				<button class="menu-btn" onclick="scrollToSection('section3')">"Фюзис" и "Техне"</button>
				<button class="menu-btn" onclick="scrollToSection('section4')">Три фактора </button>
                <button class="menu-btn" onclick="scrollToSection('section5')">Три уровня</button>
                <button class="menu-btn" onclick="scrollToSection('section6')">Два ИИ</button>
                <button class="menu-btn" onclick="scrollToSection('section7')">Цель иценность</button>
               
            </div>
            <button class="theme-toggle" id="themeToggle" title="Переключить тему">☀️</button>
        </nav>


<section id="llm-factors" class="section"> <h2 class="section-title">1. От чего зависит ответ большой языковой модели?</h2> <div class="001"> <h3 class="001-title">Что определяет поведение LLM?</h3> <div class="001-card"> <h4>Комплексная система факторов</h4> <p>Ответ, создаваемый большой языковой моделью (LLM), является результатом взаимодействия архитектуры, данных, обучения и запроса пользователя. Модель не "думает" в привычном смысле, но "реагирует" на запрос, исходя из вероятностной оценки слов на основе предобученного распределения.</p> <p>На финальную генерацию текста влияет не один фактор, а их сеть — от нейросетевой архитектуры и корпуса обучения до формулировки пользовательского запроса.</p> <p><strong>Список 1:</strong></p> <ul> <li>Архитектура модели</li> <li>Тип и объём обучающих данных</li> <li>Методы предобучения и дообучения</li> <li>Системные параметры генерации</li> <li>Контекст и язык запроса</li> </ul> <p><strong>Пояснение:</strong> Каждая из этих категорий включает в себя множество подфакторов, которые будут разобраны далее по разделам.</p> </div> </div> 

<div class="table-kmp"> <table class="table"> <thead><tr><th>Категория</th><th>Примеры факторов</th></tr></thead> <tr><td>Архитектура</td><td>Transformer, параметры, токенизация</td></tr> <tr><td>Данные</td><td>Тип корпуса, языки, жанры</td></tr> <tr><td>Обучение</td><td>Pretraining, fine-tuning, RLHF</td></tr> <tr><td>Генерация</td><td>Температура, Top-k, системный промпт</td></tr> <tr><td>Пользователь</td><td>Запрос, стиль, цели</td></tr> </table> </div> <div class="kmp11"><strong>Примечание:</strong> Даже небольшое изменение в формулировке запроса может активировать разные параметры векторного пространства модели.</div> <div class="kmp12"><strong>Важно:</strong> Поведение модели всегда вероятностное, а не детерминированное. Результат — это один из возможных вариантов, не единственно верный.</div> <div class="kmp13"><strong>Внимание:</strong> Не существует универсального объяснения для каждого конкретного ответа — только совокупностное влияние факторов.</div> <div class="kmp14"><strong>Пояснение:</strong> В следующих разделах каждый фактор будет представлен индивидуально с примерами и деталями.</div> <a target="_blank" href="https://arxiv.org/abs/2005.14165" class="link-kmp1">Пример исследования о многомодальном влиянии факторов в LLM</a> </section>


<section id="architecture" class="section"> <h2 class="section-title">2. Архитектура модели</h2> <div class="001"> <h3 class="001-title">Что лежит в основе мышления LLM?</h3> <div class="001-card"> <h4>Определение архитектурного фундамента</h4> <p>Архитектура — это скелет любой LLM. Она задаёт правила, по которым модель обрабатывает входной текст, сохраняет информацию и генерирует ответы. Большинство современных моделей (включая GPT, BERT и их производные) основаны на архитектуре <em>Transformer</em>.</p> <p>Именно архитектура отвечает за такие механизмы, как самовнимание, позиционное кодирование, слоистая обработка и параллельность вычислений.</p> <p><strong>Список 1:</strong></p> <ul> <li>Тип архитектуры (Transformer, RNN, CNN и др.)</li> <li>Глубина сети (количество слоёв)</li> <li>Ширина слоёв (размер скрытых представлений)</li> <li>Количество голов внимания</li> <li>Механизмы нормализации и регуляризации</li> <li>Тип токенизатора (BPE, WordPiece, SentencePiece)</li> </ul> <p><strong>Пояснение:</strong> Даже при одном и том же наборе данных две модели с разной архитектурой будут вести себя по-разному — от стиля формулировки до способности к логическим обобщениям.</p> </div> </div>

<div class="table-kmp"> <table class="table"> <thead><tr><th>Компонент</th><th>Функция</th></tr></thead> <tr><td>Self-Attention</td><td>Определяет, какие слова важны в контексте</td></tr> <tr><td>Position Encoding</td><td>Добавляет информацию о порядке слов</td></tr> <tr><td>Feed-Forward Layers</td><td>Обрабатывают представление после внимания</td></tr> <tr><td>Layer Normalization</td><td>Стабилизирует обучение</td></tr> <tr><td>Multi-Head Attention</td><td>Позволяет учитывать разные аспекты контекста</td></tr> </table> </div>

<div class="kmp11"><strong>Примечание:</strong> Архитектура определяет не только структуру модели, но и её «стиль мышления»: склонность к логике, креативности или краткости.</div> <div class="kmp12"><strong>Важно:</strong> Даже при одинаковом обучении архитектура ограничивает возможности модели в плане абстракции или объёма информации, которую она может удерживать.</div> <div class="kmp13"><strong>Внимание:</strong> Некоторые архитектурные изменения (например, добавление "модальности") принципиально расширяют границы применения LLM — от текста к изображениям или коду.</div> <div class="kmp14"><strong>Пояснение:</strong> Следующий раздел раскроет, как тип и объём обучающих данных задают модельному «миру» масштаб, язык и культурный ландшафт.</div>

<a target="_blank" href="https://arxiv.org/abs/1706.03762" class="link-kmp1">Оригинальная статья про Transformer</a> </section>

<section id="training-data" class="section"> <h2 class="section-title">3. Обучающие данные</h2> <div class="001"> <h3 class="001-title">Что формирует знания LLM?</h3> <div class="001-card"> <h4>Роль корпусов в обучении модели</h4> <p>Большая языковая модель обучается на колоссальном массиве текстов, охватывающем книги, статьи, веб-страницы, диалоги, код и многое другое. Эти данные не просто дают модели "факты" — они формируют её представления о языке, стиле, логике и даже "мировоззрение".</p> <p>Характер данных влияет на лексический запас, склонность к определённым темам, способность отвечать в разных жанрах — от научной статьи до бытового диалога.</p> <p><strong>Список 1:</strong></p> <ul> <li>Разнообразие источников (жанры, тематики, форматы)</li> <li>Языковое покрытие (одно- или многоязычный корпус)</li> <li>Качество текста (грамматическая корректность, стиль)</li> <li>Хронологическая свежесть данных</li> <li>Обработка и очистка данных (удаление токсичных, дубликатов и т. д.)</li> </ul> <p><strong>Пояснение:</strong> Если корпус преимущественно состоит из интернет-форумов, модель будет склонна к неформальной лексике; если из научных статей — к аналитичности и сдержанности.</p> </div> </div>

<div class="table-kmp"> <table class="table"> <thead><tr><th>Параметр корпуса</th><th>Влияние на поведение модели</th></tr></thead> <tr><td>Тематика</td><td>Формирует знания и примеры внутри темы</td></tr> <tr><td>Язык</td><td>Определяет лингвистическую компетенцию</td></tr> <tr><td>Стиль текстов</td><td>Влияет на тон и форму генерации</td></tr> <tr><td>Пропорции жанров</td><td>Склоняют к диалогам, статьям или кодированию</td></tr> <tr><td>Цензура / отбор</td><td>Ограничивает риск нежелательных ответов</td></tr> </table> </div>

<div class="kmp11"><strong>Примечание:</strong> Даже если архитектура идеальна, модель не превзойдёт ограничений своего корпуса. Её знание мира всегда производное от того, что она "читала".</div> <div class="kmp12"><strong>Важно:</strong> Некоторые темы или языки могут быть представлены недостаточно, что приводит к перекосам в генерации или пробелам в знаниях.</div> <div class="kmp13"><strong>Внимание:</strong> Локальные культурные нормы, устойчивые выражения и исторические контексты усваиваются моделью из соответствующих источников — либо не усваиваются вовсе.</div> <div class="kmp14"><strong>Пояснение:</strong> Следующий раздел покажет, как обучение (предобучение и дообучение) трансформирует корпус в поведенческий навык модели.</div>

<a target="_blank" href="https://arxiv.org/abs/2309.12445" class="link-kmp1">Обзор обучающих корпусов и их эффектов</a> </section>


<section id="training" class="section"> <h2 class="section-title">4. Режимы обучения модели</h2> <div class="001"> <h3 class="001-title">Как из данных рождается поведение?</h3> <div class="001-card"> <h4>От предобучения к "характеру" модели</h4> <p>Обучение модели проходит в несколько этапов, каждый из которых закладывает определённые аспекты её "поведения". Предобучение формирует языковую компетенцию и мировые знания, а дообучение — задаёт стиль, инструкции и социально приемлемое поведение.</p> <p>Тонкие настройки и методы обучения с подкреплением позволяют превратить просто "текстовую машину" в полезного собеседника, помощника или учителя.</p> <p><strong>Список 1:</strong></p> <ul> <li><strong>Предобучение (Pretraining):</strong> на огромном корпусе для предсказания следующего токена</li> <li><strong>Инструкционное дообучение:</strong> на парных примерах запроса и ожидаемого ответа</li> <li><strong>RLHF (обучение с подкреплением от обратной связи человека):</strong> на ранжированных ответах</li> <li><strong>Безопасностное дообучение:</strong> подавление нежелательного поведения и токсичности</li> <li><strong>Финетюнинг под конкретные задачи:</strong> коды, юридические тексты и др.</li> </ul> <p><strong>Пояснение:</strong> Даже идентичные архитектуры и данные дадут разные модели, если этапы обучения различаются по стратегии или целям. Например, одна модель будет склонна объяснять, другая — шутить, третья — избегать сложных тем.</p> </div> </div>

<div class="table-kmp"> <table class="table"> <thead><tr><th>Этап</th><th>Назначение</th></tr></thead> <tr><td>Предобучение</td><td>Формирует базовые знания и языковую модель</td></tr> <tr><td>Дообучение</td><td>Настраивает поведение под инструкции</td></tr> <tr><td>RLHF</td><td>Оптимизирует качество ответа по человеческой обратной связи</td></tr> <tr><td>Регуляризация</td><td>Снижает переобучение, улучшает обобщение</td></tr> <tr><td>Безопасностная фильтрация</td><td>Ограничивает ответы, нарушающие нормы</td></tr> </table> </div>

<div class="kmp11"><strong>Примечание:</strong> Именно этап RLHF делает модель "вежливой", "старающейся помочь" и "нейтральной" в спорных темах.</div> <div class="kmp12"><strong>Важно:</strong> Некоторые модели обучаются на заранее подобранных парах "запрос–ответ", что придаёт им более "инструктивный" стиль общения.</div> <div class="kmp13"><strong>Внимание:</strong> Даже при хороших данных и архитектуре без стадий дообучения модель будет работать как "предсказатель слов", а не собеседник.</div> <div class="kmp14"><strong>Пояснение:</strong> Следующий раздел раскроет технические параметры генерации: температуру, системный промпт, sampling — и как они меняют "настроение" ответа.</div>

<a target="_blank" href="https://huggingface.co/blog/rlhf" class="link-kmp1">Статья HuggingFace об RLHF и инструкционном обучении</a> </section>


<section id="generation" class="section"> <h2 class="section-title">5. Параметры генерации и системный промпт</h2> <div class="001"> <h3 class="001-title">Как модель "решает", что именно сказать?</h3> <div class="001-card"> <h4>Технические регуляторы творчества модели</h4> <p>Даже при одной и той же архитектуре и идентичных данных, поведение модели в момент генерации может меняться — в зависимости от настроек генератора и "внутреннего указания", что называется системным промптом. Это то, что влияет на стиль, креативность и направленность каждого ответа.</p> <p>Генерация — это процесс сэмплирования: модель выбирает токены с определённой вероятностью, и эти вероятности трансформируются параметрами.</p> <p><strong>Список 1:</strong></p> <ul> <li><strong>Температура:</strong> регулирует случайность генерации. При температуре 0 — детерминированный ответ, при 1 — больше креатива и разнообразия.</li> <li><strong>Top-k sampling:</strong> модель выбирает токен из k самых вероятных. K ограничивает "ширину выбора".</li> <li><strong>Top-p (nucleus) sampling:</strong> вместо фиксированного числа, выбираются токены, сумма вероятностей которых ≥ p.</li> <li><strong>Системный промпт:</strong> скрытая инструкция, определяющая стиль, задачи и личность модели (например, "будь учтивым помощником", "говори в стиле учёного").</li> <li><strong>Максимальная длина вывода:</strong> ограничивает размер ответа и влияет на его сжатие.</li> </ul> <p><strong>Пояснение:</strong> Эти параметры работают на финальном этапе — они превращают вектор вероятностей в конкретный текст. Подобно настройке музыкального инструмента, они придают индивидуальность голосу модели.</p> </div> </div>

<div class="table-kmp"> <table class="table"> <thead><tr><th>Параметр</th><th>Влияние на ответ</th></tr></thead> <tr><td>Температура = 0</td><td>Наиболее предсказуемый и безопасный текст</td></tr> <tr><td>Температура = 1.0</td><td>Больше вариантов, неожиданные формулировки</td></tr> <tr><td>Top-k = 50</td><td>Только из топ-50 вероятных токенов</td></tr> <tr><td>Top-p = 0.9</td><td>Сэмплирование из токенов, сумма вероятностей которых ≥ 90%</td></tr> <tr><td>Системный промпт: "будь кратким"</td><td>Ответы становятся сдержанными и лаконичными</td></tr> </table> </div>

<div class="kmp11"><strong>Примечание:</strong> Параметры генерации часто незаметны для пользователя, но именно они определяют «настроение» и форму ответа.</div> <div class="kmp12"><strong>Важно:</strong> Изменив только температуру с 0.2 до 0.8, можно получить совершенно другой стиль ответа — от сухого до экспрессивного.</div> <div class="kmp13"><strong>Внимание:</strong> Системный промпт — это не часть пользовательского запроса. Он «приходит сверху» от разработчиков или среды выполнения и может ограничивать темы, стиль и тон.</div> <div class="kmp14"><strong>Пояснение:</strong> Далее — рассмотрим, как формулировка самого запроса пользователя влияет на поведение модели, независимо от внутренней настройки.</div>
<a target="_blank" href="https://huggingface.co/blog/how-to-generate" class="link-kmp1">Пример: как работают sampling-параметры</a> </section>



<section id="user-prompt" class="section"> <h2 class="section-title">6. Запрос пользователя и контекст</h2> <div class="001"> <h3 class="001-title">Что передаёт модель через ваш вопрос?</h3> <div class="001-card"> <h4>Роль формулировки и контекста диалога</h4> <p>Каждый запрос, который вы вводите, — это "координационный сигнал", направляющий модель. Даже одна и та же модель с одинаковыми данными и архитектурой может по-разному реагировать, если фраза запроса будет изменена на лексическом или синтаксическом уровне.</p> <p>Кроме того, важен не только текущий текст, но и <em>контекст предыдущей беседы</em>, включая стиль общения, заданные темы, предпочтения и даже тональность. Это превращает диалог с LLM в адаптивный и интерактивный процесс.</p> <p><strong>Список 1:</strong></p> <ul> <li>Чёткая или размытая формулировка</li> <li>Инструктивный стиль ("сделай список") vs. открытый ("расскажи")</li> <li>Содержит ли запрос примеры, параметры, желаемый стиль</li> <li>Использование технических терминов (влияет на "режим" мышления)</li> <li>Язык общения и культурные маркеры</li> </ul> <p><strong>Пояснение:</strong> Сказать "напиши статью про время" и "создай сатирический памфлет о линейности времени" — совершенно разные запросы, активирующие разные поведенческие паттерны модели.</p> </div> </div>

<div class="table-kmp"> <table class="table"> <thead><tr><th>Форма запроса</th><th>Пример влияния</th></tr></thead> <tr><td>Открытый стиль</td><td>Ответ будет развёрнутый, с интерпретациями</td></tr> <tr><td>Жёстко структурированный</td><td>Ответ станет более точным, формальным</td></tr> <tr><td>С указанием жанра</td><td>Ответ адаптируется под сказку, эссе или диалог</td></tr> <tr><td>С контекстом предыдущей беседы</td><td>Ответ продолжит начатую тему</td></tr> <tr><td>На определённом языке</td><td>Стиль, примеры и фрейм мышления будут соответствовать этому языку</td></tr> </table> </div>

<div class="kmp11"><strong>Примечание:</strong> Ты как дирижёр, управляющий моделью одной палочкой — формулировкой фразы.</div> <div class="kmp12"><strong>Важно:</strong> Модель не "понимает" цели пользователя, если они не явно заданы. Чем яснее инструкция — тем ближе ответ к нужному результату.</div> <div class="kmp13"><strong>Внимание:</strong> Вопрос без контекста может вызвать "среднестатистический" ответ — модель будет заполнять пробелы по вероятностным паттернам.</div> <div class="kmp14"><strong>Пояснение:</strong> Далее мы рассмотрим, как внешняя среда — технические и социокультурные ограничения — влияет на окончательную форму вывода.</div>

<a target="_blank" href="https://arxiv.org/abs/2302.13971" class="link-kmp1">Статья: влияние стиля запроса на поведение LLM</a> </section>


<section id="external-factors" class="section"> <h2 class="section-title">7. Влияние внешней среды и платформы</h2> <div class="001"> <h3 class="001-title">Что окружает модель и влияет извне?</h3> <div class="001-card"> <h4>Контекст использования и технические ограничения</h4> <p>Даже идеально обученная модель в реальной работе «живёт» в рамках определённой среды — веб-платформы, API-интерфейса, мобильного приложения. Эти оболочки накладывают ограничения, фильтры и даже настраивают модель под конкретную аудиторию или задачу. Механизмы модерации, кэширования, адаптивной персонализации — всё это воздействует на то, какой конкретный ответ получает пользователь.</p> <p>Также возможны региональные фильтры, локализация на определённый язык, адаптация под юридические нормы или цензурные ограничения.</p> <p><strong>Список 1:</strong></p> <ul> <li>Платформа (веб, API, мобильное приложение)</li> <li>Механизмы безопасности и фильтрации (контент-фильтры, антиабьюз)</li> <li>Лимиты по длине запроса и ответа</li> <li>Интерфейс пользователя (наличие подсказок, UI-промпт-инжиниринг)</li> <li>Региональные или юридические ограничения</li> <li>Персональные настройки пользователя (если разрешены)</li> </ul> <p><strong>Пояснение:</strong> Даже если два пользователя задают один и тот же запрос, они могут получить разные ответы в зависимости от интерфейса, региональных настроек и текущего состояния модели (например, обновлений).</p> </div> </div>

<div class="table-kmp"> <table class="table"> <thead><tr><th>Фактор среды</th><th>Пример влияния</th></tr></thead> <tr><td>Фильтрация контента</td><td>Блокировка определённых тем или стилей</td></tr> <tr><td>Региональные законы</td><td>Скрытие или адаптация информации под локальные нормы</td></tr> <tr><td>Веб-интерфейс</td><td>Может дополнять или трансформировать запрос</td></tr> <tr><td>Персонализированный режим</td><td>Коррекция ответов под предпочтения (если предусмотрено)</td></tr> <tr><td>Обновления модели</td><td>Изменение поведения между сессиями</td></tr> </table> </div>

<div class="kmp11"><strong>Примечание:</strong> LLM — это не замкнутая капсула, а часть экосистемы: каждый слой — от модели до UI — вносит свою лепту.</div> <div class="kmp12"><strong>Важно:</strong> Платформа может усиливать или ограничивать креативность, глубину или даже доступные темы в ответе модели.</div> <div class="kmp13"><strong>Внимание:</strong> Даже "та же" модель может вести себя иначе на разных платформах из-за различий в промптах, ограничениях и генеративной политике.</div> <div class="kmp14"><strong>Пояснение:</strong> Этот фактор часто остается за кадром, но именно он объясняет, почему ответы модели могут отличаться между различными реализациями.</div>

<a target="_blank" href="https://arxiv.org/abs/2305.14718" class="link-kmp1">Исследование: как среда выполнения влияет на LLM</a> </section>


<section id="ethics-responsibility" class="section"> <h2 class="section-title">8. Ответственность за AI: угроза не от машин, а от нас самих</h2> <div class="001"> <h3 class="001-title">Кто на самом деле «в ответе» за поведение AI?</h3> <div class="001-card"> <h4>Люди как источник как пользы, так и угрозы</h4> <p>Часто в обществе говорят о «опасности искусственного интеллекта», как если бы он был автономным субъектом. Но на деле AI — лишь зеркало: он отражает то, что в него вложили мы сами. Именно <em>люди</em> выбирают, какие данные использовать для обучения. Именно <em>люди</em> решают, как интерпретировать результаты, как применять модели — и с какими целями.</p> <p>Угрозы исходят не от модели как таковой, а от:</p> <ul> <li>низкокачественных или токсичных обучающих данных</li> <li>отсутствия этических норм в дообучении</li> <li>непрозрачных бизнес- и политических интересов</li> <li>попыток подменить человеческую ответственность алгоритмами</li> <li>поверхностного, «магического» восприятия AI как сверхразума</li> </ul> <p><strong>Пояснение:</strong> Машины не выбирают зло, не нарушают нормы и не принимают моральных решений. Это делают их создатели, операторы и пользователи. И именно с них начинается путь либо к дегуманизации, либо к осознанному использованию AI во благо.</p> </div> </div>

<div class="table-kmp"> <table class="table"> <thead><tr><th>Область риска</th><th>Происхождение угрозы</th></tr></thead> <tr><td>Биас и дискриминация</td><td>Историческая предвзятость в обучающих данных</td></tr> <tr><td>Манипуляция</td><td>Применение модели для дезинформации или давления</td></tr> <tr><td>Технические сбои</td><td>Неполное тестирование и слабый контроль качества</td></tr> <tr><td>Размывание ответственности</td><td>Передача решений алгоритму без человеческого надзора</td></tr> <tr><td>Культ AI</td><td>Слепое доверие без понимания принципов работы</td></tr> </table> </div>

<div class="kmp11"><strong>Примечание:</strong> AI — не внешняя угроза, а продолжение нас самих. Он не может быть «хуже» или «лучше» человечества — он наша проекция.</div> <div class="kmp12"><strong>Важно:</strong> Нельзя перекладывать моральную или политическую ответственность на алгоритмы. Каждый шаг — от подбора данных до внедрения — требует зрелости и этики.</div> <div class="kmp13"><strong>Внимание:</strong> Образование, критическое мышление и персональные компетенции в работе с AI — ключ к безопасному будущему. Никакие фильтры не заменят личную ответственность.</div> <div class="kmp14"><strong>Пояснение:</strong> Студент будущего должен быть не просто пользователем нейросетей, но и гражданином эпохи алгоритмов — ответственным, этически зрелым, осознанным.</div>

<blockquote class="kmp13" style="margin-top: 2em; font-style: italic;"> Gott, gib mir die Gelassenheit, die Dinge hinzunehmen, <br />die ich nicht ändern kann, den Mut, die Dinge zu ändern, die ich ändern kann, <br />und die Weisheit, das eine vom anderen zu unterscheiden.<br /><br />Господи, дай мне спокойствие принять то, чего я не могу изменить,<br /> дай мне мужество изменить то, что я могу изменить.<br /> И дай мне мудрость отличить одно от другого.<br /> <div style="text-align: right; margin-top: 0.5em;">— Карл Фридрих Этингер (1702–1782)</div> </blockquote>

<a target="_blank" href="https://futureoflife.org/open-letter/policy-makers-ai/" class="link-kmp1">Открытое письмо к политикам о будущем AI</a> </section>

            </div>
 <footer class="footer">
<div class="container">
<p>© 2025 | kmp CC BY-NC-SA 4.0<br>
Разработано для студентов БрГУ имени А.С. Пушкина</p>
</div>
</div>
</footer>
<div style="position: fixed; bottom: 10px; right: 33px; opacity: 0.3; font-size: 14px;">kmp+</div>
    
    <script>
        // Функция для плавной прокрутки к разделу
        function scrollToSection(sectionId) {
            const section = document.getElementById(sectionId);
            const menuHeight = document.querySelector('.menu').offsetHeight;
            
            window.scrollTo({
                top: section.offsetTop - menuHeight - 20,
                behavior: 'smooth'
            });
        }

        // Функция для переключения темы
        document.getElementById('themeToggle').addEventListener('click', function() {
            const currentTheme = document.documentElement.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            
            document.documentElement.setAttribute('data-theme', newTheme);
            this.textContent = newTheme === 'dark' ? '🌙' : '☀️';
        });

        // Анимация появления секций при прокрутке
        document.addEventListener('DOMContentLoaded', function() {
            const sections = document.querySelectorAll('.section');
            
            const observerOptions = {
                root: null,
                rootMargin: '0px',
                threshold: 0.1
            };
            
            const observer = new IntersectionObserver(function(entries, observer) {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('fade-in');
                        observer.unobserve(entry.target);
                    }
                });
            }, observerOptions);
            
            sections.forEach(section => {
                section.classList.remove('fade-in');
                observer.observe(section);
            });
        });
    </script>
</body>
</html>