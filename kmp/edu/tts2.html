<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>kmp+</title>
    <style>
        /* Основные стили */
        :root {
            --primary-color: #325980;
            --secondary-color: #4CAF50;
            --background-color: #f5f5f5;
            --content-bg: #ffffff;
            --text-color: #333333;
            --header-text-color: #ffffff;
            --menu-bg: #ffffff;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            --border-radius: 8px;
			  --accent11: #4caf50;
		--accent12: #4cafff;
		--accent13: #ffaf50;
		--accent14: #821978;
        }

        /* Темная тема */
        [data-theme="dark"] {
            --primary-color: #3e76ad;
            --secondary-color: #388e3c;
            --background-color: #000000;
            --content-bg: #1e1e1e;
            --text-color: #e0e0e0;
            --header-text-color: #ffffff;
            --menu-bg: #000000;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            transition: background-color 0.3s, color 0.3s;
        }

        body {
            font-family: 'Roboto', 'Arial', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--background-color);
            padding-top: 2px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        header {
            background-color: var(--primary-color);
            color: var(--header-text-color);
            padding: 20px 0;
            text-align: center;
            border-radius: var(--border-radius);
            margin-bottom: 2px;
        }

        h1 {
            font-size: 2.2rem;
            margin-bottom: 10px;
        }

        h2 {
            color: var(--primary-color);
            margin: 25px 0 15px;
            padding-bottom: 8px;
            border-bottom: 2px solid var(--secondary-color);
        }

        h3 {
            color: var(--primary-color);
            margin: 20px 0 10px;
        }

        p {
            margin-bottom: 15px;
        }

        /* Меню навигации */
        .menu {
            background-color: var(--menu-bg);
            padding: 15px 20px;
            border-radius: var(--border-radius);
            margin-bottom: 30px;
            box-shadow: var(--menu-shadow);
            display: flex;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .menu-buttons {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }
		
		
		.menu-buttons {
    display: flex;
    flex-wrap: wrap;
    gap: 10px;
    justify-content: center; /* Изменено на center */
}

        .menu-btn {
            background-color: var(--primary-color);
            color: white;
            border: none;
            padding: 8px 15px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: background-color 0.3s;
        }

        .menu-btn:hover {
            background-color: var(--secondary-color);
        }

        .theme-toggle {
            background: none;
            border: 10px solid transparent;
            font-size: 1.5rem;
            cursor: pointer;
            color: var(--primary-color);
        }

        /* Секции контента */
        .section {
            background-color: var(--content-bg);
            border-radius: var(--border-radius);
            padding: 25px;
            margin-bottom: 30px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        /* Выделение важного */
        .important {
            background-color: rgba(76, 175, 80, 0.1);
            border-left: 4px solid var(--secondary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        .note {
            background-color: rgba(50, 89, 128, 0.1);
            border-left: 4px solid var(--primary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        /* Таблицы */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            padding: 12px 15px;
            border: 1px solid #ddd;
            text-align: left;
        }

        th {
            background-color: var(--primary-color);
            color: white;
        }

        tr:nth-child(even) {
            background-color: rgba(0, 0, 0, 0.03);
        }

        /* Списки */
        ul, ol {
            padding-left: 25px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
        }

        /* Адаптивный дизайн */
        @media (max-width: 768px) {
            h1 {
                font-size: 1.8rem;
            }
            
            h2 {
                font-size: 1.5rem;
            }
            
            .menu {
                flex-direction: column;
                gap: 15px;
            }
            
            .menu-buttons {
                width: 100%;
                justify-content: center;
            }
            
            .theme-toggle {
                margin-top: 10px;
            }
            
            .section {
                padding: 15px;
            }
        }

        /* Анимации */
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .fade-in {
            animation: fadeIn 0.5s ease-in;
        }

        footer {
            text-align: center;
            padding: 20px 0;
            margin-top: 40px;
            font-size: 0.9rem;
        }
		
					.kmp11, .example {
      background: rgba(76, 175, 80, 0.1);
      border-left: 4px solid var(--accent11);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp12, .example {
      background: rgba(95, 182, 237, 0.1);
      border-left: 4px solid var(--accent12);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
 
		.kmp13, .example {
      background: rgba(205, 170, 110, 0.1);
      border-left: 4px solid var(--accent13);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp14, .example {
      background: rgba(205, 110, 200, 0.1);
      border-left: 4px solid var(--accent14);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
	
	
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>TTS LLM</h1>
            <p>Генеративные модели синтеза устной речи (по письменному тексту)</p>
        </header>

        <nav class="menu">
            <div class="menu-buttons">
                <button class="menu-btn" onclick="scrollToSection('1')">Введение</button>
                <button class="menu-btn" onclick="scrollToSection('2')">Подходы</button>
                <button class="menu-btn" onclick="scrollToSection('3')">Аспекты</button>
                <button class="menu-btn" onclick="scrollToSection('4')">Перспективы</button>
				<button class="menu-btn" onclick="scrollToSection('5')">Глоссарий</button>
				                           </div>
            <button class="theme-toggle" id="themeToggle" title="Переключить тему">☀️</button>
        </nav>


<section id="1" class="section">
    <h2 class="section-title">Введение в генеративные LLM-модели синтеза речи</h2>
    <div class="001">
        <h3 class="001-title">Основы генеративного моделирования в TTS</h3>
        <div class="001-card">
            <h4>Парадигма генеративного подхода</h4>
            <p>Генеративные модели в синтезе речи представляют собой системы, способные моделировать сложные вероятностные распределения речевых сигналов. В отличие от конкатенативных и параметрических методов, генеративный LLM учится воспроизводить естественные паттерны человеческой речи через глубокое обучение на больших корпусах данных.</p>
            <p>Ключевое преимущество заключается в способности генерировать речь, которая не просто соединяет заранее записанные фрагменты, а создаёт новые акустические реализации на основе изученных закономерностей.</p>
        </div>
        <div class="001-card">
            <h4>Авторегрессионные модели</h4>
            <p>Авторегрессионные архитектуры, такие как Tacotron 2, генерируют речевой сигнал последовательно, где каждый новый элемент зависит от предыдущих. Модель преобразует текстовую последовательность в мел-спектрограмму через механизм внимания (attention mechanism).</p>
            <p><strong>Архитектурные компоненты Tacotron 2:</strong></p>
            <ul>
                <li>Энкодер для обработки текстовых эмбеддингов</li>
                <li>Декодер с рекуррентными слоями для генерации спектрограмм</li>
                <li>Механизм внимания для выравнивания текста и аудио</li>
                <li>Постпроцессинговая сеть для улучшения качества</li>
            </ul>
        </div>
        <div class="001-card">
            <h4>Генеративно-состязательные сети (GANs)</h4>
            <p>GAN-архитектуры для аудио, включая HiFi-GAN и MelGAN, используют состязательное обучение для создания высококачественных вокодеров. Генератор создаёт аудиосигнал из мел-спектрограммы, а дискриминатор оценивает его реалистичность.</p>
            <p><strong>Преимущества GAN-вокодеров:</strong></p>
            <ul>
                <li>Высокая скорость инференса (real-time и выше)</li>
                <li>Отсутствие авторегрессии при генерации формы волны</li>
                <li>Способность к генерализации на новые голоса</li>
                <li>Меньшие вычислительные требования по сравнению с WaveNet</li>
            </ul>
        </div>
        <div class="001-card">
            <h4>WaveNet и нейронные вокодеры</h4>
            <p>WaveNet революционизировал синтез речи, моделируя аудио на уровне отдельных сэмплов через глубокие свёрточные сети с расширенными (dilated) свёртками. Модель предсказывает распределение вероятностей для каждого аудиосэмпла условно от предыдущих сэмплов и лингвистических признаков.</p>
            <p>Ключевые инновации включают использование причинных (causal) свёрток для сохранения временной последовательности и остаточных связей для эффективного обучения глубоких сетей.</p>
            <div class="kmp14"><strong>Пояснение:</strong> Хотя оригинальный WaveNet требовал значительных вычислительных ресурсов, последующие оптимизации (Parallel WaveNet, WaveRNN) сделали технологию практически применимой.</div>
        </div>
    </div>
</section>

<section id="2" class="section">
    <h2 class="section-title">Современные генеративные подходы (2025 год)</h2>
    <div class="002">
        <h3 class="002-title">Передовые архитектуры и методы</h3>
        <div class="002-card">
            <h4>Технологии клонирования голоса</h4>
            <p>Современные системы voice cloning, такие как VALL-E, Tortoise TTS и их последователи, способны воспроизводить уникальные характеристики голоса из минимального количества образцов (3-10 секунд записи). Эти модели используют контрастивное обучение и метаобучение для быстрой адаптации к новым голосам.</p>
            <p><strong>Ключевые компоненты систем клонирования:</strong></p>
            <ul>
                <li>Энкодер говорящего для извлечения голосовых эмбеддингов</li>
                <li>Адаптивные слои для персонализации синтеза</li>
                <li>Механизмы few-shot learning для работы с ограниченными данными</li>
                <li>Системы верификации для контроля качества клонирования</li>
            </ul>
        </div>
        <div class="002-card">
            <h4>Speech-to-Speech модели реального времени</h4>
            <p>S2S системы 2025 года обеспечивают прямое преобразование речи в речь без промежуточного текстового представления. Это позволяет сохранять паралингвистические характеристики, эмоции и индивидуальные особенности произношения.</p>
            <p>Интеграция с transformer-архитектурами обеспечивает эффективное моделирование долгосрочных зависимостей, а использование кросс-внимания позволяет точно передавать просодические паттерны исходной речи.</p>
        </div>
        <div class="002-card">
            <h4>Диффузионные модели в синтезе речи</h4>
            <p>Diffusion models представляют новую парадигму в генеративном TTS, где синтез происходит через постепенное удаление шума из случайного сигнала. Модели типа DiffWave и Grad-TTS демонстрируют высокое качество синтеза при стабильном обучении.</p>
            <p><strong>Преимущества диффузионного подхода:</strong></p>
            <ul>
                <li>Стабильность обучения без mode collapse</li>
                <li>Высокое качество генерации без артефактов</li>
                <li>Возможность контролируемой генерации через conditioning</li>
                <li>Параллельная генерация для ускорения инференса</li>
            </ul>
            <div class="kmp12"><strong>Важно:</strong> Современные диффузионные модели достигают субсекундной латентности благодаря оптимизированным сэмплерам и дистилляции.</div>
        </div>
        <div class="002-card">
            <h4>Мультиязычность и масштабирование</h4>
            <p>Современные TTS-системы поддерживают 55+ языков в единой модели благодаря использованию многоязычных представлений и универсальных фонетических энкодеров. Модели обучаются на массивных многоязычных корпусах с использованием техник transfer learning.</p>
            <p>Ключевые достижения включают zero-shot синтез для языков с ограниченными ресурсами и способность переключаться между языками внутри одного высказывания с сохранением естественности.</p>
        </div>
    </div>
    <div class="003">
        <h3 class="003-title">Открытые решения и доступность</h3>
        <div class="003-card">
            <h4>Экосистема open-source TTS</h4>
            <p>Развитие открытых TTS-решений демократизирует доступ к технологиям синтеза речи. Проекты как Coqui TTS, Mozilla TTS и различные реализации на HuggingFace предоставляют исследователям и разработчикам мощные инструменты.</p>
            <p><strong>Популярные открытые фреймворки:</strong></p>
            <ul>
                <li>Coqui TTS - продолжение Mozilla TTS с поддержкой новейших архитектур</li>
                <li>ESPnet - комплексный toolkit для речевых технологий</li>
                <li>SpeechBrain - модульная платформа для речевого LLM</li>
                <li>PaddleSpeech - фреймворк с фокусом на производительность</li>
            </ul>
            <div class="kmp11"><strong>Примечание:</strong> Многие открытые модели достигают качества коммерческих решений при правильной настройке и обучении.</div>
        </div>
    </div>
</section>

<section id="3" class="section">
    <h2 class="section-title">Лингвистические аспекты в генеративном TTS</h2>
    <div class="004">
        <h3 class="004-title">Моделирование просодии и интонации</h3>
        <div class="004-card">
            <h4>Просодическое моделирование в нейронных сетях</h4>
            <p>Современные TTS-системы моделируют просодию через многоуровневые представления, включающие pitch (F0), энергию, длительность и спектральные характеристики. Генеративные модели учатся извлекать просодические паттерны непосредственно из данных без явной разметки.</p>
            <p>Использование вариационных автоэнкодеров (VAE) позволяет создавать латентные просодические представления, которые можно модифицировать для изменения эмоциональной окраски или стиля речи.</p>
        </div>
        <div class="004-card">
            <h4>Семантическая интонация и контекст</h4>
            <p>Интеграция контекстуальных языковых моделей (BERT, RoBERTa) в TTS-пайплайн обеспечивает понимание семантики текста для правильной интонации. Модель учится связывать синтаксические структуры, семантические роли и прагматические функции с соответствующими интонационными контурами.</p>
            <p><strong>Уровни контекстного анализа:</strong></p>
            <ul>
                <li>Лексический - ударения и фонетические особенности слов</li>
                <li>Синтаксический - фразовые границы и синтагматическое членение</li>
                <li>Семантический - выделение фокуса и контраста</li>
                <li>Прагматический - речевые акты и коммуникативные намерения</li>
            </ul>
        </div>
    </div>
    <div class="005">
        <h3 class="005-title">Языковая специфика и типологические вызовы</h3>
        <div class="005-card">
            <h4>Тональные языки</h4>
            <p>Синтез тональных языков (китайский, вьетнамский, тайский) требует точного моделирования лексических тонов, которые несут смыслоразличительную функцию. Генеративные модели должны учитывать взаимодействие тонов с интонацией на уровне высказывания.</p>
            <p>Современные подходы используют отдельные тональные эмбеддинги и специализированные слои для моделирования тоновых сандхи и коартикуляционных эффектов.</p>
            <div class="kmp14"><strong>Пояснение:</strong> В мандаринском китайском четыре основных тона могут изменяться в зависимости от контекста, что требует сложного моделирования фонологических правил.</div>
        </div>
        <div class="005-card">
            <h4>Агглютинативные языки</h4>
            <p>Языки с богатой морфологией (турецкий, финский, японский) представляют вызов из-за длинных словоформ и сложных морфофонологических процессов. TTS-системы должны правильно обрабатывать морфемные границы и связанные с ними просодические изменения.</p>
            <p><strong>Стратегии обработки:</strong></p>
            <ul>
                <li>Морфологический анализ на этапе препроцессинга</li>
                <li>Субсловные токенизаторы (BPE, SentencePiece)</li>
                <li>Специальные архитектуры для длинных последовательностей</li>
                <li>Учёт гармонии гласных и других фонологических процессов</li>
            </ul>
        </div>
        <div class="005-card">
            <h4>Полисинтетические языки</h4>
            <p>Языки с полисинтетической структурой (инуктитут, навахо) требуют особого подхода из-за возможности выражения целых предложений одним словом. Модели должны обрабатывать сложные внутрисловные структуры и их просодические последствия.</p>
        </div>
    </div>
    <div class="006">
        <h3 class="006-title">Корпусная лингвистика и данные</h3>
        <div class="006-card">
            <h4>Влияние речевых корпусов на качество моделей</h4>
            ```html
            <p>Качество и разнообразие обучающих данных критически влияют на способности TTS-систем. Корпусы типа LibriSpeech, Common Voice и специализированные датасеты определяют лингвистическое покрытие и естественность синтезированной речи.</p>
            <p><strong>Ключевые характеристики эффективных корпусов:</strong></p>
            <ul>
                <li>Фонетическая сбалансированность и покрытие всех фонем языка</li>
                <li>Разнообразие дикторов (пол, возраст, диалекты)</li>
                <li>Различные стили речи (чтение, спонтанная речь, эмоциональная речь)</li>
                <li>Качественная транскрипция и временная разметка</li>
            </ul>
            <div class="kmp12"><strong>Важно:</strong> Несбалансированность корпуса может привести к систематическим ошибкам в редких фонетических контекстах или недопредставленных диалектах.</div>
        </div>
        <div class="006-card">
            <h4>Аннотация и лингвистическая разметка</h4>
            <p>Современные TTS-системы benefit от богатой лингвистической аннотации, включающей просодическую разметку (ToBI), морфологический анализ, синтаксические деревья и семантические роли. Автоматические инструменты разметки интегрируются в пайплайны подготовки данных.</p>
            <p>Использование forced alignment tools и автоматических транскрипторов ускоряет создание размеченных корпусов, хотя ручная проверка остаётся необходимой для высококачественных датасетов.</p>
        </div>
    </div>
</section>

<section id="4" class="section">
    <h2 class="section-title">Применения, этика и перспективы</h2>
    <div class="007">
        <h3 class="007-title">Практические применения</h3>
        <div class="007-card">
            <h4>Голосовые ассистенты нового поколения</h4>
            <p>Интеграция генеративного TTS в голосовые ассистенты обеспечивает более естественное и контекстно-зависимое взаимодействие. Системы способны адаптировать стиль речи под ситуацию, выражать эмпатию и поддерживать длительные диалоги с сохранением консистентности голоса.</p>
            <p><strong>Ключевые улучшения в 2025:</strong></p>
            <ul>
                <li>Эмоциональная адаптивность и понимание контекста</li>
                <li>Мультимодальная интеграция (жесты, мимика в AR/VR)</li>
                <li>Персонализация под предпочтения пользователя</li>
                <li>Поддержка code-switching в многоязычных контекстах</li>
            </ul>
        </div>
        <div class="007-card">
            <h4>Образовательные технологии</h4>
            <p>TTS революционизирует образование через создание интерактивных обучающих материалов, персонализированных языковых тренажёров и систем для изучения произношения. Технология обеспечивает доступность образовательного контента для людей с дислексией и нарушениями зрения.</p>
            <p>Адаптивные системы могут изменять скорость речи, чёткость артикуляции и сложность языка в зависимости от уровня учащегося.</p>
        </div>
        <div class="007-card">
            <h4>Технологии доступности</h4>
            <p>Генеративный TTS играет критическую роль в создании инклюзивных технологий. Системы чтения с экрана становятся более выразительными, а коммуникационные устройства для людей с речевыми нарушениями - более персонализированными.</p>
            <p><strong>Инновационные решения включают:</strong></p>
            <ul>
                <li>Восстановление голоса для пациентов после ларингэктомии</li>
                <li>Системы аугментативной коммуникации с естественной речью</li>
                <li>Real-time субтитрирование с аудиодескрипцией</li>
                <li>Мультисенсорные интерфейсы для слепоглухих пользователей</li>
            </ul>
        </div>
    </div>
    <div class="008">
        <h3 class="008-title">Этические вызовы и регулирование</h3>
        <div class="008-card">
            <h4>Проблема deepfakes и злоупотреблений</h4>
            <p>Способность клонировать голоса с высокой точностью создаёт риски для кибербезопасности, включая голосовой фишинг, подделку доказательств и нарушение privacy. Индустрия разрабатывает технологии детекции синтетической речи и системы водяных знаков.</p>
            <p>Регуляторные инициативы 2025 года включают обязательную маркировку синтетического контента и криминализацию создания deepfakes без согласия.</p>
            <div class="kmp12"><strong>Важно:</strong> Образовательные программы по цифровой грамотности должны включать распознавание синтетической речи.</div>
        </div>
        <div class="008-card">
            <h4>Bias и справедливость в TTS</h4>
            <p>Системы TTS могут воспроизводить и усиливать социальные предубеждения, присутствующие в обучающих данных. Это проявляется в недопредставленности определённых акцентов, диалектов или социолектов.</p>
            <p><strong>Стратегии митигации bias:</strong></p>
            <ul>
                <li>Диверсификация обучающих корпусов</li>
                <li>Аудит моделей на предмет дискриминации</li>
                <li>Инклюзивный дизайн с участием маргинализированных сообществ</li>
                <li>Прозрачность в документации ограничений систем</li>
            </ul>
        </div>
        <div class="008-card">
            <h4>Privacy и защита голосовых данных</h4>
            <p>Голос является биометрическим идентификатором, требующим особой защиты. Современные подходы включают федеративное обучение, дифференциальную приватность и гомоморфное шифрование для защиты голосовых данных.</p>
            <p>Пользователи должны иметь контроль над использованием их голосовых данных, включая право на удаление и ограничение использования.</p>
        </div>
    </div>
    <div class="009">
        <h3 class="009-title">Будущие направления развития</h3>
        <div class="009-card">
            <h4>Интеграция с AR/VR и метавселенными</h4>
            <p>TTS становится ключевым компонентом иммерсивных виртуальных миров, обеспечивая реалистичные голоса для аватаров и NPC. Пространственный аудио и контекстная адаптация создают убедительные виртуальные взаимодействия.</p>
            <p>Развитие включает real-time lip-sync, эмоциональную синхронизацию с анимацией лица и адаптацию к виртуальной акустике помещений.</p>
        </div>
        <div class="009-card">
            <h4>Нейроинтерфейсы и прямой синтез мыслей</h4>
            <p>Исследования в области brain-computer interfaces открывают возможности для синтеза речи непосредственно из нейронной активности. Это революционизирует коммуникацию для людей с тяжёлыми двигательными нарушениями.</p>
            <p><strong>Технологические вызовы:</strong></p>
            <ul>
                <li>Декодирование намерений из ЭЭГ/ЭКоГ сигналов</li>
                <li>Сохранение индивидуальности голоса</li>
                <li>Минимизация латентности для естественного диалога</li>
                <li>Этические вопросы чтения мыслей</li>
            </ul>
        </div>
        <div class="009-card">
            <h4>Квантовые вычисления в TTS</h4>
            <p>Потенциал квантовых компьютеров для оптимизации нейронных архитектур и ускорения обучения открывает новые горизонты. Квантовые алгоритмы могут революционизировать обработку высокоразмерных акустических пространств.</p>
            <div class="kmp11"><strong>Примечание:</strong> Практическое применение квантовых вычислений в TTS остаётся в стадии исследований, но потенциал огромен.</div>
        </div>
    </div>
</section>

<section id="5" class="section">
    <h2 class="section-title">Ключевые термины и определения</h2>
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Английский термин</th>
                    <th>Русский эквивалент</th>
                    <th>Пояснение</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Text-to-Speech (TTS)</td>
                    <td>Синтез речи</td>
                    <td>Технология преобразования письменного текста в звучащую речь</td>
                </tr>
                <tr>
                    <td>Generative AI</td>
                    <td>Генеративный LLM</td>
                    <td>Класс моделей машинного обучения, способных создавать новый контент на основе изученных паттернов</td>
                </tr>
                <tr>
                    <td>Autoregressive model</td>
                    <td>Авторегрессионная модель</td>
                    <td>Модель, генерирующая последовательность элемент за элементом, где каждый зависит от предыдущих</td>
                </tr>
                <tr>
                    <td>Vocoder</td>
                    <td>Вокодер</td>
                    <td>Компонент TTS-системы, преобразующий акустические признаки в аудиосигнал</td>
                </tr>
                <tr>
                    <td>Mel-spectrogram</td>
                    <td>Мел-спектрограмма</td>
                    <td>Представление звука через частотно-временной анализ с использованием мел-шкалы</td>
                </tr>
                <tr>
                    <td>Attention mechanism</td>
                    <td>Механизм внимания</td>
                    <td>Техника в нейронных сетях для фокусировки на релевантных частях входных данных</td>
                </tr>
                <tr>
                    <td>Voice cloning</td>
                    <td>Клонирование голоса</td>
                    <td>Создание синтетической речи, имитирующей голос конкретного человека</td>
                </tr>
                <tr>
                    <td>Prosody</td>
                    <td>Просодия</td>
                    <td>Супрасегментные характеристики речи: интонация, ритм, ударение, темп</td>
                </tr>
                <tr>
                    <td>Diffusion model</td>
                    <td>Диффузионная модель</td>
                    <td>Генеративная модель, создающая данные через постепенное удаление шума</td>
                </tr>
                <tr>
                    <td>Few-shot learning</td>
                    <td>Обучение на малых данных</td>
                    <td>Способность модели адаптироваться к новым задачам с минимальным количеством примеров</td>
                </tr>
                <tr>
                    <td>Latency</td>
                    <td>Задержка/латентность</td>
                    <td>Время между подачей текста и началом генерации речи</td>
                </tr>
                <tr>
                    <td>Speech-to-Speech (S2S)</td>
                    <td>Преобразование речи в речь</td>
                    <td>Прямая трансформация речевого сигнала без промежуточного текста</td>
                </tr>
                <tr>
                    <td>Embedding</td>
                    <td>Эмбеддинг/векторное представление</td>
                    <td>Числовое представление лингвистических единиц в многомерном пространстве</td>
                </tr>
                <tr>
                    <td>Transfer learning</td>
                    <td>Перенос обучения</td>
                    <td>Использование знаний, полученных при решении одной задачи, для улучшения решения другой</td>
                </tr>
                <tr>
                    <td>Zero-shot synthesis</td>
                    <td>Синтез без примеров</td>
                    <td>Генерация речи для языков или голосов, отсутствовавших в обучающих данных</td>
                </tr>
                <tr>
                    <td>Forced alignment</td>
                    <td>Принудительное выравнивание</td>
                    <td>Автоматическое определение временных границ фонем в аудиозаписи</td>
                </tr>
                                <tr>
                    <td>Code-switching</td>
                    <td>Переключение кодов</td>
                    <td>Смена языка или диалекта внутри одного высказывания</td>
                </tr>
                <tr>
                    <td>Deepfake</td>
                    <td>Дипфейк</td>
                    <td>Синтетический медиаконтент, созданный с помощью LLM для имитации реального человека</td>
                </tr>
                <tr>
                    <td>Bias</td>
                    <td>Смещение/предвзятость</td>
                    <td>Систематические ошибки модели, отражающие неравномерность обучающих данных</td>
                </tr>
                <tr>
                    <td>Federated learning</td>
                    <td>Федеративное обучение</td>
                    <td>Метод обучения моделей на распределённых данных без их централизации</td>
                </tr>
                <tr>
                    <td>Pitch (F0)</td>
                    <td>Основной тон/высота голоса</td>
                    <td>Частота основного тона голоса, определяющая воспринимаемую высоту</td>
                </tr>
                <tr>
                    <td>Phoneme</td>
                    <td>Фонема</td>
                    <td>Минимальная смыслоразличительная единица звукового строя языка</td>
                </tr>
                <tr>
                    <td>Coarticulation</td>
                    <td>Коартикуляция</td>
                    <td>Взаимное влияние соседних звуков друг на друга при произнесении</td>
                </tr>
                <tr>
                    <td>Tone sandhi</td>
                    <td>Тоновые сандхи</td>
                    <td>Изменение тонов в тональных языках в зависимости от контекста</td>
                </tr>
                <tr>
                    <td>Agglutinative language</td>
                    <td>Агглютинативный язык</td>
                    <td>Язык, в котором грамматические значения выражаются последовательным присоединением аффиксов</td>
                </tr>
                <tr>
                    <td>Polysynthetic language</td>
                    <td>Полисинтетический язык</td>
                    <td>Язык, способный выражать сложные значения в рамках одного слова</td>
                </tr>
                <tr>
                    <td>ToBI (Tones and Break Indices)</td>
                    <td>ТоБИ</td>
                    <td>Система транскрипции просодических явлений в речи</td>
                </tr>
                <tr>
                    <td>VAE (Variational Autoencoder)</td>
                    <td>Вариационный автоэнкодер</td>
                    <td>Генеративная модель для создания латентных представлений данных</td>
                </tr>
                <tr>
                    <td>GAN (Generative Adversarial Network)</td>
                    <td>Генеративно-состязательная сеть</td>
                    <td>Архитектура из двух нейросетей (генератор и дискриминатор), обучающихся в процессе соревнования</td>
                </tr>
                <tr>
                    <td>Dilated convolution</td>
                    <td>Расширенная свёртка</td>
                    <td>Свёрточная операция с пропусками для увеличения рецептивного поля</td>
                </tr>
                <tr>
                    <td>Causal convolution</td>
                    <td>Причинная свёртка</td>
                    <td>Свёртка, использующая только предшествующие элементы последовательности</td>
                </tr>
                <tr>
                    <td>Inference</td>
                    <td>Инференс/вывод</td>
                    <td>Процесс генерации предсказаний обученной моделью</td>
                </tr>
                <tr>
                    <td>Distillation</td>
                    <td>Дистилляция</td>
                    <td>Перенос знаний из большой модели в меньшую для ускорения работы</td>
                </tr>
                <tr>
                    <td>BPE (Byte Pair Encoding)</td>
                    <td>Кодирование пар байтов</td>
                    <td>Алгоритм сжатия для создания субсловного словаря токенов</td>
                </tr>
                <tr>
                    <td>Watermarking</td>
                    <td>Водяные знаки</td>
                    <td>Встраивание скрытых меток в синтетическую речь для идентификации</td>
                </tr>
                <tr>
                    <td>Differential privacy</td>
                    <td>Дифференциальная приватность</td>
                    <td>Математический фреймворк для защиты индивидуальных данных при обучении</td>
                </tr>
                <tr>
                    <td>Brain-Computer Interface (BCI)</td>
                    <td>Нейрокомпьютерный интерфейс</td>
                    <td>Система прямой связи между мозгом и внешним устройством</td>
                </tr>
                <tr>
                    <td>EEG/ECoG</td>
                    <td>ЭЭГ/ЭКоГ</td>
                    <td>Электроэнцефалография/электрокортикография - методы регистрации электрической активности мозга</td>
                </tr>
                <tr>
                    <td>Lip-sync</td>
                    <td>Синхронизация губ</td>
                    <td>Согласование движений губ с синтезированной речью</td>
                </tr>
                <tr>
                    <td>NPC (Non-Player Character)</td>
                    <td>Неигровой персонаж</td>
                    <td>Управляемый компьютером персонаж в играх или виртуальных мирах</td>
                </tr>
                <tr>
                    <td>Spatial audio</td>
                    <td>Пространственный звук</td>
                    <td>Технология создания трёхмерного звукового пространства</td>
                </tr>
                <tr>
                    <td>Mode collapse</td>
                    <td>Коллапс мод</td>
                    <td>Проблема в GAN, когда генератор производит ограниченное разнообразие выходов</td>
                </tr>
                <tr>
                    <td>Conditioning</td>
                    <td>Обусловливание</td>
                    <td>Управление генерацией модели через дополнительные входные параметры</td>
                </tr>
                <tr>
                    <td>Cross-attention</td>
                    <td>Кросс-внимание</td>
                    <td>Механизм внимания между различными модальностями или последовательностями</td>
                </tr>
                <tr>
                    <td>Meta-learning</td>
                    <td>Метаобучение</td>
                    <td>Обучение модели быстро адаптироваться к новым задачам</td>
                </tr>
                <tr>
                    <td>Contrastive learning</td>
                    <td>Контрастивное обучение</td>
                    <td>Метод обучения представлений через сравнение похожих и различных примеров</td>
                </tr>
            </tbody>
        </table>
    </div>
    <div class="kmp11"><strong>Примечание:</strong> Данный глоссарий охватывает основные термины, используемые в современных системах синтеза речи. Для углублённого изучения рекомендуется обращаться к специализированной литературе и документации конкретных фреймворков.</div>
    <div class="kmp14"><strong>Пояснение:</strong> Терминология в области TTS быстро эволюционирует с появлением новых технологий. Важно следить за актуальными публикациями и стандартами индустрии для поддержания терминологической точности.</div>
</section>
			
			
			

	
<footer class="footer">
<div class="container">
<p>© 2025 | kmp | CC BY-NC-SA 4.0<br>
Разработано для студентов БрГУ имени А.С. Пушкина</p>
</div>
</footer>
<div style="position: fixed; bottom: 10px; color: #777777; right: 30px; opacity: 0.3; font-size: 14px;">kmp+</div>
    <script>
        // Функция для плавной прокрутки к разделу
        function scrollToSection(sectionId) {
            const section = document.getElementById(sectionId);
            const menuHeight = document.querySelector('.menu').offsetHeight;
            
            window.scrollTo({
                top: section.offsetTop - menuHeight - 20,
                behavior: 'smooth'
            });
        }

        // Функция для переключения темы
        document.getElementById('themeToggle').addEventListener('click', function() {
            const currentTheme = document.documentElement.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            
            document.documentElement.setAttribute('data-theme', newTheme);
            this.textContent = newTheme === 'dark' ? '🌙' : '☀️';
        });

        // Анимация появления секций при прокрутке
        document.addEventListener('DOMContentLoaded', function() {
            const sections = document.querySelectorAll('.section');
            
            const observerOptions = {
                root: null,
                rootMargin: '0px',
                threshold: 0.1
            };
            
            const observer = new IntersectionObserver(function(entries, observer) {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('fade-in');
                        observer.unobserve(entry.target);
                    }
                });
            }, observerOptions);
            
            sections.forEach(section => {
                section.classList.remove('fade-in');
                observer.observe(section);
            });
        });
    </script>
</body>
</html>