<!DOCTYPE html>
<html lang="ru">
<head>
     <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>kmp+</title>
    <style>
        /* Основные стили */
        :root {
            --primary-color: #325980;
            --secondary-color: #4CAF50;
            --background-color: #f5f5f5;
            --content-bg: #ffffff;
            --text-color: #333333;
            --header-text-color: #ffffff;
            --menu-bg: #ffffff;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            --border-radius: 8px;
            --accent11: #4caf50;
            --accent12: #4cafff;
            --accent13: #ffaf50;
            --accent14: #821978;
        }

        /* Темная тема */
        [data-theme="dark"] {
            --primary-color: #3e76ad;
            --secondary-color: #388e3c;
            --background-color: #000000;
            --content-bg: #1e1e1e;
            --text-color: #e0e0e0;
            --header-text-color: #ffffff;
            --menu-bg: #000000;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            transition: background-color 0.3s, color 0.3s;
        }

        body {
            font-family: 'Roboto', 'Arial', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--background-color);
            padding-top: 2px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 15px;
        }

        header {
            background-color: var(--primary-color);
            color: var(--header-text-color);
            padding: 20px 0;
            text-align: center;
            border-radius: var(--border-radius);
            margin-bottom: 2px;
        }

        h1 {
            font-size: 2.2rem;
            margin-bottom: 10px;
        }

        h2 {
            color: var(--primary-color);
            margin: 25px 0 15px;
            padding-bottom: 8px;
            border-bottom: 2px solid var(--secondary-color);
        }

        h3 {
            color: var(--primary-color);
            margin: 20px 0 10px;
        }

        p {
            margin-bottom: 15px;
        }

        /* Мобильное меню */
        .mobile-menu-container {
            position: sticky;
            top: 0;
            z-index: 1000;
            background-color: var(--menu-bg);
            box-shadow: var(--menu-shadow);
            border-radius: 0 0 var(--border-radius) var(--border-radius);
        }

        .mobile-menu-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 15px 20px;
        }

        .hamburger-btn {
            background: none;
            border: none;
            font-size: 1.8rem;
            cursor: pointer;
            color: var(--primary-color);
            padding: 5px;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .theme-toggle {
            background: none;
            border: none;
            font-size: 1.5rem;
            cursor: pointer;
            color: var(--primary-color);
            padding: 5px;
        }

        .mobile-menu {
            display: none;
            flex-direction: column;
            padding: 0 20px 20px;
            background-color: var(--menu-bg);
        }

        .mobile-menu.active {
            display: flex;
            animation: slideDown 0.3s ease;
        }

        @keyframes slideDown {
            from {
                opacity: 0;
                transform: translateY(-10px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .mobile-menu-btn {
            background-color: var(--primary-color);
            color: white;
            border: none;
            padding: 12px 15px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 1rem;
            transition: background-color 0.3s;
            margin-bottom: 10px;
            text-align: left;
            width: 100%;
        }

        .mobile-menu-btn:hover {
            background-color: var(--secondary-color);
        }

        /* Десктопное меню */
        .desktop-menu {
            display: none;
            background-color: var(--menu-bg);
            padding: 15px 20px;
            border-radius: var(--border-radius);
            margin-bottom: 30px;
            box-shadow: var(--menu-shadow);
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .desktop-menu-buttons {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }

        .desktop-menu-btn {
            background-color: var(--primary-color);
            color: white;
            border: none;
            padding: 8px 15px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: background-color 0.3s;
        }

        .desktop-menu-btn:hover {
            background-color: var(--secondary-color);
        }

        /* Секции контента */
        .section {
            background-color: var(--content-bg);
            border-radius: var(--border-radius);
            padding: 25px;
            margin-bottom: 30px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        /* Выделение важного */
        .important {
            background-color: rgba(76, 175, 80, 0.1);
            border-left: 4px solid var(--secondary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        .note {
            background-color: rgba(50, 89, 128, 0.1);
            border-left: 4px solid var(--primary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        /* Адаптивные таблицы */
        .table-container {
            width: 100%;
            overflow-x: auto;
            margin: 20px 0;
            border-radius: var(--border-radius);
            border: 1px solid #ddd;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            min-width: 600px; /* Минимальная ширина для таблиц */
        }

        th, td {
            padding: 12px 15px;
            border: 1px solid #ddd;
            text-align: left;
        }

        th {
            background-color: var(--primary-color);
            color: white;
            position: sticky;
            left: 0;
        }

        tr:nth-child(even) {
            background-color: rgba(0, 0, 0, 0.03);
        }

        /* Для мобильных - альтернативный вид таблиц */
        .responsive-table {
            display: block;
            width: 100%;
        }

        .responsive-table tr {
            display: block;
            margin-bottom: 15px;
            border: 1px solid #ddd;
            border-radius: var(--border-radius);
            padding: 10px;
            background-color: var(--content-bg);
        }

        .responsive-table td {
            display: block;
            text-align: right;
            padding: 8px 10px;
            border: none;
            border-bottom: 1px solid #eee;
        }

        .responsive-table td:before {
            content: attr(data-label);
            float: left;
            font-weight: bold;
            color: var(--primary-color);
        }

        .responsive-table thead {
            display: none;
        }

        /* Списки */
        ul, ol {
            padding-left: 25px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
        }

        /* Адаптивный дизайн */
        @media (min-width: 769px) {
            .desktop-menu {
                display: flex;
            }
            
            .mobile-menu-container {
                display: none;
            }
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.8rem;
            }
            
            h2 {
                font-size: 1.5rem;
            }
            
            .section {
                padding: 15px;
            }
            
            .container {
                padding: 0 10px;
            }
            
            /* На мобильных используем responsive таблицы */
            .table-kmp table:not(.responsive-table) {
                display: none;
            }
            
            .table-kmp .responsive-table {
                display: block;
            }
        }

        @media (min-width: 769px) {
            .table-kmp .responsive-table {
                display: none;
            }
        }

        /* Анимации */
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .fade-in {
            animation: fadeIn 0.5s ease-in;
        }

        footer {
            text-align: center;
            padding: 20px 0;
            margin-top: 40px;
            font-size: 0.9rem;
        }
        
        .kmp11, .example {
            background: rgba(76, 175, 80, 0.1);
            border-left: 4px solid var(--accent11);
            padding: 10px 15px;
            margin: 15px 0;
            border-radius: 4px;
        }

        .kmp12, .example {
            background: rgba(95, 182, 237, 0.1);
            border-left: 4px solid var(--accent12);
            padding: 10px 15px;
            margin: 15px 0;
            border-radius: 4px;
        }
 
        .kmp13, .example {
            background: rgba(205, 170, 110, 0.1);
            border-left: 4px solid var(--accent13);
            padding: 10px 15px;
            margin: 15px 0;
            border-radius: 4px;
        }

        .kmp14, .example {
            background: rgba(205, 110, 200, 0.1);
            border-left: 4px solid var(--accent14);
            padding: 10px 15px;
            margin: 15px 0;
            border-radius: 4px;
        }
        
        .back-to-top {
            position: fixed;
            bottom: 20px;
            right: 20px;
            background-color: var(--primary-color);
            color: white;
            border: none;
            border-radius: 50%;
            width: 50px;
            height: 50px;
            font-size: 1.2rem;
            cursor: pointer;
            display: none;
            justify-content: center;
            align-items: center;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.2);
            z-index: 999;
        }
        
        .back-to-top.show {
            display: flex;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>LLM Learning</h1>
            <p>модели обучения больших языковых моделей</p>
        </header>

        
        <div class="mobile-menu-container">
    <div class="mobile-menu-header">
        <button class="hamburger-btn" id="hamburgerBtn" title="Открыть меню">☰</button>
        <button class="theme-toggle" id="themeToggle" title="Переключить тему">☀️</button>
    </div>
    <div class="mobile-menu" id="mobileMenu">
        <button class="mobile-menu-btn" onclick="scrollToSection('0')">Intro</button>
        <button class="mobile-menu-btn" onclick="scrollToSection('1')">Language</button>
        <button class="mobile-menu-btn" onclick="scrollToSection('2')">Emergent</button>
        <button class="mobile-menu-btn" onclick="scrollToSection('3')">Learning</button>
        <button class="mobile-menu-btn" onclick="scrollToSection('4')">Pipeline</button>
		<button class="mobile-menu-btn" onclick="scrollToSection('5')">AI-context </button>
    </div>
</div>

<nav class="desktop-menu">
    <div class="desktop-menu-buttons">
        <button class="desktop-menu-btn" onclick="scrollToSection('0')">Intro</button>
        <button class="desktop-menu-btn" onclick="scrollToSection('1')">Language</button>
        <button class="desktop-menu-btn" onclick="scrollToSection('2')">Emergent</button>
        <button class="desktop-menu-btn" onclick="scrollToSection('3')">Learning</button>
        <button class="desktop-menu-btn" onclick="scrollToSection('4')">Pipeline</button>
		<button class="desktop-menu-btn" onclick="scrollToSection('5')">AI-context </button>
    </div>
    <button class="theme-toggle" id="desktopThemeToggle" title="Переключить тему">☀️</button>
</nav>


<section id="0" class="section">
    <h2 class="section-title">Large Language Model</h2>
    <div class="intro-block">
        <h3 class="intro-title">Large Language Model — Большая языковая модель</h3>
        <div class="intro-card">
            <p><strong>LLM (Large Language Model)</strong> — это класс эмерджентных нейросетевых моделей, обученных на огромных массивах текстовых данных для понимания, генерации и трансформации естественного языка.</p>
            <p>Для лингвиста LLM представляет особый интерес как <em>вычислительная модель языковой компетенции</em> — система, которая демонстрирует поведение, схожее с человеческим владением языком, но достигает этого принципиально иным путём.</p>
            <p><strong>Ключевые характеристики:</strong></p>
            <ul>
                <li><strong>Large (Большая):</strong> миллиарды обучаемых параметров (весов)</li>
                <li><strong>Language (Языковая):</strong> специализация на естественном языке</li>
                <li><strong>Model (Модель):</strong> математическое представление языковых феноменов</li>
            </ul>
            <p><strong>Примеры LLM:</strong> Grok, Claude, Gemini, Mistral</p>
        </div>
    </div>
    <div class="kmp12"><strong>Важно:</strong> следует различать LLM (стохастическую вероятностную эмерджентную систему, которая «вычисляет» каждый ответ заново на основе усвоенных паттернов языка) и LLM-сервисы.</div>
	
</section>

<section id="1" class="section">
    <h2 class="section-title">1. Language Comprehension</h2>
    
    <div class="paradigm-shift">
        <h3 class="paradigm-title">1.1. Смена парадигм: от правил к вероятностям</h3>
        <div class="paradigm-card">
            <p>История компьютерной лингвистики демонстрирует фундаментальный сдвиг в понимании того, как машина может «работать» с языком.</p>
            <p><strong>Три этапа развития:</strong></p>
            <ul>
                <li><strong>Rule-based (1950–1980-е):</strong> Язык описывается формальными грамматиками и словарями. Пример: машинный перевод через синтаксический разбор.</li>
                <li><strong>Статистические методы (1990–2010-е):</strong> Вероятностные модели на основе корпусов. N-граммы, скрытые марковские модели.</li>
                <li><strong>Нейросетевая революция (2017+):</strong> Архитектура Transformer, обучение end-to-end на сырых данных.</li>
            </ul>
        </div>
    </div>
    
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Парадигма</th>
                    <th>Основа</th>
                    <th>Пример задачи</th>
                    <th>Ограничение</th>
                </tr>
            </thead>
            <tr>
                <td>Rule-based</td>
                <td>Явные правила лингвиста</td>
                <td>ELIZA (1966)</td>
                <td>Хрупкость, невозможность охватить все случаи</td>
            </tr>
            <tr>
                <td>Статистическая</td>
                <td>Частотность в корпусе</td>
                <td>Google Translate (ранний)</td>
                <td>Нет глубокого понимания контекста</td>
            </tr>
            <tr>
                <td>Нейросетевая (LLM)</td>
                <td>Распределённые представления</td>
                <td>ChatGPT, Claude</td>
                <td>«Чёрный ящик», галлюцинации</td>
            </tr>
        </table>
    </div>
    
    <div class="distributional">
        <h3 class="distributional-title">1.2. Дистрибутивная семантика: теоретический фундамент</h3>
        <div class="distributional-card">
            <p>LLM реализует на практике идеи, сформулированные лингвистами XX века:</p>
            <p><strong>Джон Р. Фёрт (1957):</strong> <em>«You shall know a word by the company it keeps»</em> — «Слово узнаётся по его окружению».</p>
            <p><strong>Зеллиг Харрис (1954):</strong> Слова со схожим дистрибутивным профилем (встречающиеся в похожих контекстах) имеют схожее значение.</p>
            <p><strong>Как это работает в LLM:</strong></p>
            <ul>
                <li>Модель анализирует миллиарды контекстов употребления каждого слова</li>
                <li>Слова с похожими паттернами сочетаемости получают близкие векторные представления</li>
                <li>Семантическая близость = геометрическая близость в многомерном пространстве</li>
            </ul>
            <p><strong>Пример:</strong> Слова «врач» и «доктор» в обучающем корпусе встречаются в схожих контекстах («принимает пациентов», «выписал рецепт»), поэтому их векторы оказываются близки.</p>
        </div>
    </div>
    <div class="kmp14"><strong>Пояснение:</strong> Дистрибутивная гипотеза объясняет, почему LLM может работать с синонимией, полисемией и даже метафорой — все эти явления отражаются в паттернах совместной встречаемости слов.</div>
    
    <div class="transformer">
        <h3 class="transformer-title">1.3. Архитектура Transformer: параллельная обработка</h3>
        <div class="transformer-card">
            <p><strong>Transformer</strong> (Vaswani et al., 2017) — архитектура нейронной сети, ставшая основой всех современных LLM.</p>
            <p><strong>Революционное отличие от предшественников:</strong></p>
            <ul>
                <li><strong>RNN/LSTM:</strong> обрабатывали текст последовательно (слово за словом) → медленно, теряли «длинные» связи</li>
                <li><strong>Transformer:</strong> «видит» весь текст одновременно → быстро, сохраняет дальние зависимости</li>
            </ul>
            <p><strong>Лингвистическая аналогия:</strong> Представьте читателя, который не скользит глазами по строчке, а охватывает всю страницу целиком, мгновенно устанавливая связи между любыми её частями.</p>
        </div>
    </div>
    
    <div class="attention">
        <h3 class="attention-title">1.4. Self-Attention: механизм внимания</h3>
        <div class="attention-card">
            <p><strong>Self-Attention</strong> — ключевой механизм Transformer, позволяющий каждому элементу текста «обращать внимание» на все остальные элементы.</p>
            <p><strong>Как это работает (упрощённо):</strong></p>
            <ul>
                <li>Для каждого токена вычисляется «вопрос» (Query): «На что мне обратить внимание?»</li>
                <li>Каждый токен также имеет «ключ» (Key): «Вот что я могу предложить»</li>
                <li>И «значение» (Value): «Вот моя семантическая информация»</li>
                <li>Модель вычисляет «силу связи» между всеми парами токенов</li>
            </ul>
            <p><strong>Лингвистические задачи, решаемые Attention:</strong></p>
            <ul>
                <li><strong>Разрешение анафоры:</strong> «Мария позвонила Анне. <em>Она</em> была рада» — модель вычисляет, к кому относится «она»</li>
                <li><strong>Снятие многозначности:</strong> «Ключ от замка» vs. «Скрипичный ключ» — контекст определяет значение</li>
                <li><strong>Синтаксические связи:</strong> Согласование подлежащего и сказуемого на расстоянии</li>
            </ul>
        </div>
    </div>
    
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Лингвистическое явление</th>
                    <th>Как Attention помогает</th>
                </tr>
            </thead>
            <tr>
                <td>Тема-рематическое членение</td>
                <td>Выделение фокуса высказывания через веса внимания</td>
            </tr>
            <tr>
                <td>Кореференция (анафора, катафора)</td>
                <td>Связывание местоимений с антецедентами</td>
            </tr>
            <tr>
                <td>Полисемия</td>
                <td>Контекстуализация значения через окружение</td>
            </tr>
            <tr>
                <td>Эллипсис</td>
                <td>Восстановление опущенных элементов из контекста</td>
            </tr>
        </table>
    </div>
    
    <div class="tokenization">
        <h3 class="tokenization-title">1.5. Токенизация: субморфемные единицы</h3>
        <div class="tokenization-card">
            <p><strong>Токен</strong> — минимальная единица, которой оперирует LLM. Это <em>не слово</em> и часто <em>не морфема</em> в традиционном понимании.</p>
            <p><strong>Почему не слова?</strong></p>
            <ul>
                <li>Словарь всех словоформ был бы огромным (особенно для флективных языков)</li>
                <li>Новые слова (неологизмы) не попали бы в словарь</li>
                <li>Опечатки ломали бы систему</li>
            </ul>
            <p><strong>Решение — BPE (Byte Pair Encoding):</strong></p>
            <ul>
                <li>Алгоритм находит частотные последовательности символов</li>
                <li>Частые слова → один токен («the», «что»)</li>
                <li>Редкие слова → несколько токенов («не|йро|лингв|истика»)</li>
            </ul>
            <p><strong>Пример токенизации (Claude):</strong></p>
            <ul>
                <li>«Hello» → 1 токен</li>
                <li>«Привет» → 1-2 токена</li>
                <li>«Нейропсихолингвистика» → 4-6 токенов</li>
            </ul>
        </div>
    </div>
    <div class="kmp11"><strong>Примечание:</strong> Токенизация объясняет, почему LLM иногда ошибается в подсчёте букв или слогов — модель «видит» текст как последовательность токенов, а не символов или слов.</div>
</section>

<section id="2" class="section">
    <h2 class="section-title">2. Emergent Abilities</h2>
    
    <div class="emergence-def">
        <h3 class="emergence-def-title">2.1. Определение эмерджентности</h3>
        <div class="emergence-def-card">
            <p><strong>Эмерджентные способности</strong> — навыки, которые:</p>
            <ul>
                <li>Отсутствуют у моделей меньшего размера</li>
                <li>Не были целью специального обучения</li>
                <li>Появляются «внезапно» при достижении определённого масштаба</li>
            </ul>
            <p>Это явление описывается принципом: <strong>количественные изменения переходят в качественные</strong>.</p>
            <p><strong>Философский контекст:</strong> Термин «эмерджентность» пришёл из философии сознания, где обозначает свойства системы, не сводимые к свойствам её частей (пример: сознание как эмерджентное свойство нейронов).</p>
        </div>
    </div>
    
    <div class="scaling">
        <h3 class="scaling-title">2.2. Scaling Laws: законы масштабирования</h3>
        <div class="scaling-card">
            <p><strong>Scaling Laws</strong> (OpenAI, 2020) — эмпирически установленные закономерности:</p>
            <ul>
                <li>Качество модели предсказуемо растёт с увеличением: числа параметров, объёма данных, вычислительных ресурсов</li>
                <li>Но некоторые способности появляются <em>скачкообразно</em></li>
            </ul>
            <p><strong>Примеры «скачков»:</strong></p>
            <ul>
                <li><strong>Арифметика:</strong> Модель с 10 млрд параметров: 10% точности. С 100 млрд: 80% точности.</li>
                <li><strong>Логические загадки:</strong> Только модели >50 млрд параметров справляются</li>
                <li><strong>Программирование:</strong> Внезапное появление способности генерировать работающий код</li>
            </ul>
        </div>
    </div>
    
    <div class="emergent-examples">
        <h3 class="emergent-examples-title">2.3. Примеры эмерджентных способностей</h3>
        <div class="emergent-examples-card">
            <p><strong>Способности, которым модель «не обучалась»:</strong></p>
            <ul>
                <li><strong>Chain of Thought (Цепочка рассуждений):</strong> Пошаговое логическое мышление при решении задач</li>
                <li><strong>Zero-shot перевод:</strong> Перевод между парами языков, которые не встречались вместе в обучении</li>
                <li><strong>Понимание метафор и иронии:</strong> Распознавание непрямых речевых актов</li>
                <li><strong>Генерация кода:</strong> Написание программ на языках программирования</li>
                <li><strong>Математические рассуждения:</strong> Решение задач, требующих многошаговой логики</li>
            </ul>
        </div>
    </div>
    
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Масштаб модели</th>
                    <th>Типичные способности</th>
                    <th>Примеры моделей</th>
                </tr>
            </thead>
            <tr>
                <td>&lt;1 млрд параметров</td>
                <td>Базовая генерация текста, простая классификация</td>
                <td>DistilBERT, BERT-base</td>
            </tr>
            <tr>
                <td>1-10 млрд параметров</td>
                <td>Качественный перевод, суммаризация</td>
                <td>T5, LLaMA-7B</td>
            </tr>
            <tr>
                <td>10-100 млрд параметров</td>
                <td>Chain of Thought, базовое программирование</td>
                <td>GPT-3, LLaMA-70B</td>
            </tr>
            <tr>
                <td>>100 млрд параметров</td>
                <td>Сложное рассуждение, экспертные задачи</td>
                <td>GPT-4, Claude 3</td>
            </tr>
        </table>
    </div>
    
    <div class="understanding-debate">
        <h3 class="understanding-debate-title">2.4. Дискуссия: понимание или аппроксимация?</h3>
        <div class="understanding-debate-card">
            <p><strong>Центральный вопрос для лингвистов:</strong> Демонстрирует ли LLM «понимание» языка или это лишь сверхсложная статистическая аппроксимация?</p>
            <p><strong>Позиция А: «Китайская комната» (Дж. Сёрл):</strong></p>
            <ul>
                <li>Манипуляция символами ≠ понимание</li>
                <li>Модель не имеет интенциональности и опыта</li>
                <li>Отсутствует связь языка с реальным миром (grounding)</li>
            </ul>
            <p><strong>Позиция Б: Функциональное понимание:</strong></p>
            <ul>
                <li>Если поведение неотличимо от понимающего — это и есть понимание</li>
                <li>Способность к переносу знаний указывает на абстрактные репрезентации</li>
                <li>Человеческое понимание тоже может быть «вычислением»</li>
            </ul>
            <p><strong>Связь с лингвистикой:</strong> Концепция <em>«языкового чутья»</em> (Sprachgefühl) — интуитивное ощущение правильности высказывания. LLM демонстрирует нечто функционально похожее.</p>
        </div>
    </div>
    <div class="kmp12"><strong>Важно:</strong> Эта дискуссия не имеет однозначного решения и затрагивает фундаментальные вопросы философии сознания и когнитивной науки. Для практической работы с LLM полезно сохранять «агностическую» позицию.</div>
</section>

<section id="3" class="section">
    <h2 class="section-title">3. Learning Process</h2>
    
    <div class="weights">
        <h3 class="weights-title">3.1. Оптимизация весов: что происходит при обучении</h3>
        <div class="weights-card">
            <p><strong>Веса (Weights)</strong> — числовые параметры нейронной сети, которые определяют её поведение. В современных LLM их миллиарды.</p>
            <p><strong>Процесс обучения:</strong></p>
            <ul>
                <li>Модель получает фрагмент текста с «закрытым» последним токеном</li>
                <li>Делает предсказание: какой токен следующий?</li>
                <li>Сравнивает предсказание с реальностью (Loss Function)</li>
                <li>Корректирует веса, чтобы уменьшить ошибку (Gradient Descent)</li>
            </ul>
            <p><strong>Loss Function (Функция потерь):</strong> Мера «удивления» модели. Чем менее вероятный токен модель предсказала, тем выше loss. Цель обучения — минимизировать суммарное «удивление».</p>
            <p><strong>Аналогия:</strong> Представьте, что вы учите иностранный язык, угадывая следующее слово в предложении. Каждая ошибка немного «подстраивает» вашу интуицию.</p>
        </div>
    </div>
    
    <div class="static-vs-dynamic">
        <h3 class="static-vs-dynamic-title">3.2. Статичные знания vs. Оперативная память</h3>
        <div class="static-vs-dynamic-card">
            <p><strong>Фундаментальное разграничение:</strong></p>
            <p><strong>Веса модели (Weights) — «долговременная память»:</strong></p>
            <ul>
                <li>Формируются во время обучения (месяцы вычислений)</li>
                <li><strong>Не меняются</strong> во время диалога с пользователем</li>
                <li>Содержат «знания о мире» и языковую компетенцию</li>
            </ul>
            <p><strong>Контекстное окно (Context Window) — «рабочая память»:</strong></p>
            <ul>
                <li>Вся история текущего диалога</li>
                <li>Ограничено по размеру (8K — 200K токенов)</li>
                <li>«Забывается» при начале нового диалога</li>
            </ul>
        </div>
    </div>
    
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Характеристика</th>
                    <th>Веса (Weights)</th>
                    <th>Контекст (Context Window)</th>
                </tr>
            </thead>
            <tr>
                <td>Когда формируется</td>
                <td>При обучении</td>
                <td>Во время диалога</td>
            </tr>
            <tr>
                <td>Изменяемость</td>
                <td>Неизменны при инференсе</td>
                <td>Обновляется с каждым сообщением</td>
            </tr>
            <tr>
                <td>Объём</td>
                <td>Миллиарды параметров</td>
                <td>Тысячи-сотни тысяч токенов</td>
            </tr>
            <tr>
                <td>Аналогия</td>
                <td>Языковая компетенция</td>
                <td>Краткосрочная память в разговоре</td>
            </tr>
        </table>
    </div>
    
    <div class="in-context">
        <h3 class="in-context-title">3.3. In-Context Learning: контекстная адаптация</h3>
        <div class="in-context-card">
            <p><strong>In-Context Learning (ICL)</strong> — способность модели адаптироваться к новой задаче на основе примеров, данных прямо в промпте, <em>без изменения весов</em>, основываясь только на примерах, предоставленных в промпте.  .</p>
			<p><strong>ICL</strong>  важно отличать от традиционного обучения (например, Fine-Tuning), где <em>веса модели изменяются</em></p>
			
			
            <p><strong>Типы:</strong></p>
            <ul>
                <li><strong>Zero-shot:</strong> Задача без примеров. «Переведи на французский: Hello»</li>
                <li><strong>One-shot:</strong> Один пример. «cat → кот. dog → ?»</li>
                <li><strong>Few-shot:</strong> Несколько примеров. «cat → кот, dog → собака, bird → ?»</li>
            </ul>
            <p><strong>Почему это работает?</strong> Модель во время обучения «видела» миллионы примеров задач с образцами решений. Она научилась паттерну «пример → решение» как метанавыку.</p>
            <p><strong>Лингвистическая параллель:</strong> Это похоже на то, как носитель языка, увидев несколько примеров нового словообразовательного паттерна, может экстраполировать его на новые случаи.</p>
			<p><strong>Контекстная адаптация</strong>  — это русскоязычный аналог In-Context Learning, который популярен в русскоязычных исследовательских кругах, где даже переводится на английский как contextual adaptation. Он точен, но менее стандартизирован, чем ICL. В англоязычной литературе чаще встречается именно In-Context Learning.</p>
			
        </div>
    </div>
    
    <div class="training-time">
        <h3 class="training-time-title">3.4. Время обучения vs. время ответа</h3>
        <div class="training-time-card">
            <p><strong>Контраст масштабов:</strong></p>
            <ul>
                <li><strong>Обучение:</strong> Недели и месяцы вычислений на тысячах GPU</li>
                <li><strong>Инференс:</strong> Миллисекунды на генерацию одного токена</li>
            </ul>
            <p><strong>Ресурсы на обучение GPT-4 (оценка):</strong></p>
            <ul>
                <li>~25 000 GPU A100</li>
                <li>~3-6 месяцев вычислений</li>
                <li>~$100 млн на вычисления</li>
            </ul>
            <p><strong>Инференс (генерация ответа):</strong></p>
            <ul>
                <li>Один GPU (или часть его)</li>
                <li>Секунды для полного ответа</li>
            </ul>
        </div>
    </div>
    <div class="kmp14"><strong>Пояснение:</strong> Это объясняет экономику LLM: обучение — огромные единовременные затраты, инференс — относительно дешёвая повторяемая операция.</div>
</section>

<section id="4" class="section">
    <h2 class="section-title">4. Training Pipeline</h2>
    
    <div class="pipeline-overview">
        <h3 class="pipeline-overview-title">4.0. Обзор этапов</h3>
        <div class="pipeline-overview-card">
            <p>Создание современной LLM — многоэтапный процесс, где каждый этап решает свою задачу:</p>
            <ul>
                <li><strong>Pre-training:</strong> Формирование базовой языковой компетенции</li>
                <li><strong>SFT:</strong> Обучение формату диалога и выполнению инструкций</li>
                <li><strong>RLHF/RLAIF:</strong> Выравнивание с человеческими предпочтениями</li>
            </ul>
        </div>
    </div>
    
    <div class="pretraining">
        <h3 class="pretraining-title">4.1. Pre-training (Предобучение)</h3>
        <div class="pretraining-card">
            <p><strong>Цель:</strong> Создать модель с обширной «языковой базой» — знанием грамматики, фактов, стилей, логики.</p>
            <p><strong>Данные:</strong></p>
            <ul>
                <li><strong>Common Crawl:</strong> Миллиарды веб-страниц</li>
                <li><strong>Wikipedia:</strong> Энциклопедические знания</li>
                <li><strong>Книги:</strong> Художественная и научная литература</li>
                <li><strong>Код:</strong> GitHub-репозитории</li>
                <li><strong>Научные статьи:</strong> arXiv, PubMed</li>
            </ul>
            <p><strong>Объём:</strong> Триллионы токенов (терабайты текста)</p>
            <p><strong>Задача:</strong> Предсказание следующего токена (Next Token Prediction) или восстановление маскированных токенов (Masked LM)</p>
        </div>
    </div>
    
    <div class="krashen">
        <h3 class="krashen-title">4.1.1. Параллель с Input Hypothesis С. Крашена</h3>
        <div class="krashen-card">
            <p><strong>Input Hypothesis</strong> (Стивен Крашен, 1980-е) — теория усвоения второго языка:</p>
            <p><em>«Мы усваиваем язык, получая понятный входной материал (comprehensible input), чуть превышающий наш текущий уровень (i+1)»</em></p>
            <p><strong>Параллель с Pre-training LLM:</strong></p>
            <ul>
                <li>Огромный объём языкового материала → формирование «интуитивной» грамматики</li>
                <li>Нет явного обучения правилам — только примеры употребления</li>
                <li>Статистические закономерности «впитываются» через exposure</li>
            </ul>
            <p><strong>Важное отличие:</strong> Крашен говорил о <em>понятном</em> входе. LLM не «понимает» в человеческом смысле — она оптимизирует предсказание.</p>
        </div>
    </div>
    <div class="kmp11"><strong>Примечание:</strong> Эта параллель — эвристика для понимания, а не точная аналогия. Механизмы усвоения языка человеком и обучения нейросети различаются фундаментально.</div>
    
    <div class="sft">
        <h3 class="sft-title">4.2. SFT (Supervised Fine-Tuning)</h3>
        <div class="sft-card">
            <p><strong>Цель:</strong> Превратить «сырую» языковую модель в полезного ассистента.</p>
            <p><strong>Проблема базовой модели:</strong> После pre-training модель умеет продолжать текст, но не умеет:</p>
            <ul>
                <li>Отвечать на вопросы (вместо этого может задать следующий вопрос)</li>
                <li>Следовать инструкциям</li>
                <li>Вести диалог</li>
            </ul>
            <p><strong>Решение:</strong> Обучение на размеченных парах «инструкция → правильный ответ»</p>
            <p><strong>Примеры обучающих данных:</strong></p>
            <ul>
                <li>Вопрос: «Что такое фотосинтез?» → Ответ: «Фотосинтез — это процесс...»</li>
                <li>Инструкция: «Переведи на английский: Добрый день» → Ответ: «Good afternoon»</li>
                <li>Задание: «Напиши формальное письмо с извинениями» → [образцовое письмо]</li>
            </ul>
        </div>
    </div>
    
    <div class="rlhf">
        <h3 class="rlhf-title">4.3. RLHF (Reinforcement Learning from Human Feedback)</h3>
        <div class="rlhf-card">
            <p><strong>Цель:</strong> «Выравнивание» (Alignment) модели с человеческими ценностями и предпочтениями.</p>
            <p><strong>Проблема после SFT:</strong> Модель может давать формально правильные, но нежелательные ответы:</p>
            <ul>
                <li>Слишком длинные или слишком короткие</li>
                <li>Тонально неуместные</li>
                <li>Потенциально вредные</li>
            </ul>
            <p><strong>Процесс RLHF:</strong></p>
            <ul>
                <li><strong>Шаг 1:</strong> Модель генерирует несколько вариантов ответа</li>
                <li><strong>Шаг 2:</strong> Люди-аннотаторы ранжируют ответы по качеству</li>
                <li><strong>Шаг 3:</strong> На основе рейтингов обучается Reward Model (модель вознаграждения)</li>
                <li><strong>Шаг 4:</strong> LLM дообучается максимизировать reward (Proximal Policy Optimization)</li>
            </ul>
        </div>
    </div>
    
    <div class="annotators">
        <h3 class="annotators-title">4.3.1. Роль лингвистов-аннотаторов</h3>
        <div class="annotators-card">
            <p><strong>Лингвисты в RLHF:</strong> Критически важная роль в формировании «ценностей» модели.</p>
            <p><strong>Задачи аннотаторов:</strong></p>
            <ul>
                <li>Оценка грамматической корректности</li>
                <li>Проверка фактической точности</li>
                <li>Оценка логической связности</li>
                <li>Выявление стилистических несоответствий</li>
                <li>Фильтрация вредного контента</li>
                <li>Оценка «полезности» ответа</li>
            </ul>
            <p><strong>Критерии ранжирования:</strong></p>
            <ul>
                <li><strong>Helpfulness:</strong> Насколько ответ полезен для пользователя</li>
                <li><strong>Harmlessness:</strong> Отсутствие потенциального вреда</li>
                <li><strong>Honesty:</strong> Честность, признание незнания</li>
            </ul>
        </div>
    </div>
    
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Этап</th>
                    <th>Данные</th>
                    <th>Цель</th>
                    <th>Результат</th>
                </tr>
            </thead>
            <tr>
                <td>Pre-training</td>
                <td>Триллионы токенов «сырого» текста</td>
                <td>Языковая компетенция</td>
                <td>Base model (предсказатель текста)</td>
            </tr>
            <tr>
                <td>SFT</td>
                <td>Тысячи пар «инструкция-ответ»</td>
                <td>Формат ассистента</td>
                <td>Модель, следующая инструкциям</td>
            </tr>
            <tr>
                <td>RLHF</td>
                <td>Человеческие рейтинги ответов</td>
                <td>Alignment с ценностями</td>
                <td>Безопасный, полезный ассистент</td>
            </tr>
        </table>
    </div>
    <div class="kmp12"><strong>Важно:</strong> Качество RLHF напрямую зависит от качества человеческой разметки. Это делает профессию лингвиста-аннотатора критически важной для развития AI.</div>
</section>

<section id="5" class="section">
    <h2 class="section-title">LLM в контексте AI</h2>
    
    <div class="hierarchy">
        <h3 class="hierarchy-title">5.1. Иерархия понятий</h3>
        <div class="hierarchy-card">
                        <ul>
                <li><strong>Artificial Intelligence (AI):</strong> Широкое поле — любые системы, имитирующие человеческий интеллект</li>
                <li><strong>→ Machine Learning (ML):</strong> AI, который учится на данных, а не программируется явно</li>
                <li><strong>→ → Deep Learning (DL):</strong> ML на основе глубоких нейронных сетей</li>
                <li><strong>→ → → LLM:</strong> DL-модели для работы с естественным языком</li>
            </ul>
        </div>
    </div>
    
    
    
    <div class="for-linguists">
        <h3 class="for-linguists-title">5.2. Значение для лингвистики</h3>
        <div class="for-linguists-card">
                        <ul>
                <li><strong>Новый инструмент исследования:</strong> LLM как «испытательный стенд» для лингвистических теорий</li>
                <li><strong>Профессиональные возможности:</strong> Аннотирование, prompt engineering, оценка качества</li>
                <li><strong>Критическая перспектива:</strong> Понимание ограничений и «слепых пятен» моделей</li>
                <li><strong>Этические вопросы:</strong> Bias в языковых данных, культурная репрезентация</li>
                <li><strong>Педагогика:</strong> Использование LLM в преподавании языков</li>
            </ul>
            <p><strong>Вопросы для размышления:</strong></p>
            			<ul>
                <li>Что успех LLM говорит нам о природе языка?</li>
                <li>Что успех LLM говорит нам о соотношении компетенции и перформанса?</li>
				<li>Что успех LLM говорит нам о границах между «знанием» и «вычислением»?</li>
            </ul>
			
        </div>
    </div>
    
    <div class="takeaways">
        <h3 class="takeaways-title">5.3. Ключевые выводы</h3>
        <div class="takeaways-card">
                        <ul>
                <li>LLM — это вероятностная модель языка, основанная на архитектуре Transformer</li>
                <li>Механизм Self-Attention позволяет модели учитывать контекст всего текста</li>
                <li>Эмерджентные способности появляются при достижении критического масштаба</li>
                <li>Обучение проходит в три этапа: Pre-training → SFT → RLHF</li>
                <li>Во время диалога веса модели не меняются — работает только контекстное окно</li>
                <li>Лингвисты могут и должны играть ключевые роли в создании перспективных LLM</li>
            </ul>
        </div>
    </div>
    <div class="kmp11"><strong>Примечание:</strong> Данный материал очень упрощенная учебная модель LLM Learning.</div>
</section>
	
	
<footer class="footer">
<div class="container">
<p>© 2026 | kmp | CC BY-NC-SA 4.0<br>
для всех</p>
</div>
</footer>
<div style="position: fixed; bottom: 10px; color: #777777; right: 30px; opacity: 0.3; font-size: 14px;">kmp+</div>
        <button class="back-to-top" id="backToTop" title="Наверх">↑</button>

    <script>
        // Функция для плавной прокрутки к разделу
        function scrollToSection(sectionId) {
            const section = document.getElementById(sectionId);
            let offsetTop = section.offsetTop - 20;
            
            // На мобильных учитываем высоту меню
            if (window.innerWidth <= 768) {
                offsetTop -= document.querySelector('.mobile-menu-container').offsetHeight;
            } else {
                offsetTop -= document.querySelector('.desktop-menu').offsetHeight;
            }
            
            window.scrollTo({
                top: offsetTop,
                behavior: 'smooth'
            });
            
            // Закрываем мобильное меню после клика
            closeMobileMenu();
        }

        // Переключение мобильного меню
        const hamburgerBtn = document.getElementById('hamburgerBtn');
        const mobileMenu = document.getElementById('mobileMenu');
        
        function toggleMobileMenu() {
            mobileMenu.classList.toggle('active');
            hamburgerBtn.textContent = mobileMenu.classList.contains('active') ? '✕' : '☰';
        }
        
        function closeMobileMenu() {
            mobileMenu.classList.remove('active');
            hamburgerBtn.textContent = '☰';
        }
        
        hamburgerBtn.addEventListener('click', toggleMobileMenu);
        
        // Закрытие меню при клике вне его
        document.addEventListener('click', function(event) {
            if (!mobileMenu.contains(event.target) && !hamburgerBtn.contains(event.target)) {
                closeMobileMenu();
            }
        });

        // Функция для переключения темы
        function toggleTheme() {
            const currentTheme = document.documentElement.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            
            document.documentElement.setAttribute('data-theme', newTheme);
            const themeIcon = newTheme === 'dark' ? '🌙' : '☀️';
            
            // Обновляем обе кнопки переключения темы
            document.getElementById('themeToggle').textContent = themeIcon;
            document.getElementById('desktopThemeToggle').textContent = themeIcon;
        }
        
        document.getElementById('themeToggle').addEventListener('click', toggleTheme);
        document.getElementById('desktopThemeToggle').addEventListener('click', toggleTheme);

        // Анимация появления секций при прокрутке
        document.addEventListener('DOMContentLoaded', function() {
            const sections = document.querySelectorAll('.section');
            
            const observerOptions = {
                root: null,
                rootMargin: '0px',
                threshold: 0.1
            };
            
            const observer = new IntersectionObserver(function(entries, observer) {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('fade-in');
                        observer.unobserve(entry.target);
                    }
                });
            }, observerOptions);
            
            sections.forEach(section => {
                section.classList.remove('fade-in');
                observer.observe(section);
            });
        });

        // Кнопка "Наверх"
        const backToTopBtn = document.getElementById('backToTop');
        
        window.addEventListener('scroll', function() {
            if (window.pageYOffset > 300) {
                backToTopBtn.classList.add('show');
            } else {
                backToTopBtn.classList.remove('show');
            }
        });
        
        backToTopBtn.addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Преобразование существующих таблиц в адаптивные
        function makeTablesResponsive() {
            const tables = document.querySelectorAll('.table-kmp table:not(.responsive-table)');
            
            tables.forEach(table => {
                // Создаем адаптивную версию
                const responsiveTable = document.createElement('table');
                responsiveTable.className = 'responsive-table';
                const tbody = document.createElement('tbody');
                
                // Получаем заголовки
                const headers = [];
                table.querySelectorAll('thead th').forEach(th => {
                    headers.push(th.textContent.trim());
                });
                
                // Преобразуем строки
                table.querySelectorAll('tbody tr').forEach(row => {
                    const newRow = document.createElement('tr');
                    const cells = row.querySelectorAll('td');
                    
                    cells.forEach((cell, index) => {
                        const newCell = document.createElement('td');
                        newCell.textContent = cell.textContent;
                        newCell.setAttribute('data-label', headers[index] || '');
                        newRow.appendChild(newCell);
                    });
                    
                    tbody.appendChild(newRow);
                });
                
                responsiveTable.appendChild(tbody);
                
                // Добавляем после оригинальной таблицы
                table.parentNode.insertBefore(responsiveTable, table.nextSibling);
            });
        }
        
        // Вызываем при загрузке
        document.addEventListener('DOMContentLoaded', makeTablesResponsive);
        
        // Закрытие меню при изменении размера окна
        window.addEventListener('resize', function() {
            if (window.innerWidth > 768) {
                closeMobileMenu();
            }
        });
    </script>
</body>
</html>