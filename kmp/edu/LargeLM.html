<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>kmp+</title>
    <style>
        /* Основные стили */
        :root {
            --primary-color: #325980;
			--primary1-color: #3498db;
			--primary2-color: #8c130d;
            --secondary-color: #4CAF50;
			--secondary1-color: #d9ebfc;
            --background-color: #f5f5f5;
			--content-bg: #fffff;
			--text-color: #333333;
            --header-text-color: #ffffff;
            --menu-bg: #ffffff;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            --border-radius: 8px;
            --warning-color: #e74c3c;
            --caution-color: #f39c12;
			--accent11: #4caf50;
			--accent12: #4cafff;
			--accent13: #ffaf50;
			--accent14: #821978;
			--card-bg: #fff;
			--card-shadow: rgba(0, 0, 0, 0.1);
			--link-bg: #3498db;
			--link-hover: #2980b9;
        }

        /* Темная тема */
        [data-theme="dark"] {
            --primary-color: #3e76ad;
			--primary1-color: #3498db;
			--primary2-color: #8c130d;
            --secondary-color: #388e3c;
			--secondary1-color: #093f73;
            --background-color: #121212;
            --content-bg: #1e1e1e;
			--text-color: #e0e0e0;
            --header-text-color: #ffffff;
            --menu-bg: #1e1e1e;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);
			--warning-color: #e74c3c;
            --caution-color: #f39c12;
			--card-bg: #2d2d2d;
			--card-shadow: rgba(0, 0, 0, 0.3);
			--link-bg: #2980b9;
			--link-hover: #3498db;
			
			
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            transition: background-color 0.3s, color 0.3s;
        }

        body {
            font-family: 'Roboto', 'Arial', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--background-color);
            padding-top: 60px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        header {
            background-color: var(--primary-color);
            color: var(--header-text-color);
            padding: 20px 0;
            text-align: center;
            border-radius: var(--border-radius);
            margin-bottom: 20px;
            background-image: linear-gradient(135deg, var(--primary-color) 0%, #2c3e50 100%);
        }

        h1 {
            font-size: 2.2rem;
            margin-bottom: 10px;
        }

        h2 {
            color: var(--primary-color);
            margin: 25px 0 15px;
            padding-bottom: 8px;
            border-bottom: 2px solid var(--secondary-color);
        }

        h3 {
            color: var(--primary-color);
            margin: 20px 0 10px;
        }

        p {
            margin-bottom: 15px;
        }

        /* Меню навигации */
        .menu {
            background-color: var(--menu-bg);
            padding: 15px 20px;
            border-radius: var(--border-radius);
            margin-bottom: 30px;
            box-shadow: var(--menu-shadow);
            display: flex;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .menu-buttons {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            justify-content: center;
        }

        .menu-btn {
            background-color: var(--primary-color);
            color: white;
            border: none;
            padding: 8px 15px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: background-color 0.3s;
        }

        .menu-btn:hover {
            background-color: var(--secondary-color);
        }

        .theme-toggle {
            background: none;
            border: 10px solid transparent;
            font-size: 1.5rem;
            cursor: pointer;
            color: var(--primary-color);
        }

        /* Секции контента */
        .section {
            background-color: var(--content-bg);
            border-radius: var(--border-radius);
            padding: 25px;
            margin-bottom: 30px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

       
    
		
		/* Таблицы */
		.table {
            background-color: var(--content-bg);
            border-radius: 2px;
            box-shadow: 0 2px 2px rgba(0, 0, 0, 0.1);
            padding: 20px;
            margin-bottom: 20px; width: 100%;
            color: var(--text-color);
			border-collapse: collapse;
			width: 100%;
            margin: 20px 0;
			box-sizing: border-box; /* Добавлено */
        }
        
        /* Заголовки таблицы с цветным фоном */
        .table thead th {
            background-color: #3498db;
            color: white;
            padding: 12px 15px;
            text-align: left;
            font-weight: 600;
			box-sizing: border-box; /* Добавлено */
        }
        
        /* Ячейки таблицы */
        .table tbody td {
            padding: 12px 15px;
            border-bottom: 1px solid #e0e0e0;
			box-sizing: border-box; /* Добавлено */
        }
        
        /* Чередование цветов строк для лучшей читаемости */
.table tbody tr:nth-child(even) {
    background-color: var(--content-bg);
}

/* Эффект при наведении на строку */
.table tbody tr:hover {
    background-color: var(--secondary1-color);
}
	
        
        /* Списки */
        ul, ol {
            padding-left: 50px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
			padding-left: 20px;
			        }

        

        /* Адаптивный дизайн */
        @media (max-width: 768px) {
            h1 {
                font-size: 1.8rem;
            }
            
            h2 {
                font-size: 1.5rem;
            }
            
            .menu {
                flex-direction: column;
                gap: 15px;
            }
            
            .menu-buttons {
                width: 100%;
                justify-content: center;
            }
            
            .theme-toggle {
                margin-top: 10px;
            }
            
            .section {
                padding: 15px;
            }
        }

        /* Анимации */
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .fade-in {
            animation: fadeIn 0.5s ease-in;
        }

        footer {
            text-align: center;
            padding: 20px 0;
            margin-top: 40px;
            font-size: 0.9rem;
            border-top: 1px solid #ddd;
        }

        /* Цитаты */
        blockquote {
            border-left: 4px solid var(--secondary-color);
            padding: 15px 20px;
            margin: 20px 0;
            background-color: rgba(76, 175, 80, 0.05);
            font-style: italic;
        }
		
		.btn {
            display: inline-block;
            background-color: var(--primary-color);
            color: white;
            padding: 8px 18px;
            border-radius: 10px;
            text-decoration: none;
            margin-top: 12px;
            transition: var(--transition);
        }
        
        .btn:hover {
            background-color: var(--secondary-color);
            text-decoration: none;
            transform: translateY(-2px);
        }
		
		.btn1 {
            display: inline-block;
            background-color: var(--primary1-color);
            color: white;
            padding: 8px 18px;
            border-radius: 10px;
            text-decoration: none;
            margin-top: 12px;
            transition: var(--transition);
        }
		
		.btn1:hover {
            background-color: var(--secondary-color);
            text-decoration: none;
            transform: translateY(-2px);
        }
		
		.btn2 {
            display: inline-block;
            background-color: var(--primary2-color);
            color: white;
            padding: 8px 18px;
            border-radius: 10px;
            text-decoration: none;
            margin-top: 12px;
            transition: var(--transition);
        }
		
		.btn2:hover {
            background-color: var(--secondary-color);
            text-decoration: none;
            transform: translateY(-2px);
        }
		
		.btn3 {
            display: inline-block;
            background-color: var(--secondary-color);
            color: white;
            padding: 8px 18px;
            border-radius: 10px;
            text-decoration: none;
            margin-top: 12px;
            transition: var(--transition);
        }
		
		.btn3:hover {
            background-color: var(--secondary-color);
            text-decoration: none;
            transform: translateY(-2px);
        }
		
		.tag {
    display: inline-block;
    background-color: var(--secondary-color);
    color: white;
    padding: 3px 8px;
    border-radius: 4px;
    font-size: 12px;
    margin-right: 5px;
    margin-bottom: 5px;
}

.tag2 {
    display: inline-block;
    background-color: #8c130d;
    color: white;
    padding: 3px 8px;
    border-radius: 4px;
    font-size: 12px;
    margin-right: 5px;
    margin-bottom: 5px;
}

        /* Кнопка "Наверх" */
        .back-to-top {
            position: fixed;
            bottom: 30px;
            right: 30px;
            background-color: var(--primary-color);
            color: white;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            display: flex;
            justify-content: center;
            align-items: center;
            cursor: pointer;
            box-shadow: 0 2px 10px rgba(0,0,0,0.2);
            opacity: 0;
            transition: opacity 0.3s;
            z-index: 99;
        }

        .back-to-top.visible {
            opacity: 1;
        }
		
	
		.link-kmp1 {
            color: #fffee0; 
            background-color: #007bff;  /
            padding: 0.2em 0.3em; 
            margin: 0 -0.3em; 
            text-decoration: none; 
			border-radius: 5px; /* Добавление скругленных углов */
             transition: background-color 0.2s ease-in-out, color 0.2s ease-in-out;
        }
		
		        .link-kmp1:hover,
        .link-kmp1:focus {
            color: #ffffff; 
            background-color: #0bb313; 
            text-decoration: none; 
        }
		
		.kmp11, .example {
      background: rgba(76, 175, 80, 0.1);
      border-left: 4px solid var(--accent11);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp12, .example {
      background: rgba(95, 182, 237, 0.1);
      border-left: 4px solid var(--accent12);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
 
		.kmp13, .example {
      background: rgba(205, 170, 110, 0.1);
      border-left: 4px solid var(--accent13);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp14, .example {
      background: rgba(205, 110, 200, 0.1);
      border-left: 4px solid var(--accent14);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

    .gallery {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 20px;
      max-width: 1200px;
      margin: 0 auto;
    }

    .card {
      background-color: var(--card-bg);
      border-radius: 8px;
      box-shadow: 0 4px 8px var(--card-shadow);
      padding: 20px;
      transition: transform 0.2s, box-shadow 0.2s;
    }

    .card:hover {
      transform: translateY(-5px);
      box-shadow: 0 6px 12px var(--card-shadow);
    }

    .card h3 {
      margin: 0 0 10px;
      color: var(--heading-color);
    }

    .card p {
      margin: 0 0 15px;
      font-size: 0.95em;
      line-height: 1.5;
      color: var(--paragraph-color);
    }

    .card a {
      display: block;
      width: 100%;
      padding: 8px;
      background-color: var(--link-bg);
      color: #fff;
      text-align: center;
      text-decoration: none;
      border-radius: 5px;
      font-size: 1em;
      transition: background-color 0.2s;
    }

    .card a:hover {
      background-color: var(--link-hover);
    }

    .theme-toggle {
      position: fixed;
      top: 20px;
      right: 20px;
      background: none;
      border: none;
      font-size: 1.5rem;
      cursor: pointer;
      color: var(--text-color);
      transition: transform 0.2s;
    }

    .theme-toggle:hover {
      transform: scale(1.1);
    }

    footer {
      text-align: center;
      padding: 2rem;
      color: var(--footer-color);
      font-size: 0.7rem;
    }

    @media (max-width: 600px) {
      .gallery {
        grid-template-columns: 1fr;
      }

      .card {
        padding: 15px;
      }
      
      .theme-toggle {
        top: 10px;
        right: 10px;
        font-size: 1.2rem;
      }
    }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ</h1>
            <p>в профессиональной деятельности преподавателя и лингвиста</p>
        </header>

        <nav class="menu">
            <div class="menu-buttons">
                <button class="menu-btn" onclick="scrollToSection('introduction')">Введение</button>
				<button class="menu-btn" onclick="scrollToSection('transformative-influence')">Трансформации</button>
                <button class="menu-btn" onclick="scrollToSection('interaction-distribution')">Системность</button>
				<button class="menu-btn" onclick="scrollToSection('context-environment')">Среда</button>
				<button class="menu-btn" onclick="scrollToSection('psychological-cognitive')">Ответственность</button>
				<button class="menu-btn" onclick="scrollToSection('conclusion')">Заключение</button>
				<button class="menu-btn" onclick="scrollToSection('dict')">Термины</button>
				<button class="menu-btn" onclick="scrollToSection('resources')">Литература</button>
			</div>
            <button class="theme-toggle" id="themeToggle" title="Переключить тему">☀️</button>
        </nav>

        <section id="big-characteristic" class="section">
            <h2 class="section-title">1. «Большие» как ключевая характеристика LLM</h2>
            <div class="001">
                <h3 class="001-title">Большие данные</h3>
                <div class="001-card">
                    <h4>Корпуса текстов беспрецедентного объёма</h4>
                    <p>Современные LLM обучаются на триллионах токенов (слов и частей слов), что составляет петабайты текстовой информации. Например, GPT-4 обучался на сотнях миллиардов слов из интернета, книг, научных статей и других источников.</p>
                    <p>Этот объём данных позволяет моделям улавливать тончайшие нюансы языка, редкие конструкции и специализированную терминологию практически во всех областях знаний.</p>
                    <p><strong>Основные источники данных:</strong></p>
                    <ul>
                        <li>веб-страницы и социальные сети</li>
                        <li>оцифрованные книги и научные публикации</li>
                        <li>кодовые базы программного обеспечения</li>
                        <li>специализированные корпуса текстов</li>
                    </ul>
                    <p><strong>Пояснение:</strong> Масштаб данных настолько велик, что для обработки традиционными методами потребовались бы тысячи лет человеческого чтения.</p>
                </div>
            </div>

            <div class="001">
                <h3 class="001-title">Большие нейросети</h3>
                <div class="001-card">
                    <h4>Архитектура с миллиардами параметров</h4>
                    <p>Современные LLM содержат от нескольких миллиардов до триллионов настраиваемых параметров (весов нейронных связей). GPT-4 оценивается в более чем триллион параметров, Claude 3 и другие современные модели — сотни миллиардов.</p>
                    <p>Эта масштабная архитектура позволяет моделям фиксировать сложные многоуровневые зависимости в языке и мыслить более абстрактно.</p>
                    <p><strong>Сравнение масштабов:</strong></p>
                    <ul>
                        <li>BERT (2018): 340 млн параметров</li>
                        <li>GPT-3 (2020): 175 млрд параметров</li>
                        <li>Claude/GPT-4 (2023+): от 500 млрд до >1 трлн параметров</li>
                    </ul>
                    <p><strong>Пояснение:</strong> Чем больше параметров, тем больше информации может хранить модель и тем сложнее взаимосвязи она способна улавливать.</p>
                </div>
            </div>

            <div class="001">
                <h3 class="001-title">Большое (глубокое) обучение</h3>
                <div class="001-card">
                    <h4>Масштабируемые алгоритмы самообучения</h4>
                    <p>LLM используют комплексные алгоритмы обучения, позволяющие эффективно настраивать миллиарды параметров. Процесс обучения включает предобучение и тонкую настройку (fine-tuning), а также обучение с подкреплением на основе обратной связи человека (RLHF).</p>
                    <p>Для эффективной работы с такими моделями требуются специализированные алгоритмы и оптимизации, особые подходы к распределенным вычислениям.</p>
                    <p><strong>Ключевые техники обучения:</strong></p>
                    <ul>
                        <li>самообучение с маскированием (masked self-supervised learning)</li>
                        <li>автоматическое кодирование (autoencoding)</li>
                        <li>предсказание следующего токена (next token prediction)</li>
                        <li>обучение с подкреплением на основе обратной связи человека (RLHF)</li>
                    </ul>
                    <p><strong>Пояснение:</strong> Обучение GPT-4 или подобной модели может занимать месяцы даже на тысячах специализированных GPU и стоить десятки миллионов долларов.</p>
                </div>
            </div>

            <div class="table-kmp">
                <table class="table">
                    <thead>
                        <tr>
                            <th>Характеристика</th>
                            <th>Маленькие модели (до 2018)</th>
                            <th>Большие модели (2020+)</th>
                        </tr>
                    </thead>
                    <tr>
                        <td>Объем данных</td>
                        <td>Гигабайты (миллионы примеров)</td>
                        <td>Петабайты (триллионы токенов)</td>
                    </tr>
                    <tr>
                        <td>Параметры</td>
                        <td>Миллионы</td>
                        <td>Миллиарды или триллионы</td>
                    </tr>
                    <tr>
                        <td>Вычислительные ресурсы</td>
                        <td>Один GPU, дни</td>
                        <td>Тысячи GPU, месяцы</td>
                    </tr>
                </table>
            </div>

            <div class="kmp12"><strong>Важно:</strong> Слово "Large" в LLM — не просто описательное прилагательное, а фундаментальная характеристика, определяющая саму природу таких моделей. Именно масштаб определяет их возможности.</div>
        </section>

        <section id="resource-costs" class="section">
            <h2 class="section-title">2. Ресурсные затраты</h2>
            <div class="001">
                <h3 class="001-title">Финансовые затраты</h3>
                <div class="001-card">
                    <h4>Стоимость обучения и эксплуатации</h4>
                    <p>Создание современной LLM требует колоссальных финансовых вложений. Стоимость обучения крупнейших моделей исчисляется десятками миллионов долларов, что делает их разработку доступной лишь крупным корпорациям.</p>
                    <p>Помимо затрат на обучение, значительные средства уходят на инфраструктуру, поддержку и масштабирование для обслуживания миллионов запросов.</p>
                    <p><strong>Основные статьи расходов:</strong></p>
                    <ul>
                        <li>приобретение специализированного оборудования (GPU/TPU)</li>
                        <li>электроэнергия и охлаждение</li>
                        <li>создание и поддержание инфраструктуры</li>
                        <li>оплата труда высококвалифицированных специалистов</li>
                    </ul>
                    <p><strong>Пояснение:</strong> По оценкам, обучение модели GPT-4 могло стоить более 100 млн долларов, не считая затрат на разработку и исследования.</p>
                </div>
            </div>

            <div class="001">
                <h3 class="001-title">Энергетические затраты</h3>
                <div class="001-card">
                    <h4>Высокое потребление электроэнергии</h4>
                    <p>Обучение и эксплуатация LLM требуют огромных энергетических ресурсов. Датацентры, где проходит обучение, потребляют мегаватты электроэнергии, сравнимые с энергопотреблением небольших городов.</p>
                    <p>Даже после обучения, инференция (использование) LLM требует значительных энергетических затрат для обработки каждого запроса пользователя.</p>
                    <p><strong>Масштаб энергопотребления:</strong></p>
                    <ul>
                        <li>обучение крупной модели — сотни или тысячи мегаватт-часов</li>
                        <li>один диалог с GPT-4 — до нескольких ватт-часов</li>
                        <li>миллионы пользователей — гигаватты энергии ежедневно</li>
                    </ul>
                    <p><strong>Пояснение:</strong> Энергопотребление одного запроса к LLM может в сотни раз превышать потребление обычного поискового запроса.</p>
                </div>
            </div>

            <div class="001">
                <h3 class="001-title">Интеллектуальные затраты</h3>
                <div class="001-card">
                    <h4>Требуется экспертиза для разработки и настройки</h4>
                    <p>Создание LLM требует привлечения высококвалифицированных специалистов из различных областей: машинного обучения, лингвистики, компьютерных наук, инженерии и других дисциплин.</p>
                    <p>Помимо технической экспертизы, необходимы специалисты по этике, безопасности, социальным наукам для минимизации рисков и предвзятости моделей.</p>
                    <p><strong>Ключевые специалисты:</strong></p>
                    <ul>
                        <li>исследователи в области машинного обучения</li>
                        <li>инженеры по распределенным системам</li>
                        <li>лингвисты и специалисты по NLP</li>
                        <li>эксперты по этике ИИ и безопасности</li>
                    </ul>
                    <p><strong>Пояснение:</strong> Ведущие компании в сфере ИИ конкурируют за ограниченный пул специалистов, предлагая зарплаты на уровне сотен тысяч долларов в год.</p>
                </div>
            </div>

            <div class="001">
                <h3 class="001-title">Экологические затраты</h3>
                <div class="001-card">
                    <h4>Углеродный след и водопотребление</h4>
                    <p>Высокое энергопотребление LLM напрямую связано с экологическими проблемами. Углеродный след от обучения и эксплуатации моделей зависит от источников энергии, используемых датацентрами.</p>
                    <p>Помимо CO₂, датацентры потребляют огромные объемы воды для охлаждения серверов, что особенно проблематично в регионах с ограниченными водными ресурсами.</p>
                    <p><strong>Экологические последствия:</strong></p>
                    <ul>
                        <li>выбросы парниковых газов (при использовании ископаемого топлива)</li>
                        <li>потребление миллионов литров воды для охлаждения</li>
                        <li>создание электронных отходов при обновлении оборудования</li>
                    </ul>
                    <p><strong>Пояснение:</strong> Углеродный след от обучения одной крупной LLM может быть эквивалентен сотням трансатлантических перелетов.</p>
                </div>
            </div>

            <div class="table-kmp">
                <table class="table">
                    <thead>
                        <tr>
                            <th>Тип затрат</th>
                            <th>Масштаб</th>
                            <th>Последствия</th>
                        </tr>
                    </thead>
                    <tr>
                        <td>Финансовые</td>
                        <td>10-100+ млн $ на обучение</td>
                        <td>Концентрация технологии у крупных корпораций</td>
                    </tr>
                    <tr>
                        <td>Энергетические</td>
                        <td>Сотни МВт·ч на обучение</td>
                        <td>Нагрузка на энергосистемы, рост потребления</td>
                    </tr>
                    <tr>
                        <td>Интеллектуальные</td>
                        <td>Сотни высококлассных специалистов</td>
                        <td>"Утечка мозгов" из других областей науки</td>
                    </tr>
                    <tr>
                        <td>Экологические</td>
                        <td>Тонны CO₂, миллионы литров воды</td>
                        <td>Вклад в изменение климата, водный стресс</td>
                    </tr>
                </table>
            </div>

            <div class="kmp13"><strong>Внимание:</strong> Разработка и эксплуатация LLM сопряжены с существенными ресурсными затратами, которые нередко остаются "за кадром" при обсуждении этих технологий, но имеют важное значение для долгосрочной устойчивости.</div>
        </section>

        <section id="social-ethical" class="section">
            <h2 class="section-title">3. Социальные и этические аспекты</h2>
            <div class="001">
                <h3 class="001-title">Большие надежды</h3>
                <div class="001-card">
                    <h4>Потенциал для науки, образования, бизнеса</h4>
                    <p>LLM открывают беспрецедентные возможности для множества сфер человеческой деятельности. Они способны демократизировать доступ к знаниям, автоматизировать рутинные задачи и стать катализатором инноваций.</p>
                    <p>В научной среде LLM уже помогают анализировать литературу, генерировать гипотезы и ускорять исследования в различных дисциплинах от биологии до астрофизики.</p>
                    <p><strong>Ключевые возможности:</strong></p>
                    <ul>
                        <li>персонализированное образование и поддержка обучения</li>
                        <li>ускорение научных исследований и анализа данных</li>
                        <li>повышение доступности информации для людей с ограниченными возможностями</li>
                        <li>автоматизация рутинного документооборота и коммуникаций</li>
                    </ul>
                    <p><strong>Пояснение:</strong> LLM могут стать "мультипликаторами продуктивности", позволяя специалистам сосредоточиться на творческих и стратегических аспектах работы.</p>
                </div>
            </div>

            <div class="001">
                <h3 class="001-title">Большие риски</h3>
                <div class="001-card">
                    <h4>Дезинформация, предвзятость, киберугрозы</h4>
                    <p>С увеличением возможностей LLM растут и сопутствующие риски. Одна из главных проблем — способность этих моделей генерировать убедительный, но ложный контент, который трудно отличить от достоверного.</p>
                    <p>Модели могут наследовать и усиливать предвзятости, содержащиеся в обучающих данных, а также использоваться злоумышленниками для создания вредоносного кода или организации кибератак.</p>
                    <p><strong>Основные угрозы:</strong></p>
                    <ul>
                        <li>массовая генерация дезинформации и фейковых новостей</li>
                        <li>усиление социальных предрассудков и дискриминации</li>
                        <li>автоматизация создания вредоносного ПО и фишинговых материалов</li>
                        <li>манипуляция общественным мнением через убедительный контент</li>
                    </ul>
                    <p><strong>Пояснение:</strong> Масштаб потенциального вреда от злонамеренного использования LLM может значительно превышать возможности предыдущих технологий из-за их автономности и убедительности.</p>
                </div>
            </div>

            <div class="001">
                <h3 class="001-title">Большие опасения</h3>
                <div class="001-card">
                    <h4>Потеря рабочих мест, влияние на креативные профессии</h4>
                    <p>Автоматизация интеллектуального труда с помощью LLM вызывает обоснованные опасения о будущем многих профессий. Модели уже способны выполнять задачи, ранее требовавшие высокой квалификации: написание текстов, программирование, перевод.</p>
                    <p>Особую тревогу вызывает проникновение LLM в креативные профессии: копирайтинг, дизайн, журналистику, что ставит вопросы о будущем творческих индустрий.</p>
                    <p><strong>Потенциально уязвимые профессии:</strong></p>
                    <ul>
                        <li>копирайтеры, редакторы, журналисты</li>
                        <li>переводчики и специалисты по локализации</li>
                        <li>юристы (документооборот, типовые консультации)</li>
                        <li>программисты начального и среднего уровня</li>
                    </ul>
                    <p><strong>Пояснение:</strong> Хотя LLM вряд ли полностью заменят людей в этих профессиях, они могут существенно изменить характер работы и создать давление на рынок труда.</p>
                </div>
            </div>

            <div class="001">
                <h3 class="001-title">Большие спекуляции</h3>
                <div class="001-card">
                    <h4>Гиперболизированные ожидания в медиа</h4>
                    <p>Вокруг LLM сформировался значительный информационный шум, включающий как чрезмерно оптимистичные, так и катастрофические сценарии. СМИ и социальные сети склонны к драматизации возможностей и рисков этих технологий.</p>
                    <p>Непонимание реальных возможностей и ограничений LLM приводит к формированию нереалистичных ожиданий и необоснованных страхов в обществе.</p>
                    <p><strong>Распространенные спекуляции:</strong></p>
                    <ul>
                        <li>утверждения о "сознательности" или "разумности" LLM</li>
                        <li>прогнозы о полной замене человеческого труда в ближайшие годы</li>
                        <li>сценарии неконтролируемого "восстания ИИ"</li>
                        <li>обещания решения всех глобальных проблем с помощью LLM</li>
                    </ul>
                    <p><strong>Пояснение:</strong> Медийный дискурс вокруг LLM часто формируется людьми, не имеющими глубокого понимания принципов работы этих технологий.</p>
                </div>
            </div>

            <div class="001">
                <h3 class="001-title">Большие манипуляции</h3>
                <div class="001-card">
                    <h4>Использование для пропаганды, мошенничества</h4>
                    <p>Способность LLM генерировать убедительный, персонализированный контент делает их мощным инструментом для целенаправленных манипуляций. Модели могут использоваться для создания таргетированной пропаганды или сложных мошеннических схем.</p>
                    <p>Особую опасность представляет сочетание LLM с другими технологиями, такими как дипфейки или автоматизированное распространение контента.</p>
                    <p><strong>Потенциальные сценарии манипуляций:</strong></p>
                    <ul>
                        <li>массовая генерация персонализированной политической пропаганды</li>
                        <li>создание убедительных мошеннических писем и фишинговых материалов</li>
                        <li>имитация реальных людей в онлайн-коммуникации</li>
                        <li>манипулирование финансовыми рынками через генерацию фальшивых новостей</li>
                    </ul>
                    <p><strong>Пояснение:</strong> Эффективность манипуляций с использованием LLM многократно возрастает благодаря их способности адаптировать сообщения под конкретную аудиторию.</p>
                </div>
            </div>

            <div class="table-kmp">
                <table class="table">
                    <thead>
                        <tr>
                            <th>Аспект</th>
                            <th>Позитивный потенциал</th>
                            <th>Негативный потенциал</th>
                        </tr>
                    </thead>
                    <tr>
                        <td>Информационный</td>
                        <td>Демократизация доступа к знаниям</td>
                        <td>Массовая дезинформация</td>
                    </tr>
                    <tr>
                        <td>Экономический</td>
                        <td>Повышение продуктивности</td>
                        <td>Дестабилизация рынка труда</td>
                    </tr>
                    <tr>
                        <td>Социальный</td>
                        <td>Преодоление языковых барьеров</td>
                        <td>Усиление социального неравенства</td>
                    </tr>
                    <tr>
                        <td>Политический</td>
                        <td>Расширение участия в демократических процессах</td>
                        <td>Манипуляция общественным мнением</td>
                    </tr>
                </table>
            </div>

            <div class="kmp11"><strong>Примечание:</strong> Социальные и этические аспекты LLM часто являются предметом междисциплинарных исследований, объединяющих специалистов в области компьютерных наук, философии, социологии, права и других дисциплин.</div>
        </section>

        <section id="emergent-properties" class="section">
            <h2 class="section-title">4. Главное: Большие - обретшие эмерджентные свойства</h2>
            <div class="001">
                <h3 class="001-title">Масштабирование → новые качества</h3>
                <div class="001-card">
                    <h4>Эмерджентность как ключевое свойство LLM</h4>
                    <p>Эмерджентность — возникновение у системы свойств, которые не наблюдаются у её отдельных компонентов. В контексте LLM это означает появление способностей, которые не были явно заложены в алгоритм и не наблюдались в моделях меньшего масштаба.</p>
                    <p>Эмерджентные свойства LLM появляются не постепенно, а скачкообразно при достижении определенного масштаба, что делает их особенно значимыми для понимания природы этих моделей.</p>
                    <p><strong>Ключевые эмерджентные свойства:</strong></p>
                    <ul>
                        <li>способность к абстрактному мышлению и обобщению</li>
                        <li>контекстуальное понимание языка на уровне, близком к человеческому</li>
                        <li>решение задач, не включенных в обучающие данные</li>
                        <li>адаптация к новым доменам и типам задач без дополнительного обучения</li>
                    </ul>
                    <p><strong>Пояснение:</strong> Именно эмерджентные свойства отличают современные LLM от предыдущих поколений языковых моделей и делают их революционными.</p>
                </div>
            </div>

            <div class="001">
                <h3 class="001-title">Уникальное выявление связей и паттернов</h3>
                <div class="001-card">
                    <h4>Культурные и лингвистические корреляции</h4>
                    <p>LLM способны выявлять неочевидные для человека связи между концепциями, идеями и культурными феноменами, анализируя огромные объемы текстов из разных источников и эпох.</p>
                    <p>Модели могут выявлять сложные лингвистические паттерны и культурные контексты, даже если они не были явно размечены или объяснены в обучающих данных.</p>
                    <p><strong>Примеры выявляемых связей:</strong></p>
                    <ul>
                        <li>исторические параллели между разными эпохами и культурами</li>
                        <li>скрытые лингвистические влияния между языками</li>
                        <li>концептуальные связи между научными дисциплинами</li>
                        <li>эволюция значений слов и выражений во времени</li>
                    </ul>
                    <p><strong>Пояснение:</strong> LLM действуют как своеобразные "когнитивные археологи", извлекающие скрытые структуры из массивов текстовых данных.</p>
                </div>
            </div>

            <div class="001">
                <h3 class="001-title">Способность к обобщению</h3>
                <div class="001-card">
                    <h4>Решение задач без явного обучения</h4>
                    <p>Одно из самых удивительных свойств LLM — способность решать задачи, которым они не были явно обучены. Это проявляется в феномене "few-shot learning" (обучение по нескольким примерам) и даже "zero-shot learning" (решение задачи без предварительных примеров).</p>
                    <p>Эта способность к обобщению позволяет моделям переносить знания из одного контекста в другой и применять их к новым ситуациям, что приближает их к человеческому когнитивному процессу.</p>
                    <p><strong>Проявления способности к обобщению:</strong></p>
                    <ul>
                        <li>перевод между языками, которым модель не обучалась напрямую</li>
                        <li>решение логических задач новых типов</li>
                        <li>применение знаний из одной области к проблемам в другой</li>
                        <li>понимание и генерация текста в необычных стилях или форматах</li>
                    </ul>
                    <p><strong>Пояснение:</strong> Способность к обобщению делает LLM гораздо более гибкими инструментами, чем традиционные алгоритмы, заточенные под конкретные задачи.</p>
                </div>
            </div>

            <div class="001">
                <h3 class="001-title">Появление рассуждений</h3>
                <div class="001-card">
                    <h4>Chain-of-thought, логические выводы</h4>
                    <p>С увеличением масштаба LLM приобретают способность к последовательным рассуждениям (chain-of-thought reasoning) — построению логических цепочек выводов для решения сложных задач.</p>
                    <p>Модели демонстрируют элементы критического мышления: анализируют предпосылки, выявляют противоречия, строят аргументацию и приходят к обоснованным выводам.</p>
                    <p><strong>Типы рассуждений, доступные LLM:</strong></p>
                    <ul>
                        <li>дедуктивные (от общего к частному)</li>
                        <li>индуктивные (от частного к общему)</li>
                        <li>абдуктивные (поиск наиболее вероятного объяснения)</li>
                        <li>аналогические (перенос знаний по аналогии)</li>
                    </ul>
                    <p><strong>Пояснение:</strong> Хотя процесс рассуждения LLM внешне напоминает человеческий, внутренние механизмы существенно отличаются, что приводит к характерным ошибкам и ограничениям.</p>
                </div>
            </div>

            <div class="001">
                <h3 class="001-title">Стохастическая оптимизация</h3>
                <div class="001-card">
                    <h4>Синтез закономерностей и творческая непредсказуемость</h4>
                    <p>LLM используют стохастические (вероятностные) процессы для генерации текста, что обеспечивает баланс между предсказуемостью и креативностью. Это позволяет моделям создавать тексты, которые следуют языковым и логическим закономерностям, но не являются полностью детерминированными.</p>
                    <p>Этот элемент случайности, контролируемый параметром "температуры" генерации, даёт LLM возможность находить неожиданные решения и создавать творческий контент.</p>
                    <p><strong>Проявления стохастической природы:</strong></p>
                    <ul>
                        <li>творческое письмо (поэзия, художественная проза)</li>
                        <li>генерация нестандартных идей и подходов</li>
                        <li>варьирование стилей и регистров языка</li>
                        <li>адаптация к неоднозначным контекстам</li>
                    </ul>
                    <p><strong>Пояснение:</strong> Стохастическая природа LLM делает их более гибкими и адаптивными, но также вносит элемент непредсказуемости в их работу.</p>
                </div>
            </div>

            <div class="001">
                <h3 class="001-title">Мультимодальность</h3>
                <div class="001-card">
                    <h4>Работа с любым семиотическим текстом</h4>
                    <p>Современные LLM эволюционируют в направлении мультимодальности — способности работать с различными типами данных: текстом, изображениями, звуком, видео. Это позволяет им интерпретировать и генерировать контент, сочетающий разные семиотические системы.</p>
                    <p>Мультимодальные модели могут "переводить" информацию между различными модальностями, например, описывать изображения текстом или создавать визуализацию на основе словесного описания.</p>
                    <p><strong>Поддерживаемые типы контента:</strong></p>
                    <ul>
                        <li>естественный язык (текст) различных стилей и жанров</li>
                        <li>изображения и визуальные репрезентации</li>
                        <li>программный код различных языков программирования</li>
                        <li>структурированные данные (таблицы, графики, диаграммы)</li>
                    </ul>
                    <p><strong>Пояснение:</strong> Мультимодальность позволяет LLM взаимодействовать с миром более комплексно, приближаясь к человеческому восприятию информации.</p>
                </div>
            </div>

            <div class="table-kmp">
                <table class="table">
                    <thead>
                        <tr>
                            <th>Эмерджентное свойство</th>
                            <th>Порог появления (приблизительно)</th>
                            <th>Значение для лингвистики</th>
                        </tr>
                    </thead>
                    <tr>
                        <td>In-context learning</td>
                        <td>~10 млрд параметров</td>
                        <td>Моделирование быстрой языковой адаптации</td>
                    </tr>
                    <tr>
                        <td>Chain-of-thought reasoning</td>
                        <td>~100 млрд параметров</td>
                        <td>Исследование структуры рассуждений</td>
                    </tr>
                    <tr>
                        <td>Мультилингвальность</td>
                        <td>~50 млрд параметров</td>
                        <td>Выявление универсальных языковых структур</td>
                    </tr>
                    <tr>
                        <td>Мультимодальность</td>
                        <td>~300 млрд параметров</td>
                        <td>Изучение связей между языком и другими семиотическими системами</td>
                    </tr>
                </table>
            </div>

            <div class="kmp12"><strong>Важно:</strong> Эмерджентные свойства LLM напоминают нам, что увеличение количественных характеристик системы (размер модели, объем данных) может приводить к качественным изменениям её поведения — принцип, известный в философии как "переход количества в качество".</div>
            
            <div class="kmp14"><strong>Пояснение:</strong> Термин "Large" в названии LLM подчеркивает не просто физический размер моделей, но и их принципиальное отличие от предыдущих поколений — способность проявлять эмерджентные свойства, которые делают их качественно новым типом искусственного интеллекта.</div>
        </section>
		
		
		
		
<section id="language-models-scale" class="section">
    <h2 class="section-title">1. Классификация языковых моделей по масштабу</h2>
    
    <div class="scale-intro">
        <h3 class="scale-intro-title">Почему появились вариации размеров?</h3>
        <div class="scale-intro-card">
            <h4>Эволюция понятия "большие"</h4>
            <p>Терминология вокруг размеров языковых моделей стала запутанной из-за быстрого прогресса в области ИИ и маркетинговых стратегий. Термины вроде XLLM или SLLM (Small Large Language Model — оксюморон!) привлекают внимание, но часто не имеют четких критериев.</p>
            <p>То, что считалось "большой" моделью 5 лет назад, сегодня может относиться к категории "малых" моделей. Ранние «LLM» (например, GPT-2 с 1.5B параметров) сегодня считаются Small Language Models (SLM), но тогда их называли «большими» — это вопрос относительности. Уменьшенные модели (Tiny, Small) создаются для конкретных задач (медицина, юриспруденция) или устройств (смартфоны, IoT).</p>
            
            <p><strong>Ключевые причины разнообразия:</strong></p>
            <ul>
                <li>Исторический контекст - быстрый прогресс в области ИИ</li>
                <li>Специализация моделей под конкретные задачи</li>
                <li>Маркетинговые стратегии компаний</li>
            </ul>
            <p><strong>Пояснение:</strong> Категории моделей постоянно пересматриваются по мере развития технологий.</p>
        </div>
    </div>

    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Категория</th>
                    <th>Параметры</th>
                    <th>Примеры</th>
                    <th>Эмерджентность</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Tiny/TLM</td>
                    <td>&lt;1B</td>
                    <td>TinyBERT, DistilGPT-2</td>
                    <td>❌ Нет (узкие задачи)</td>
                </tr>
                <tr>
                    <td>Small/SLM</td>
                    <td>1B-10B</td>
                    <td>Phi-2, Gemini Nano</td>
                    <td>⚠️ Ограниченная</td>
                </tr>
                <tr>
                    <td>Medium/MLM</td>
                    <td>10B-100B</td>
                    <td>LLaMA-2 (13B), GPT-3.5</td>
                    <td>✅ Да (базовая)</td>
                </tr>
                <tr>
                    <td>Large/LLM</td>
                    <td>100B+</td>
                    <td>GPT-4, Claude 3</td>
                    <td>✅ Сильная</td>
                </tr>
                <tr>
                    <td>XLLM/XXL</td>
                    <td>&gt;1T (?)</td>
                    <td>GPT-5 (ожидается)</td>
                    <td>✅ Гипотетически новая</td>
                </tr>
            </tbody>
        </table>
    </div>

    <div class="kmp11">
        <strong>Примечание:</strong> Границы между категориями условны и постоянно пересматриваются.
    </div>
    
    <div class="kmp12">
        <strong>Важно:</strong> Размер модели не всегда коррелирует с качеством - важна оптимизация архитектуры и данных.
    </div>
    
    <div class="kmp13">
        <strong>Внимание:</strong> Термин SLLM (Small Large Language Model) является маркетинговым оксюмороном.
    </div>
    
    <div class="kmp14">
        <strong>Пояснение:</strong> Эмерджентные свойства могут появляться и у относительно небольших моделей (7B+), если они правильно обучены.
    </div>
</section>
		
	<footer class="footer">
<div class="container">
<p>© 2025 | Искусственный интеллект в профессиональной деятельности<br>
Разработано для студентов БрГУ имени А.С. Пушкина</p>
</div>
</footer>
<div style="position: fixed; bottom: 10px; right: 33px; opacity: 0.3; font-size: 14px;">kmp+</div>

        <div class="back-to-top" id="backToTop" onclick="window.scrollTo({top: 0, behavior: 'smooth'})">↑</div>
    </div>
	
        <script>
        // Функция для плавной прокрутки к разделу
        function scrollToSection(sectionId) {
            const section = document.getElementById(sectionId);
            const menuHeight = document.querySelector('.menu').offsetHeight;
            
            window.scrollTo({
                top: section.offsetTop - menuHeight - 20,
                behavior: 'smooth'
            });
        }

        // Функция для переключения темы
        document.getElementById('themeToggle').addEventListener('click', function() {
            const currentTheme = document.documentElement.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            
            document.documentElement.setAttribute('data-theme', newTheme);
            this.textContent = newTheme === 'dark' ? '🌙' : '☀️';
        });

        // Анимация появления секций при прокрутке
        document.addEventListener('DOMContentLoaded', function() {
            const sections = document.querySelectorAll('.section');
            
            const observerOptions = {
                root: null,
                rootMargin: '0px',
                threshold: 0.1
            };
            
            const observer = new IntersectionObserver(function(entries, observer) {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('fade-in');
                        observer.unobserve(entry.target);
                    }
                });
            }, observerOptions);
            
            sections.forEach(section => {
                section.classList.remove('fade-in');
                observer.observe(section);
            });
        });
		
		
		
    </script>
	
</body>
</html>