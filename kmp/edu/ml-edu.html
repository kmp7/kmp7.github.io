<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>kmp+</title>
    <style>
        /* Основные стили */
        :root {
            --primary-color: #325980;
            --secondary-color: #4CAF50;
            --background-color: #f5f5f5;
            --content-bg: #ffffff;
            --text-color: #333333;
            --header-text-color: #ffffff;
            --menu-bg: #ffffff;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            --border-radius: 8px;
			  --accent11: #4caf50;
		--accent12: #4cafff;
		--accent13: #ffaf50;
		--accent14: #821978;
        }

        /* Темная тема */
        [data-theme="dark"] {
            --primary-color: #3e76ad;
            --secondary-color: #388e3c;
            --background-color: #000000;
            --content-bg: #1e1e1e;
            --text-color: #e0e0e0;
            --header-text-color: #ffffff;
            --menu-bg: #000000;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            transition: background-color 0.3s, color 0.3s;
        }

        body {
            font-family: 'Roboto', 'Arial', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--background-color);
            padding-top: 2px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        header {
            background-color: var(--primary-color);
            color: var(--header-text-color);
            padding: 20px 0;
            text-align: center;
            border-radius: var(--border-radius);
            margin-bottom: 2px;
        }

        h1 {
            font-size: 2.2rem;
            margin-bottom: 10px;
        }

        h2 {
            color: var(--primary-color);
            margin: 25px 0 15px;
            padding-bottom: 8px;
            border-bottom: 2px solid var(--secondary-color);
        }

        h3 {
            color: var(--primary-color);
            margin: 20px 0 10px;
        }

        p {
            margin-bottom: 15px;
        }

        /* Меню навигации */
        .menu {
            background-color: var(--menu-bg);
            padding: 15px 20px;
            border-radius: var(--border-radius);
            margin-bottom: 30px;
            box-shadow: var(--menu-shadow);
            display: flex;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .menu-buttons {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }
		
		
		.menu-buttons {
    display: flex;
    flex-wrap: wrap;
    gap: 10px;
    justify-content: center; /* Изменено на center */
}

        .menu-btn {
            background-color: var(--primary-color);
            color: white;
            border: none;
            padding: 8px 15px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: background-color 0.3s;
        }

        .menu-btn:hover {
            background-color: var(--secondary-color);
        }

        .theme-toggle {
            background: none;
            border: 10px solid transparent;
            font-size: 1.5rem;
            cursor: pointer;
            color: var(--primary-color);
        }

        /* Секции контента */
        .section {
            background-color: var(--content-bg);
            border-radius: var(--border-radius);
            padding: 25px;
            margin-bottom: 30px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        /* Выделение важного */
        .important {
            background-color: rgba(76, 175, 80, 0.1);
            border-left: 4px solid var(--secondary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        .note {
            background-color: rgba(50, 89, 128, 0.1);
            border-left: 4px solid var(--primary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        /* Таблицы */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            padding: 12px 15px;
            border: 1px solid #ddd;
            text-align: left;
        }

        th {
            background-color: var(--primary-color);
            color: white;
        }

        tr:nth-child(even) {
            background-color: rgba(0, 0, 0, 0.03);
        }

        /* Списки */
        ul, ol {
            padding-left: 25px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
        }

        /* Адаптивный дизайн */
        @media (max-width: 768px) {
            h1 {
                font-size: 1.8rem;
            }
            
            h2 {
                font-size: 1.5rem;
            }
            
            .menu {
                flex-direction: column;
                gap: 15px;
            }
            
            .menu-buttons {
                width: 100%;
                justify-content: center;
            }
            
            .theme-toggle {
                margin-top: 10px;
            }
            
            .section {
                padding: 15px;
            }
        }

        /* Анимации */
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .fade-in {
            animation: fadeIn 0.5s ease-in;
        }

        footer {
            text-align: center;
            padding: 20px 0;
            margin-top: 40px;
            font-size: 0.9rem;
        }
		
					.kmp11, .example {
      background: rgba(76, 175, 80, 0.1);
      border-left: 4px solid var(--accent11);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp12, .example {
      background: rgba(95, 182, 237, 0.1);
      border-left: 4px solid var(--accent12);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
 
		.kmp13, .example {
      background: rgba(205, 170, 110, 0.1);
      border-left: 4px solid var(--accent13);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp14, .example {
      background: rgba(205, 110, 200, 0.1);
      border-left: 4px solid var(--accent14);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>DL LLM</h1>
            <p>Глубокое обучение больших языковых моделей</p>
        </header>

        <nav class="menu">
            <div class="menu-buttons">
                <button class="menu-btn" onclick="scrollToSection('section1')">ML</button>
                <button class="menu-btn" onclick="scrollToSection('section2')">Типы ML</button>
                <button class="menu-btn" onclick="scrollToSection('section3')">DL</button>
                <button class="menu-btn" onclick="scrollToSection('section4')">DL LLM</button>
                <button class="menu-btn" onclick="scrollToSection('section5')">Pretraining</button>
				<button class="menu-btn" onclick="scrollToSection('section6')">Transfer Learning</button>
				<button class="menu-btn" onclick="scrollToSection('section7')">Fine-Tuning</button>
				<button class="menu-btn" onclick="scrollToSection('section8')">Заключение</button>
                    </div>
            <button class="theme-toggle" id="themeToggle" title="Переключить тему">☀️</button>
        </nav>

    
<section id="section1" class="section">
    <h2 class="section-title">1. Понятие машинного обучения (ML)</h2>
    <div class="001">
        <h3 class="001-title">Основы машинного обучения</h3>
        <div class="001-card">
            <h4>Что такое машинное обучение</h4>
            <p>Машинное обучение (Machine Learning, ML) — это подраздел искусственного интеллекта, который фокусируется на разработке алгоритмов и моделей, позволяющих компьютерам обучаться на основе данных без явного программирования.</p>
            <p>В отличие от традиционного программирования, где разработчик пишет конкретные инструкции для решения задачи, в машинном обучении система самостоятельно выявляет закономерности в данных и строит модели для принятия решений.</p>
            <p><strong>Ключевые компоненты машинного обучения:</strong></p>
            <ul>
                <li>Данные — основа для обучения моделей</li>
                <li>Алгоритмы — методы обработки данных и построения моделей</li>
                <li>Модели — математические представления изучаемых явлений</li>
                <li>Метрики — способы оценки качества работы моделей</li>
            </ul>
            <p><strong>Пояснение:</strong> Машинное обучение позволяет решать задачи, которые сложно формализовать традиционными алгоритмическими методами, например, распознавание речи, компьютерное зрение, перевод текстов.</p>
        </div>
    </div>
    <div class="002">
        <h3 class="002-title">Процесс машинного обучения</h3>
        <div class="002-card">
            <h4>Этапы машинного обучения</h4>
            <p>Процесс машинного обучения включает несколько последовательных этапов, которые необходимы для создания эффективной модели.</p>
            <p><strong>Основные этапы:</strong></p>
            <ul>
                <li>Сбор и подготовка данных</li>
                <li>Выбор и создание признаков (feature engineering)</li>
                <li>Выбор алгоритма обучения</li>
                <li>Обучение модели</li>
                <li>Оценка качества модели</li>
                <li>Настройка гиперпараметров</li>
                <li>Развертывание модели</li>
            </ul>
        </div>
    </div>
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Термин</th>
                    <th>Определение</th>
                </tr>
            </thead>
            <tr>
                <td>Обучающая выборка</td>
                <td>Набор данных, используемый для обучения модели</td>
            </tr>
            <tr>
                <td>Тестовая выборка</td>
                <td>Набор данных для оценки качества обученной модели</td>
            </tr>
            <tr>
                <td>Валидационная выборка</td>
                <td>Набор данных для настройки гиперпараметров модели</td>
            </tr>
            <tr>
                <td>Переобучение</td>
                <td>Ситуация, когда модель слишком хорошо запоминает обучающие данные, но плохо обобщает новые</td>
            </tr>
        </table>
    </div>
    <div class="kmp14"><strong>Важно:</strong> Качество и количество данных являются критическими факторами успеха в машинном обучении. Даже самые сложные алгоритмы не смогут компенсировать недостатки в данных.</div>
</section>

<section id="section2" class="section">
    <h2 class="section-title">2. История, типы и виды ML</h2>
    <div class="001">
        <h3 class="001-title">История машинного обучения</h3>
        <div class="001-card">
            <h4>Ключевые этапы развития</h4>
            <p>История машинного обучения насчитывает несколько десятилетий и тесно связана с развитием искусственного интеллекта и компьютерных наук.</p>
            <p><strong>Основные вехи:</strong></p>
            <ul>
                <li>1943 — Создание первой математической модели нейрона (Маккаллок и Питтс)</li>
                <li>1957 — Перцептрон Розенблатта, первая модель нейронной сети</li>
                <li>1969 — Критика перцептрона Минским и Папертом</li>
                <li>1980-е — Возрождение интереса к нейронным сетям, алгоритм обратного распространения ошибки</li>
                <li>1990-е — Развитие методов опорных векторов (SVM) и деревьев решений</li>
                <li>2000-е — Рост популярности ансамблевых методов (Random Forest, Gradient Boosting)</li>
                <li>2010-е — Прорыв в глубоком обучении, появление сверточных и рекуррентных нейронных сетей</li>
                <li>2017-2020 — Архитектура Transformer и модели на ее основе (BERT, GPT)</li>
                <li>2020-настоящее время — Развитие больших языковых моделей (LLM) и мультимодальных систем</li>
            </ul>
            <p><strong>Пояснение:</strong> Развитие машинного обучения происходило волнообразно, с периодами повышенного интереса и "зим ИИ", когда исследования замедлялись из-за недостаточных результатов.</p>
        </div>
    </div>
    <div class="002">
        <h3 class="002-title">Типы машинного обучения</h3>
        <div class="002-card">
            <h4>Основные парадигмы обучения</h4>
            <p>В зависимости от характера обратной связи и доступных данных, машинное обучение делится на несколько основных типов.</p>
            <p><strong>Основные типы:</strong></p>
            <ul>
                <li>Обучение с учителем (Supervised Learning)</li>
                <li>Обучение без учителя (Unsupervised Learning)</li>
                <li>Обучение с подкреплением (Reinforcement Learning)</li>
                <li>Полуавтоматическое обучение (Semi-supervised Learning)</li>
                <li>Самообучение (Self-supervised Learning)</li>
            </ul>
        </div>
    </div>
    <div class="003">
        <h3 class="003-title">Виды алгоритмов машинного обучения</h3>
        <div class="003-card">
            <h4>Классификация алгоритмов по задачам и методам</h4>
            <p>Существует множество алгоритмов машинного обучения, которые можно классифицировать по решаемым задачам и используемым методам.</p>
            <p><strong>По решаемым задачам:</strong></p>
            <ul>
                <li>Алгоритмы классификации</li>
                <li>Алгоритмы регрессии</li>
                <li>Алгоритмы кластеризации</li>
                <li>Алгоритмы снижения размерности</li>
                <li>Алгоритмы генерации данных</li>
            </ul>
            <p><strong>По методам:</strong></p>
            <ul>
                <li>Линейные модели (линейная регрессия, логистическая регрессия)</li>
                <li>Деревья решений и ансамбли (Random Forest, Gradient Boosting)</li>
                <li>Байесовские методы</li>
                <li>Метод опорных векторов (SVM)</li>
                <li>Нейронные сети (включая глубокие нейронные сети)</li>
                <li>Генеративные модели (VAE, GAN, диффузионные модели)</li>
            </ul>
        </div>
    </div>
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Тип обучения</th>
                    <th>Характеристика</th>
                </tr>
            </thead>
            <tr>
                <td>Обучение с учителем</td>
                <td>Использует размеченные данные, где для каждого входного примера известен правильный ответ</td>
            </tr>
            <tr>
                <td>Обучение без учителя</td>
                <td>Работает с неразмеченными данными, выявляя скрытые структуры и закономерности</td>
            </tr>
            <tr>
                <td>Обучение с подкреплением</td>
                <td>Обучение через взаимодействие с окружающей средой и получение обратной связи в виде наград</td>
            </tr>
            <tr>
                <td>Самообучение</td>
                <td>Система сама создает обучающие сигналы из неразмеченных данных</td>
            </tr>
        </table>
    </div>
    <div class="kmp13"><strong>Пример:</strong> Задача распознавания рукописных цифр может решаться разными методами: с помощью SVM, случайного леса или нейронной сети. Каждый метод имеет свои преимущества и ограничения.</div>
</section>

<section id="section3" class="section">
    <h2 class="section-title">3. Глубокое обучение</h2>
    <div class="001">
        <h3 class="001-title">Основы глубокого обучения</h3>
        <div class="001-card">
            <h4>Что такое глубокое обучение</h4>
            <p>Глубокое обучение (Deep Learning) — это подраздел машинного обучения, основанный на использовании многослойных (глубоких) нейронных сетей для моделирования высокоуровневых абстракций в данных.</p>
            <p>Ключевое отличие глубокого обучения от классического машинного обучения заключается в способности автоматически извлекать признаки из сырых данных, не требуя ручного конструирования признаков (feature engineering).</p>
            <p><strong>Основные компоненты глубокого обучения:</strong></p>
            <ul>
                <li>Нейроны — базовые вычислительные единицы</li>
                <li>Слои — группы нейронов, обрабатывающие информацию</li>
                <li>Функции активации — нелинейные преобразования, позволяющие моделировать сложные зависимости</li>
                <li>Веса и смещения — настраиваемые параметры модели</li>
                <li>Функции потерь — метрики, оценивающие ошибку модели</li>
                <li>Алгоритмы оптимизации — методы настройки параметров модели</li>
            </ul>
            <p><strong>Пояснение:</strong> Глубокое обучение получило название из-за использования многослойных (глубоких) архитектур нейронных сетей, где каждый последующий слой обрабатывает всё более абстрактные представления данных.</p>
        </div>
    </div>
    <div class="002">
        <h3 class="002-title">Архитектуры глубоких нейронных сетей</h3>
        <div class="002-card">
            <h4>Основные типы архитектур</h4>
            <p>За годы развития глубокого обучения было разработано множество специализированных архитектур нейронных сетей, оптимизированных для различных задач.</p>
            <p><strong>Ключевые архитектуры:</strong></p>
            <ul>
                <li>Многослойные персептроны (MLP) — полносвязные сети для работы с табличными данными</li>
                <li>Сверточные нейронные сети (CNN) — специализированные сети для обработки изображений</li>
                <li>Рекуррентные нейронные сети (RNN) — сети с обратными связями для обработки последовательностей</li>
                <li>LSTM и GRU — улучшенные версии RNN для работы с длинными последовательностями</li>
                <li>Автоэнкодеры — сети для сжатия данных и выявления скрытых признаков</li>
                <li>Генеративно-состязательные сети (GAN) — архитектура из двух соревнующихся сетей для генерации данных</li>
                <li>Трансформеры — архитектура на основе механизма внимания для обработки последовательностей</li>
            </ul>
        </div>
    </div>
    <div class="003">
        <h3 class="003-title">Обучение глубоких нейронных сетей</h3>
        <div class="003-card">
            <h4>Методы и проблемы обучения</h4>
            <p>Обучение глубоких нейронных сетей представляет собой сложный процесс оптимизации миллионов параметров и сталкивается с рядом специфических проблем.</p>
            <p><strong>Ключевые методы обучения:</strong></p>
            <ul>
                <li>Обратное распространение ошибки (Backpropagation) — основной алгоритм для вычисления градиентов</li>
                                <li>Стохастический градиентный спуск (SGD) и его модификации (Adam, RMSProp) — алгоритмы оптимизации</li>
                <li>Пакетная нормализация (Batch Normalization) — метод стабилизации обучения</li>
                <li>Регуляризация (L1, L2, Dropout) — методы предотвращения переобучения</li>
                <li>Инициализация весов (Xavier, He) — методы начальной установки параметров</li>
            </ul>
            <p><strong>Основные проблемы:</strong></p>
            <ul>
                <li>Исчезающий и взрывной градиент</li>
                <li>Переобучение (overfitting)</li>
                <li>Вычислительная сложность</li>
                <li>Необходимость в больших объемах данных</li>
                <li>Сложность интерпретации моделей</li>
            </ul>
        </div>
    </div>
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Архитектура</th>
                    <th>Основное применение</th>
                </tr>
            </thead>
            <tr>
                <td>CNN</td>
                <td>Компьютерное зрение, распознавание изображений</td>
            </tr>
            <tr>
                <td>RNN/LSTM</td>
                <td>Обработка последовательностей, анализ временных рядов</td>
            </tr>
            <tr>
                <td>Трансформеры</td>
                <td>Обработка естественного языка, машинный перевод</td>
            </tr>
            <tr>
                <td>GAN</td>
                <td>Генерация изображений, аугментация данных</td>
            </tr>
        </table>
    </div>
    <div class="kmp11"><strong>Примечание:</strong> Глубокое обучение стало возможным благодаря трем ключевым факторам: наличию больших объемов данных, развитию алгоритмов и увеличению вычислительных мощностей (особенно GPU).</div>
    <div class="kmp14"><strong>Важно:</strong> Несмотря на впечатляющие результаты, глубокие нейронные сети часто работают как "черный ящик", что затрудняет интерпретацию их решений и может создавать проблемы в критически важных приложениях.</div>
</section>

<section id="section4" class="section">
    <h2 class="section-title">4. Deep Learning LLM</h2>
    <div class="001">
        <h3 class="001-title">Большие языковые модели</h3>
        <div class="001-card">
            <h4>Что такое LLM</h4>
            <p>Большие языковые модели (Large Language Models, LLM) — это масштабные нейронные сети, обученные на огромных корпусах текстов для понимания и генерации естественного языка.</p>
            <p>LLM относятся к классу генеративных моделей и основаны преимущественно на архитектуре Трансформер, предложенной в 2017 году. Ключевой особенностью LLM является их размер — современные модели содержат от нескольких миллиардов до триллионов параметров.</p>
            <p><strong>Ключевые характеристики LLM:</strong></p>
            <ul>
                <li>Масштаб — огромное количество параметров (от миллиардов до триллионов)</li>
                <li>Обучение на разнообразных текстовых данных (сотни гигабайт или терабайты текста)</li>
                <li>Способность к генерации связного и контекстуально релевантного текста</li>
                <li>Возможность решать разнообразные языковые задачи без специфического дообучения</li>
                <li>Эмерджентные способности — возникновение новых возможностей при увеличении масштаба модели</li>
            </ul>
            <p><strong>Пояснение:</strong> Термин "большие" в названии отражает не только физический размер моделей, но и их способность охватывать широкий спектр знаний и языковых паттернов.</p>
        </div>
    </div>
    <div class="002">
        <h3 class="002-title">Архитектура современных LLM</h3>
        <div class="002-card">
            <h4>Трансформеры как основа LLM</h4>
            <p>Современные LLM базируются на архитектуре Трансформер, которая произвела революцию в обработке естественного языка благодаря механизму внимания (attention mechanism).</p>
            <p><strong>Ключевые компоненты архитектуры:</strong></p>
            <ul>
                <li>Механизм внимания (Attention) — позволяет модели фокусироваться на различных частях входной последовательности</li>
                <li>Многоголовое внимание (Multi-head Attention) — параллельное применение нескольких механизмов внимания</li>
                <li>Позиционное кодирование (Positional Encoding) — способ учета порядка слов в последовательности</li>
                <li>Нормализация слоев (Layer Normalization) — стабилизация обучения</li>
                <li>Остаточные связи (Residual Connections) — улучшение градиентного потока</li>
                <li>Прямая нейронная сеть (Feed-Forward Network) — обработка контекстуализированных представлений</li>
            </ul>
            <p><strong>Варианты архитектур LLM:</strong></p>
            <ul>
                <li>Encoder-only (BERT) — для понимания языка и классификации</li>
                <li>Decoder-only (GPT) — для генерации текста</li>
                <li>Encoder-Decoder (T5, BART) — для задач преобразования текста</li>
            </ul>
        </div>
    </div>
    <div class="003">
        <h3 class="003-title">Процесс обучения LLM</h3>
        <div class="003-card">
            <h4>Этапы создания языковой модели</h4>
            <p>Обучение современных LLM — это сложный многоэтапный процесс, требующий значительных вычислительных ресурсов и специализированных методик.</p>
            <p><strong>Основные этапы обучения:</strong></p>
            <ul>
                <li>Сбор и подготовка корпуса текстов</li>
                <li>Токенизация — разбиение текста на базовые единицы (токены)</li>
                <li>Предобучение (Pretraining) — обучение на задаче языкового моделирования</li>
                <li>Дообучение (Fine-tuning) — адаптация к конкретным задачам</li>
                <li>Обучение с человеческими предпочтениями (RLHF) — улучшение полезности и безопасности</li>
                <li>Оценка и тестирование модели</li>
            </ul>
            <p><strong>Особенности обучения LLM:</strong></p>
            <ul>
                <li>Распределенное обучение на кластерах GPU/TPU</li>
                <li>Оптимизация памяти (gradient checkpointing, mixed precision)</li>
                <li>Специализированные оптимизаторы и расписания обучения</li>
                <li>Техники регуляризации для предотвращения переобучения</li>
            </ul>
        </div>
    </div>
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Модель</th>
                    <th>Архитектура</th>
                </tr>
            </thead>
            <tr>
                <td>GPT (OpenAI)</td>
                <td>Decoder-only Transformer</td>
            </tr>
            <tr>
                <td>BERT (Google)</td>
                <td>Encoder-only Transformer</td>
            </tr>
            <tr>
                <td>T5 (Google)</td>
                <td>Encoder-Decoder Transformer</td>
            </tr>
            <tr>
                <td>LLaMA (Meta)</td>
                <td>Decoder-only Transformer с оптимизациями</td>
            </tr>
        </table>
    </div>
    <div class="kmp12"><strong>Внимание:</strong> Обучение современных LLM требует огромных вычислительных ресурсов. Например, обучение GPT-4 оценивается в миллионы долларов только на вычислительные затраты.</div>
    <div class="kmp13"><strong>Пример:</strong> Модель GPT-3 содержит 175 миллиардов параметров и была обучена на корпусе текстов размером около 570 ГБ, включающем книги, статьи, веб-страницы и другие источники.</div>
</section>

<section id="section5" class="section">
    <h2 class="section-title">5. Предобучение LLM</h2>
    <div class="001">
        <h3 class="001-title">Концепция предобучения</h3>
        <div class="001-card">
            <h4>Что такое предобучение</h4>
            <p>Предобучение (Pretraining) — это первый и наиболее ресурсоемкий этап в создании больших языковых моделей, в ходе которого модель обучается на огромных массивах неразмеченных текстовых данных.</p>
            <p>Цель предобучения — дать модели возможность усвоить статистические закономерности языка, семантические связи между словами и общие знания о мире, содержащиеся в текстах.</p>
            <p><strong>Ключевые аспекты предобучения:</strong></p>
            <ul>
                <li>Самообучение (self-supervised learning) — модель сама создает обучающие сигналы из данных</li>
                <li>Масштаб — использование огромных корпусов текста (сотни гигабайт или терабайты)</li>
                <li>Длительность — процесс может занимать недели или месяцы даже на мощных кластерах</li>
                <li>Общность — модель приобретает универсальные языковые навыки, не специфичные для конкретных задач</li>
            </ul>
            <p><strong>Пояснение:</strong> Предобучение можно сравнить с общим образованием человека, которое дает базовые знания и навыки, на основе которых потом можно специализироваться в конкретных областях.</p>
        </div>
    </div>
    <div class="002">
        <h3 class="002-title">Задачи предобучения</h3>
        <div class="002-card">
            <h4>Основные подходы к предобучению LLM</h4>
            <p>В зависимости от архитектуры модели и целей обучения используются различные задачи предобучения.</p>
            <p><strong>Основные задачи предобучения:</strong></p>
            <ul>
                <li>Маскированное языковое моделирование (MLM) — предсказание замаскированных слов в тексте (BERT)</li>
                <li>Авторегрессивное языковое моделирование — предсказание следующего слова в последовательности (GPT)</li>
                <li>Предсказание перемешанных предложений (NSP) — определение, следуют ли предложения друг за другом</li>
                <li>Восстановление поврежденного текста (Denoising) — восстановление искаженных фрагментов (BART, T5)</li>
                <li>Контрастивное обучение — различение связанных и несвязанных фрагментов текста</li>
            </ul>
            <p><strong>Современные тенденции:</strong></p>
            <ul>
                <li>Увеличение контекстного окна — обучение на более длинных последовательностях</li>
                <li>Мультимодальное предобучение — совместное обучение на тексте и других типах данных (изображения, аудио)</li>
                <li>Инструктивное предобучение — включение инструкций и демонстраций в процесс предобучения</li>
            </ul>
        </div>
    </div>
    <div class="003">
        <h3 class="003-title">Данные для предобучения</h3>
        <div class="003-card">
            <h4>Корпуса текстов и их подготовка</h4>
            <p>Качество и разнообразие данных для предобучения имеют критическое значение для создания эффективных LLM.</p>
            <p><strong>Типичные источники данных:</strong></p>
            <ul>
                <li>Веб-корпуса (Common Crawl, C4)</li>
                <li>Книги (BookCorpus, Project Gutenberg)</li>
                <li>Научные статьи (arXiv, PubMed)</li>
                <li>Энциклопедии (Wikipedia)</li>
                <li>Код программ (GitHub)</li>
                <li>Диалоги и обсуждения (Reddit, Stack Exchange)</li>
            </ul>
            <p><strong>Процесс подготовки данных:</strong></p>
            <ul>
                <li>Сбор и агрегация из различных источников</li>
                <li>Фильтрация низкокачественного контента</li>
                <li>Дедупликация — удаление повторяющихся фрагментов</li>
                <li>Токенизация — разбиение текста на базовые единицы</li>
                                <li>Балансировка — обеспечение представленности различных доменов и языков</li>
                <li>Перемешивание — предотвращение переобучения на определенных паттернах</li>
            </ul>
        </div>
    </div>
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Задача предобучения</th>
                    <th>Применение</th>
                </tr>
            </thead>
            <tr>
                <td>Маскированное языковое моделирование (MLM)</td>
                <td>BERT, RoBERTa, DeBERTa</td>
            </tr>
            <tr>
                <td>Авторегрессивное языковое моделирование</td>
                <td>GPT, LLaMA, Claude</td>
            </tr>
            <tr>
                <td>Восстановление поврежденного текста</td>
                <td>T5, BART</td>
            </tr>
            <tr>
                <td>Контрастивное обучение</td>
                <td>CLIP, SimCSE</td>
            </tr>
        </table>
    </div>
    <div class="kmp11"><strong>Примечание:</strong> Современные LLM часто обучаются на смешанных корпусах, где различные источники данных взвешиваются в зависимости от их качества и релевантности для целевых задач.</div>
    <div class="kmp14"><strong>Важно:</strong> Данные для предобучения могут содержать предвзятости и токсичный контент, которые модель может усвоить. Поэтому фильтрация и курирование данных становятся критически важными этапами.</div>
</section>

<section id="section6" class="section">
    <h2 class="section-title">6. Transfer Learning</h2>
    <div class="001">
        <h3 class="001-title">Концепция переноса обучения</h3>
        <div class="001-card">
            <h4>Что такое Transfer Learning</h4>
            <p>Transfer Learning (перенос обучения) — это методика машинного обучения, при которой модель, обученная на одной задаче, используется как отправная точка для решения другой, связанной задачи.</p>
            <p>В контексте LLM, перенос обучения позволяет использовать знания и навыки, полученные моделью в процессе предобучения на общих текстовых данных, для решения специфических задач с меньшим количеством размеченных данных.</p>
            <p><strong>Ключевые преимущества Transfer Learning:</strong></p>
            <ul>
                <li>Экономия вычислительных ресурсов — нет необходимости обучать модель с нуля</li>
                <li>Снижение требований к объему размеченных данных</li>
                <li>Улучшение производительности на задачах с ограниченными данными</li>
                <li>Ускорение процесса обучения</li>
                <li>Возможность использования сложных архитектур для простых задач</li>
            </ul>
            <p><strong>Пояснение:</strong> Transfer Learning можно сравнить с тем, как человек, изучивший один иностранный язык, быстрее осваивает родственный язык, используя уже имеющиеся знания и навыки.</p>
        </div>
    </div>
    <div class="002">
        <h3 class="002-title">Типы переноса обучения</h3>
        <div class="002-card">
            <h4>Различные подходы к Transfer Learning</h4>
            <p>В зависимости от характера исходной и целевой задач, а также доступных данных, выделяют несколько типов переноса обучения.</p>
            <p><strong>Основные типы Transfer Learning:</strong></p>
            <ul>
                <li>Индуктивный перенос — исходная и целевая задачи различны, но связаны</li>
                <li>Трансдуктивный перенос — задачи одинаковы, но домены данных различаются</li>
                <li>Неконтролируемый перенос — целевая задача не имеет размеченных данных</li>
            </ul>
            <p><strong>В контексте LLM выделяют:</strong></p>
            <ul>
                <li>Feature-based Transfer Learning — использование предобученной модели как экстрактора признаков</li>
                <li>Fine-tuning — дообучение всей модели или ее части на целевой задаче</li>
                <li>Adapter-based Transfer Learning — добавление небольших обучаемых модулей к замороженной модели</li>
                <li>Prompt-based Learning — формулирование задачи в виде текстового запроса без изменения параметров модели</li>
                <li>In-context Learning — обучение на нескольких примерах, предоставленных в контексте запроса</li>
            </ul>
        </div>
    </div>
    <div class="003">
        <h3 class="003-title">Применение Transfer Learning в NLP</h3>
        <div class="003-card">
            <h4>Эволюция переноса обучения в обработке естественного языка</h4>
            <p>Перенос обучения произвел революцию в области обработки естественного языка, значительно повысив эффективность решения различных языковых задач.</p>
            <p><strong>Историческое развитие:</strong></p>
            <ul>
                <li>Предобученные word embeddings (Word2Vec, GloVe) — первый шаг к переносу знаний</li>
                <li>Контекстуализированные embeddings (ELMo) — учет контекста слов</li>
                <li>Предобученные языковые модели (BERT, GPT) — перенос глубоких языковых знаний</li>
                <li>Многозадачное обучение (T5, MT-DNN) — одновременное обучение на нескольких задачах</li>
                <li>Инструктивное обучение (InstructGPT) — адаптация к выполнению инструкций</li>
                <li>Few-shot и zero-shot learning — решение задач с минимальным количеством примеров или без них</li>
            </ul>
            <p><strong>Современные применения:</strong></p>
            <ul>
                <li>Классификация текстов (сентимент-анализ, определение тематики)</li>
                <li>Извлечение информации (NER, отношения между сущностями)</li>
                <li>Машинный перевод и парафразирование</li>
                <li>Генерация текста и диалоговые системы</li>
                <li>Ответы на вопросы и информационный поиск</li>
                <li>Мультимодальные задачи (текст-изображение, текст-аудио)</li>
            </ul>
        </div>
    </div>
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Метод Transfer Learning</th>
                    <th>Характеристика</th>
                </tr>
            </thead>
            <tr>
                <td>Feature Extraction</td>
                <td>Использование предобученной модели только для извлечения признаков, обучение нового классификатора</td>
            </tr>
            <tr>
                <td>Fine-tuning</td>
                <td>Дообучение всей модели или ее части на новой задаче с меньшей скоростью обучения</td>
            </tr>
            <tr>
                <td>Parameter-Efficient Fine-tuning</td>
                <td>Обновление только небольшого подмножества параметров (LoRA, Adapters, Prefix-tuning)</td>
            </tr>
            <tr>
                <td>Prompt Engineering</td>
                <td>Формулирование задачи в виде текстового запроса без изменения параметров модели</td>
            </tr>
        </table>
    </div>
    <div class="kmp12"><strong>Внимание:</strong> При переносе обучения важно учитывать различия между доменами исходных и целевых данных. Чем больше эти различия, тем сложнее эффективный перенос знаний.</div>
    <div class="kmp13"><strong>Пример:</strong> Модель BERT, предобученная на общих текстовых корпусах, может быть адаптирована для медицинской диагностики путем дообучения на клинических текстах, что требует значительно меньше размеченных данных, чем обучение специализированной модели с нуля.</div>
</section>

<section id="section7" class="section">
    <h2 class="section-title">7. Fine-Tuning</h2>
    <div class="001">
        <h3 class="001-title">Основы Fine-Tuning</h3>
        <div class="001-card">
            <h4>Что такое Fine-Tuning</h4>
            <p>Fine-Tuning (тонкая настройка) — это процесс дообучения предварительно обученной модели на специфическом наборе данных для адаптации к конкретной задаче или домену.</p>
            <p>В контексте LLM, fine-tuning позволяет адаптировать общие языковые знания, полученные в процессе предобучения, к специфическим требованиям конкретных приложений или предметных областей.</p>
            <p><strong>Ключевые аспекты Fine-Tuning:</strong></p>
            <ul>
                <li>Использование меньшей скорости обучения, чем при предобучении</li>
                <li>Обучение на относительно небольшом наборе размеченных данных</li>
                <li>Возможность обновления всех или части параметров модели</li>
                <li>Сохранение общих языковых знаний при адаптации к специфике задачи</li>
                <li>Балансирование между адаптацией и сохранением общих способностей</li>
            </ul>
            <p><strong>Пояснение:</strong> Fine-tuning можно сравнить с профессиональной специализацией человека после получения общего образования — базовые знания дополняются специфическими навыками в конкретной области.</p>
        </div>
    </div>
    <div class="002">
        <h3 class="002-title">Методы Fine-Tuning для LLM</h3>
        <div class="002-card">
            <h4>Современные подходы к тонкой настройке</h4>
            <p>С ростом размера языковых моделей традиционные методы fine-tuning становятся вычислительно затратными, что привело к разработке более эффективных подходов.</p>
            <p><strong>Основные методы Fine-Tuning:</strong></p>
            <ul>
                <li>Full Fine-Tuning — обновление всех параметров модели</li>
                <li>Parameter-Efficient Fine-Tuning (PEFT) — обновление только части параметров</li>
                <li>LoRA (Low-Rank Adaptation) — добавление низкоранговых адаптеров к весам модели</li>
                <li>Prefix Tuning — добавление обучаемых префиксов к входным последовательностям</li>
                <li>Adapter Tuning — вставка небольших обучаемых модулей между слоями</li>
                <li>Prompt Tuning — обучение непрерывных векторных промптов</li>
                <li>QLoRA — квантизация модели с низкоранговыми адаптерами для экономии памяти</li>
            </ul>
            <p><strong>Специализированные методы:</strong></p>
            <ul>
                <li>Instruction Tuning — обучение на наборах инструкций и ответов</li>
                <li>RLHF (Reinforcement Learning from Human Feedback) — обучение с подкреплением на основе человеческих предпочтений</li>
                <li>DPO (Direct Preference Optimization) — прямая оптимизация предпочтений без RL</li>
                <li>Constitutional AI — обучение с учетом заданных принципов и ограничений</li>
            </ul>
        </div>
    </div>
    <div class="003">
        <h3 class="003-title">Практические аспекты Fine-Tuning</h3>
        <div class="003-card">
            <h4>Подготовка данных и процесс обучения</h4>
            <p>Успешное fine-tuning требует тщательной подготовки данных и настройки процесса обучения с учетом специфики задачи и доступных ресурсов.</p>
            <p><strong>Подготовка данных:</strong></p>
            <ul>
                <li>Сбор и курирование качественных обучающих примеров</li>
                <li>Форматирование данных в соответствии с требованиями модели</li>
                <li>Балансировка классов или типов примеров</li>
                <li>Аугментация данных для увеличения разнообразия</li>
                <li>Разделение на обучающую, валидационную и тестовую выборки</li>
            </ul>
            <p><strong>Процесс обучения:</strong></p>
            <ul>
                <li>Выбор оптимальных гиперпараметров (скорость обучения, размер батча, количество эпох)</li>
                <li>Применение техник регуляризации для предотвращения переобучения</li>
                <li>Мониторинг метрик на валидационной выборке</li>
                <li>Раннее останавливание (early stopping) при отсутствии улучшений</li>
                <li>Сохранение контрольных точек (checkpoints) в процессе обучения</li>
                <li>Оценка на тестовой выборке после завершения обучения</li>
            </ul>
                        <p><strong>Проблемы и решения:</strong></p>
            <ul>
                <li>Катастрофическое забывание — потеря общих способностей при адаптации к специфической задаче</li>
                <li>Вычислительная сложность — использование техник оптимизации памяти и вычислений</li>
                <li>Переобучение на малых выборках — применение регуляризации и аугментации данных</li>
                <li>Сохранение этических ограничений — включение примеров безопасного поведения в обучающие данные</li>
            </ul>
        </div>
    </div>
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Метод Fine-Tuning</th>
                    <th>Преимущества</th>
                </tr>
            </thead>
            <tr>
                <td>Full Fine-Tuning</td>
                <td>Максимальная адаптация к задаче, высокая производительность</td>
            </tr>
            <tr>
                <td>LoRA</td>
                <td>Экономия памяти, быстрое обучение, возможность комбинирования адаптеров</td>
            </tr>
            <tr>
                <td>Adapter Tuning</td>
                <td>Модульность, возможность переключения между задачами</td>
            </tr>
            <tr>
                <td>Instruction Tuning</td>
                <td>Улучшение способности следовать инструкциям, обобщение на новые задачи</td>
            </tr>
            <tr>
                <td>RLHF</td>
                <td>Улучшение полезности и безопасности, соответствие человеческим предпочтениям</td>
            </tr>
        </table>
    </div>
    <div class="kmp11"><strong>Примечание:</strong> Для многих практических применений достаточно fine-tuning только на нескольких сотнях или тысячах примеров, что делает этот подход доступным даже при ограниченных ресурсах.</div>
    <div class="kmp12"><strong>Внимание:</strong> При fine-tuning важно сохранять разнообразие обучающих данных, чтобы избежать чрезмерной специализации модели и потери способности к обобщению.</div>
    <div class="kmp14"><strong>Важно:</strong> Современные методы parameter-efficient fine-tuning позволяют адаптировать модели с миллиардами параметров, обновляя менее 1% от их общего количества, что значительно снижает вычислительные требования.</div>
</section>

<section id="section8" class="section">
    <h2 class="section-title">8. Заключение</h2>
    <div class="001">
        <h3 class="001-title">Современное состояние и перспективы</h3>
        <div class="001-card">
            <h4>Текущие тенденции в обучении LLM</h4>
            <p>Обучение больших языковых моделей — быстро развивающаяся область с постоянно появляющимися инновациями и подходами.</p>
            <p><strong>Ключевые тенденции:</strong></p>
            <ul>
                <li>Масштабирование — увеличение размера моделей и объемов обучающих данных</li>
                <li>Эффективность — разработка методов, снижающих вычислительные требования</li>
                <li>Мультимодальность — интеграция текстовых, визуальных и аудиомодальностей</li>
                <li>Специализация — создание моделей для конкретных доменов и языков</li>
                <li>Интерпретируемость — разработка методов понимания внутренних механизмов моделей</li>
                <li>Безопасность — методы снижения рисков и вредоносного использования</li>
                <li>Демократизация — упрощение доступа к технологиям LLM для широкого круга пользователей</li>
            </ul>
            <p><strong>Перспективные направления исследований:</strong></p>
            <ul>
                <li>Обучение с меньшим количеством данных и вычислительных ресурсов</li>
                <li>Улучшение долговременной памяти и рассуждений</li>
                <li>Интеграция внешних инструментов и источников знаний</li>
                <li>Персонализация моделей под конкретных пользователей</li>
                <li>Непрерывное обучение и адаптация к изменяющимся условиям</li>
                <li>Разработка объективных метрик оценки производительности</li>
            </ul>
        </div>
    </div>
    <div class="002">
        <h3 class="002-title">Практические рекомендации</h3>
        <div class="002-card">
            <h4>Советы по работе с LLM для лингвистов и преподавателей</h4>
            <p>Для будущих лингвистов и преподавателей иностранных языков важно понимать, как эффективно использовать и адаптировать языковые модели в своей профессиональной деятельности.</p>
            <p><strong>Рекомендации по использованию LLM:</strong></p>
            <ul>
                <li>Начинайте с готовых моделей и адаптируйте их под свои задачи</li>
                <li>Используйте техники prompt engineering для эффективного взаимодействия с моделями</li>
                <li>Собирайте качественные данные, специфичные для вашей предметной области</li>
                <li>Применяйте parameter-efficient fine-tuning для адаптации моделей с ограниченными ресурсами</li>
                <li>Комбинируйте возможности LLM с традиционными лингвистическими методами</li>
                <li>Оценивайте результаты работы моделей с учетом лингвистических критериев</li>
                <li>Следите за этическими аспектами использования LLM в образовании</li>
            </ul>
            <p><strong>Применение в лингвистике и преподавании:</strong></p>
            <ul>
                <li>Создание персонализированных учебных материалов</li>
                <li>Разработка интерактивных языковых упражнений</li>
                <li>Автоматическая оценка и обратная связь по письменным работам</li>
                <li>Моделирование диалогов для практики разговорной речи</li>
                <li>Лингвистический анализ и корпусные исследования</li>
                <li>Перевод и локализация контента</li>
                <li>Создание адаптивных систем обучения языкам</li>
            </ul>
        </div>
    </div>
    <div class="table-kmp">
        <table class="table">
            <thead>
                <tr>
                    <th>Этап обучения LLM</th>
                    <th>Ключевая характеристика</th>
                </tr>
            </thead>
            <tr>
                <td>Предобучение</td>
                <td>Приобретение общих языковых знаний на огромных корпусах текста</td>
            </tr>
            <tr>
                <td>Transfer Learning</td>
                <td>Перенос полученных знаний на новые задачи и домены</td>
            </tr>
            <tr>
                <td>Fine-Tuning</td>
                <td>Адаптация модели к специфическим задачам и требованиям</td>
            </tr>
            <tr>
                <td>RLHF</td>
                <td>Улучшение полезности и безопасности на основе человеческих предпочтений</td>
            </tr>
        </table>
    </div>
    <div class="kmp13"><strong>Пример:</strong> Преподаватель английского языка может использовать предобученную модель, дообучить ее на корпусе академических текстов с помощью LoRA, а затем применять для создания упражнений, проверки эссе студентов и моделирования диалогов на профессиональные темы.</div>
    <div class="kmp14"><strong>Важно:</strong> Несмотря на впечатляющие возможности LLM, они должны рассматриваться как инструменты, дополняющие, а не заменяющие экспертизу лингвистов и преподавателей. Критическое мышление и профессиональная оценка остаются необходимыми при использовании этих технологий.</div>
</section>



<footer class="footer">
<div class="container">
<p>© 2025 | kmp | CC BY-NC-SA 4.0<br>
Разработано для студентов БрГУ имени А.С. Пушкина</p>
</div>
</footer>
<div style="position: fixed; bottom: 10px; color: #777777; right: 30px; opacity: 0.3; font-size: 14px;">kmp+</div>
    <script>
        // Функция для плавной прокрутки к разделу
        function scrollToSection(sectionId) {
            const section = document.getElementById(sectionId);
            const menuHeight = document.querySelector('.menu').offsetHeight;
            
            window.scrollTo({
                top: section.offsetTop - menuHeight - 20,
                behavior: 'smooth'
            });
        }

        // Функция для переключения темы
        document.getElementById('themeToggle').addEventListener('click', function() {
            const currentTheme = document.documentElement.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            
            document.documentElement.setAttribute('data-theme', newTheme);
            this.textContent = newTheme === 'dark' ? '🌙' : '☀️';
        });

        // Анимация появления секций при прокрутке
        document.addEventListener('DOMContentLoaded', function() {
            const sections = document.querySelectorAll('.section');
            
            const observerOptions = {
                root: null,
                rootMargin: '0px',
                threshold: 0.1
            };
            
            const observer = new IntersectionObserver(function(entries, observer) {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('fade-in');
                        observer.unobserve(entry.target);
                    }
                });
            }, observerOptions);
            
            sections.forEach(section => {
                section.classList.remove('fade-in');
                observer.observe(section);
            });
        });
    </script>
</body>
</html>