<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>kmp+</title>
    <style>
        /* Основные стили */
        :root {
            --primary-color: #325980;
            --secondary-color: #4CAF50;
            --background-color: #f5f5f5;
            --content-bg: #ffffff;
            --text-color: #333333;
            --header-text-color: #ffffff;
            --menu-bg: #ffffff;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            --border-radius: 8px;
			  --accent11: #4caf50;
		--accent11: #4cafff;
		--accent12: #4cafff;
		--accent13: #ffaf50;
		--accent14: #821978;
        }

        /* Темная тема */
        [data-theme="dark"] {
            --primary-color: #3e76ad;
            --secondary-color: #388e3c;
            --background-color: #000000;
            --content-bg: #1e1e1e;
            --text-color: #e0e0e0;
            --header-text-color: #ffffff;
            --menu-bg: #000000;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            transition: background-color 0.3s, color 0.3s;
        }

        body {
            font-family: 'Roboto', 'Arial', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--background-color);
            padding-top: 2px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        header {
            background-color: var(--primary-color);
            color: var(--header-text-color);
            padding: 20px 0;
            text-align: center;
            border-radius: var(--border-radius);
            margin-bottom: 2px;
        }

        h1 {
            font-size: 2.2rem;
            margin-bottom: 10px;
        }

        h2 {
            color: var(--primary-color);
            margin: 25px 0 15px;
            padding-bottom: 8px;
            border-bottom: 2px solid var(--secondary-color);
        }

        h3 {
            color: var(--primary-color);
            margin: 20px 0 10px;
        }

        p {
            margin-bottom: 15px;
        }

        /* Меню навигации */
        .menu {
            background-color: var(--menu-bg);
            padding: 15px 20px;
            border-radius: var(--border-radius);
            margin-bottom: 30px;
            box-shadow: var(--menu-shadow);
            display: flex;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .menu-buttons {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }
		
		
		.menu-buttons {
    display: flex;
    flex-wrap: wrap;
    gap: 10px;
    justify-content: center; /* Изменено на center */
}

        .menu-btn {
            background-color: var(--primary-color);
            color: white;
            border: none;
            padding: 8px 15px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: background-color 0.3s;
        }

        .menu-btn:hover {
            background-color: var(--secondary-color);
        }

        .theme-toggle {
            background: none;
            border: 10px solid transparent;
            font-size: 1.5rem;
            cursor: pointer;
            color: var(--primary-color);
        }

        /* Секции контента */
        .section {
            background-color: var(--content-bg);
            border-radius: var(--border-radius);
            padding: 25px;
            margin-bottom: 30px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        /* Выделение важного */
        .important {
            background-color: rgba(76, 175, 80, 0.1);
            border-left: 4px solid var(--secondary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        .note {
            background-color: rgba(50, 89, 128, 0.1);
            border-left: 4px solid var(--primary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        /* Таблицы */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            padding: 12px 15px;
            border: 1px solid #ddd;
            text-align: left;
        }

        th {
            background-color: var(--primary-color);
            color: white;
        }

        tr:nth-child(even) {
            background-color: rgba(0, 0, 0, 0.03);
        }

        /* Списки */
        ul, ol {
            padding-left: 25px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
        }

        /* Адаптивный дизайн */
        @media (max-width: 768px) {
            h1 {
                font-size: 1.8rem;
            }
            
            h2 {
                font-size: 1.5rem;
            }
            
            .menu {
                flex-direction: column;
                gap: 15px;
            }
            
            .menu-buttons {
                width: 100%;
                justify-content: center;
            }
            
            .theme-toggle {
                margin-top: 10px;
            }
            
            .section {
                padding: 15px;
            }
        }

        /* Анимации */
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .fade-in {
            animation: fadeIn 0.5s ease-in;
        }

        footer {
            text-align: center;
            padding: 20px 0;
            margin-top: 40px;
            font-size: 0.9rem;
        }
		
					.kmp11, .example {
      background: rgba(76, 175, 80, 0.1);
      border-left: 4px solid var(--accent11);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp12, .example {
      background: rgba(95, 182, 237, 0.1);
      border-left: 4px solid var(--accent12);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
 
		.kmp13, .example {
      background: rgba(205, 170, 110, 0.1);
      border-left: 4px solid var(--accent13);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp14, .example {
      background: rgba(205, 110, 200, 0.1);
      border-left: 4px solid var(--accent14);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
	.link-kmp1 {
            color: #fffee0; 
            background-color: #007bff;
            padding: 0em 0.3em; 
            margin: 0 0.5em; /* Добавляет 0.5em отступа справа и слева */
            text-decoration: none; 
            border-radius: 5px; 
            transition: background-color 0.2s ease-in-out, color 0.2s ease-in-out;
				}
		.link-kmp1:hover,
		.link-kmp1:focus {
           color: #ffffff; 
           background-color: #0bb313; 
           text-decoration: none; 
        }
	
	
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Model linguistics | AI-First</h1>
            <p>Модельная лингвистика эпохи LLM</p>
        </header>
От формальных правил к с
        <nav class="menu">
            <div class="menu-buttons">
				<button class="menu-btn" onclick="scrollToSection('1')">Моделирование</button>
                <button class="menu-btn" onclick="scrollToSection('2')">Вычисления</button>
				<button class="menu-btn" onclick="scrollToSection('3')">AI-First</button>
                <button class="menu-btn" onclick="scrollToSection('4')">Метолология</button>
				<button class="menu-btn" onclick="scrollToSection('5')">Приложения</button>
				<button class="menu-btn" onclick="scrollToSection('6')">Перспективы</button>
				</div>
            <button class="theme-toggle" id="themeToggle" title="Переключить тему">☀️</button>
        </nav>


<section id="1" class="section">
    <h2 class="section-title">Моделирование языка</h2>
    
    <div class="101">
        <h3 class="101-title">Язык как динамическая знаковая система</h3>
        <div class="101-card">
            <h4>Переосмысление природы языка</h4>
            <p>Традиционная лингвистика рассматривала язык преимущественно как статичный объект исследования — замкнутую систему знаков, поддающуюся исчерпывающему описанию. Парадигма AI-First радикально меняет эту перспективу: язык понимается как <em>динамическая, самоорганизующаяся система</em>, находящаяся в постоянном взаимодействии со средой.</p>
            <p>Ключевой сдвиг состоит в переходе от вопроса «Что такое язык?» к вопросу «Как язык работает?» — от онтологии к операциональности.</p>
            <p><strong>Основные характеристики динамической модели языка:</strong></p>
            <ul>
                <li><strong>Эмерджентность</strong> — языковые структуры возникают из взаимодействия простых элементов</li>
                <li><strong>Адаптивность</strong> — система изменяется под воздействием употребления</li>
                <li><strong>Контекстуальность</strong> — значение определяется окружением, а не изолированно</li>
                <li><strong>Вероятностность</strong> — языковые явления описываются распределениями, а не детерминированными правилами</li>
				<li><strong>Стохастичность</strong> — полноценное включение контролируемой случайности в процесс генерации (sampling с temperature, top-k/top-p). Это позволяет моделировать креативность, вариативность естественного языка, устойчивость к неопределённости и эмерджентные эффекты.</li>
            </ul>
        </div>
        
        <div class="101-card">
            <h4>Понятие семиозиса в компьютерной среде</h4>
            <p>Семиозис — процесс порождения и интерпретации знаков — приобретает новое измерение в контексте вычислительных систем. Если классическая семиотика Пирса описывала триаду «знак — объект — интерпретант» в рамках человеческого познания, то компьютерный семиозис ставит вопрос: может ли машина быть полноценным участником знакового процесса?</p>
            <p><strong>Уровни компьютерного семиозиса:</strong></p>
            <ul>
                <li><strong>Синтаксический</strong> — манипуляция символами по формальным правилам</li>
                <li><strong>Статистический</strong> — выявление закономерностей в распределениях знаков</li>
                <li><strong>Функциональный</strong> — порождение знаков, влияющих на поведение других агентов</li>
                <li><strong>Прагматический</strong> — достижение коммуникативных целей через знаковые операции</li>
            </ul>
            <p><strong>Пояснение:</strong> Большие языковые модели демонстрируют функциональный семиозис: они порождают тексты, которые интерпретируются людьми как осмысленные, хотя вопрос о наличии «понимания» у машины остаётся философски открытым.</p>
        </div>
    </div>
    
    <div class="102">
        <h3 class="102-title">От статического описания к операциональной модели</h3>
        <div class="102-card">
            <h4>Лингвистическая модель как «операциональная метафора»</h4>
            <p>Понятие операциональной метафоры восходит к идеям кибернетики и системной теории. Модель — это не просто описание объекта, а <em>работающий механизм</em>, способный имитировать поведение моделируемой системы.</p>
            <p>Применительно к языку это означает: лингвистическая модель должна не только описывать, какие предложения грамматичны, но и <em>порождать</em> грамматичные предложения, <em>предсказывать</em> следующее слово, <em>классифицировать</em> тексты по заданным критериям.</p>
            <p><strong>Критерии операциональности модели:</strong></p>
            <ul>
                <li><strong>Воспроизводимость</strong> — модель даёт предсказуемые результаты при одинаковых входных данных</li>
                <li><strong>Тестируемость</strong> — предсказания модели можно сопоставить с эмпирическими данными</li>
                <li><strong>Продуктивность</strong> — модель способна обрабатывать ранее не встречавшиеся примеры</li>
                <li><strong>Масштабируемость</strong> — модель работает на данных разного объёма</li>
            </ul>
        </div>
        
        <div class="102-card">
            <h4>Инструментализм в лингвистике</h4>
            <p>Инструментализм утверждает: научные теории — это инструменты для предсказания и объяснения, а не «зеркала реальности». Любое описание языка — аппроксимация, приближение к реальности, точность которого определяется целями исследования.</p>
            <p>Этот подход освобождает лингвиста от необходимости искать «истинную» модель языка и переориентирует на прагматический критерий: насколько модель полезна для решения конкретных задач?</p>
            <p><strong>Следствия инструменталистского подхода:</strong></p>
            <ul>
                <li>Допустимость множественных моделей одного явления</li>
                <li>Оценка модели по её предсказательной силе, а не по «истинности»</li>
                <li>Признание ограниченности любой формализации</li>
                <li>Фокус на практической применимости</li>
            </ul>
        </div>
    </div>
    
    <div class="103">
        <h3 class="103-title">Иерархия и классификация моделей</h3>
        <div class="103-card">
            <h4>Типология лингвистических моделей</h4>
            <p>Лингвистические модели можно классифицировать по нескольким основаниям, однако наиболее продуктивным для парадигмы AI-First является функциональный критерий: что модель делает с языковыми данными?</p>
        </div>
        
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr>
                        <th>Тип модели</th>
                        <th>Функция</th>
                        <th>Примеры</th>
                        <th>Применение в AI</th>
                    </tr>
                </thead>
                <tr>
                    <td><strong>Дескриптивные</strong></td>
                    <td>Описание и систематизация языковых фактов</td>
                    <td>Грамматики, словари, корпусы</td>
                    <td>Обучающие датасеты, аннотации</td>
                </tr>
                <tr>
                    <td><strong>Предиктивные</strong></td>
                    <td>Предсказание языковых явлений</td>
                    <td>N-граммные модели, классификаторы</td>
                    <td>Автодополнение, спам-фильтры</td>
                </tr>
                <tr>
                    <td><strong>Генеративные</strong></td>
                    <td>Порождение новых языковых единиц</td>
                    <td>Трансформеры, GPT, LLaMA</td>
                    <td>Чат-боты, генерация текстов</td>
                </tr>
                <tr>
                    <td><strong>Объяснительные</strong></td>
                    <td>Выявление каузальных связей</td>
                    <td>Когнитивные модели, интерпретируемые системы</td>
                    <td>XAI (объяснимый ИИ)</td>
                </tr>
            </table>
        </div>
        
        <div class="103-card">
            <h4>Роль формальных артефактов</h4>
            <p>Онтологии и грамматики выступают как <em>формальные артефакты</em> — структурированные представления знаний о языке, которые могут использоваться как самостоятельно, так и в качестве компонентов более сложных систем.</p>
            <p><strong>Функции формальных артефактов:</strong></p>
            <ul>
                <li><strong>Онтологии</strong> — структурируют концептуальное пространство предметной области</li>
                <li><strong>Грамматики</strong> — формализуют правила комбинаторики языковых единиц</li>
                <li><strong>Таксономии</strong> — организуют иерархические отношения между понятиями</li>
                <li><strong>Фреймы</strong> — описывают типовые ситуации и связанные с ними языковые средства</li>
            </ul>
        </div>
    </div>
    
    <div class="kmp12"><strong>Важно:</strong> В парадигме AI-First формальные артефакты не противопоставляются статистическим моделям, а интегрируются с ними. Нейросимволические системы (Neuro-Symbolic AI) сочетают преимущества обоих подходов.</div>
</section>

<section id="2" class="section">
    <h2 class="section-title">Вычислительное моделирование</h2>
    
    <div class="201">
        <h3 class="201-title">Рекурсивная аппроксимация и токенизация</h3>
        <div class="201-card">
            <h4>От континуума к дискретности</h4>
            <p>Естественный язык существует в континуальном пространстве: звуковой поток, рукописный текст, жестикуляция. Первой задачей вычислительной обработки является <em>дискретизация</em> — превращение непрерывного сигнала в последовательность отдельных единиц.</p>
            <p>Токенизация — это процесс сегментации текста на минимальные единицы обработки (токены). Выбор стратегии токенизации существенно влияет на качество модели.</p>
            <p><strong>Основные стратегии токенизации:</strong></p>
            <ul>
                <li><strong>Пословная</strong> — токеном является слово (проблема: OOV — out-of-vocabulary)</li>
                <li><strong>Посимвольная</strong> — токеном является символ (проблема: длинные последовательности)</li>
                <li><strong>Subword (BPE, WordPiece, Unigram)</strong> — токены выделяются статистически</li>
                <li><strong>Морфемная</strong> — токены соответствуют морфемам (требует лингвистической разметки)</li>
            </ul>
        </div>
        
        <div class="201-card">
            <h4>Векторизация: от дискретного к непрерывному</h4>
            <p>После токенизации каждому токену присваивается числовое представление — вектор в многомерном пространстве. Этот процесс называется <em>векторизацией</em> или <em>эмбеддингом</em> (embedding).</p>
            <p>Рекурсивная аппроксимация проявляется в том, что модель итеративно уточняет векторные представления, приближая их к распределениям, наблюдаемым в обучающих данных.</p>
            <p><strong>Эволюция методов векторизации:</strong></p>
            <ul>
                <li><strong>One-hot encoding</strong> — разреженные бинарные векторы (не учитывают семантику)</li>
                <li><strong>TF-IDF</strong> — взвешивание по частоте и значимости</li>
                <li><strong>Word2Vec, GloVe</strong> — плотные векторы на основе контекстов</li>
                <li><strong>Контекстуальные эмбеддинги (BERT, GPT)</strong> — векторы зависят от окружения</li>
            </ul>
        </div>
    </div>
    
    <div class="202">
        <h3 class="202-title">Дистрибутивная семантика</h3>
        <div class="202-card">
            <h4>Гипотеза Харриса и её развитие</h4>
            <p>Зелиг Харрис в 1954 году сформулировал дистрибутивную гипотезу: «Слова, встречающиеся в схожих контекстах, имеют схожие значения». Эта идея стала фундаментом всей современной компьютерной семантики.</p>
            <p>Дистрибутивная семантика переводит интуицию о значении слова в операциональную процедуру: значение = функция от контекстов употребления.</p>
            <p><strong>Формализация:</strong> Если слова <em>w₁</em> и <em>w₂</em> встречаются в схожих контекстах, то косинусное расстояние между их векторами будет мало: cos(v(w₁), v(w₂)) → 1.</p>
        </div>
        
        <div class="202-card">
            <h4>Динамическая реификация паттернов</h4>
            <p>Реификация (от лат. res — вещь) — процесс превращения абстрактного в конкретное, овеществления. В контексте модельной лингвистики это означает: семантико-синтаксические паттерны, существующие как статистические регулярности в корпусе, материализуются в виде числовых параметров модели.</p>
            <p>Модель «овеществляет» знание о языке, делая его доступным для вычислительных операций.</p>
            <p><strong>Примеры реификации:</strong></p>
            <ul>
                <li>Синтаксические конструкции → паттерны активации в слоях трансформера</li>
                <li>Семантические роли → направления в векторном пространстве</li>
                <li>Коллокации → высокие веса связей между токенами</li>
                <li>Стилистические особенности → области в латентном пространстве</li>
            </ul>
        </div>
    </div>
    
    <div class="203">
        <h3 class="203-title">Параметризованные преобразования</h3>
        <div class="203-card">
            <h4>Функция потерь и оптимизация</h4>
            <p>Обучение модели — это процесс подбора параметров (весов), минимизирующих функцию потерь (loss function). Функция потерь измеряет расхождение между предсказаниями модели и «правильными» ответами из обучающей выборки.</p>
            <p><strong>Основные функции потерь в NLP:</strong></p>
            <ul>
                <li><strong>Cross-entropy loss</strong> — для задач классификации и языкового моделирования</li>
                <li><strong>Contrastive loss</strong> — для обучения эмбеддингов</li>
                <li><strong>Perplexity</strong> — экспонента cross-entropy, мера «удивления» модели</li>
                <li><strong>BLEU, ROUGE</strong> — метрики для генерации текста</li>
            </ul>
        </div>
        
        <div class="203-card">
            <h4>Корпусные многообразия</h4>
            <p>Понятие многообразия (manifold) пришло из дифференциальной геометрии. В машинном обучении оно обозначает структуру данных в высокоразмерном пространстве. Гипотеза многообразия утверждает: реальные данные (включая тексты) занимают лишь малую часть возможного пространства и лежат вблизи многообразий меньшей размерности.</p>
            <p>Эмпирические распределения данных в корпусе формируют «ландшафт», по которому движется процесс оптимизации. Модель «учит» топографию этого ландшафта.</p>
        </div>
        
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr>
                        <th>Понятие</th>
                        <th>Определение</th>
                        <th>Роль в обучении</th>
                    </tr>
                </thead>
                <tr>
                    <td><strong>Веса (weights)</strong></td>
                    <td>Числовые параметры модели</td>
                    <td>Хранят «знания», извлечённые из данных</td>
                </tr>
                <tr>
                    <td><strong>Градиент</strong></td>
                    <td>Вектор частных производных функции потерь</td>
                    <td>Указывает направление улучшения модели</td>
                </tr>
                <tr>
                    <td><strong>Learning rate</strong></td>
                    <td>Шаг изменения весов</td>
                    <td>Контролирует скорость и стабильность обучения</td>
                </tr>
                <tr>
                    <td><strong>Эпоха</strong></td>
                    <td>Один проход по всему датасету</td>
                    <td>Единица измерения длительности обучения</td>
                </tr>
            </table>
        </div>
    </div>
    
    <div class="kmp14"><strong>Пояснение:</strong> Обучение нейросети можно представить как «спуск с горы» в многомерном пространстве, где высота соответствует значению функции потерь. Градиентный спуск — это итеративный процесс движения в направлении наибольшего уменьшения потерь.</div>
</section>

<section id="3" class="section">
    <h2 class="section-title">AI-First</h2>
	
	<div class="301-card">
            <h3>Парадигма AI-First в лингвистике</h3>
            <p>Парадигма AI-First предполагает, что системы искусственного интеллекта становятся первичным инструментом лингвистического анализа, а не вспомогательным средством. Это влечёт переосмысление роли лингвиста.</p>
            <p><strong>Принципы AI-First лингвистики:</strong></p>
            <ul>
                <li>Модель как отправная точка исследования, а не его финальный продукт</li>
                <li>Анализ через взаимодействие с моделью (probing, prompting)</li>
                <li>Приоритет масштабируемости и автоматизации</li>
                <li>Итеративное уточнение гипотез на основе поведения модели</li>
            </ul>
        </div>
    
    <div class="302">
        <h3 class="303-title">Эволюция лингвистического моделирования</h3>
        <div class="303-card">
            <h4>Историческая перспектива</h4>
            <p>История компьютерной лингвистики может быть прочитана как история смены парадигм моделирования. Каждый этап характеризовался доминированием определённого подхода к формализации языка.</p>
            <p><strong>Эпохи компьютерной лингвистики:</strong></p>
            <ul>
                <li><strong>1950–1970-е</strong> — символический период (формальные грамматики, логика)</li>
                <li><strong>1980-е</strong> — экспертные системы и инженерия знаний</li>
                <li><strong>1990-е</strong> — статистический поворот (скрытые марковские модели, N-граммы)</li>
                <li><strong>2010-е</strong> — глубокое обучение (RNN, LSTM, CNN для текста)</li>
                <li><strong>2017+</strong> — эра трансформеров и больших языковых моделей</li>
            </ul>
        </div>
        
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr>
                        <th>Характеристика</th>
                        <th>Rule-based системы</th>
                        <th>Нейросетевые архитектуры</th>
                    </tr>
                </thead>
                <tr>
                    <td><strong>Источник знаний</strong></td>
                    <td>Эксперт-лингвист</td>
                    <td>Данные (корпуса)</td>
                </tr>
                <tr>
                    <td><strong>Представление</strong></td>
                    <td>Явные правила</td>
                    <td>Распределённые параметры</td>
                </tr>
                <tr>
                    <td><strong>Интерпретируемость</strong></td>
                    <td>Высокая</td>
                    <td>Низкая («чёрный ящик»)</td>
                </tr>
                <tr>
                    <td><strong>Масштабируемость</strong></td>
                    <td>Ограниченная</td>
                    <td>Высокая</td>
                </tr>
                <tr>
                    <td><strong>Обработка исключений</strong></td>
                    <td>Требует ручного кодирования</td>
                    <td>Автоматически из данных</td>
                </tr>
                <tr>
                    <td><strong>Перенос на новые задачи</strong></td>
                    <td>Сложный</td>
                    <td>Fine-tuning, transfer learning</td>
                </tr>
            </table>
        </div>
    </div>
    
    <div class="303">
        <h3 class="303-title">Большие языковые модели (LLM)</h3>
        <div class="303-card">
            <h4>LLM как высшая форма модельной лингвистики</h4>
            <p>Большие языковые модели (Large Language Models) представляют собой кульминацию развития модельной лингвистики. Они демонстрируют, что достаточно большая модель, обученная на достаточно большом корпусе, способна имитировать широкий спектр языковых компетенций без явного программирования каждой из них.</p>
            <p><strong>Характеристики современных LLM:</strong></p>
            <ul>
                <li><strong>Масштаб</strong> — сотни миллиардов параметров</li>
                <li><strong>Универсальность</strong> — одна модель для множества задач</li>
                <li><strong>Эмерджентные способности</strong> — появление навыков, не заложенных явно</li>
                <li><strong>Генеративность</strong> — порождение связных текстов произвольной длины</li>
            </ul>
        </div>
        
        <div class="303-card">
            <h4>Механизм внимания (Attention)</h4>
            <p>Механизм внимания — ключевая инновация архитектуры Transformer (Vaswani et al., 2017). Он позволяет модели динамически «фокусироваться» на релевантных частях входной последовательности при обработке каждого элемента.</p>
            <p><strong>Формула scaled dot-product attention:</strong></p>
            <p style="font-family: monospace; background: #f5f5f5; padding: 10px; border-radius: 4px;">
            Attention(Q, K, V) = softmax(QK<sup>T</sup> / √d<sub>k</sub>) · V
            </p>
            <p>где Q (Query), K (Key), V (Value) — проекции входных векторов, d<sub>k</sub> — размерность ключей.</p>
            <p><strong>Лингвистическая интерпретация:</strong></p>
            <ul>
                <li>Query — «что я ищу?» (текущий контекст обработки)</li>
                <li>Key — «что я предлагаю?» (характеристика каждого элемента)</li>
                <li>Value — «что я передаю?» (информация, извлекаемая из элемента)</li>
            </ul>
            <p><strong>Пояснение:</strong> Механизм внимания можно рассматривать как вычислительную модель того, как контекст определяет интерпретацию языковой единицы — фундаментальный принцип прагматики и дискурсивной лингвистики.</p>
        </div>
        
        <div class="302-card">
            <h4>Архитектура Transformer</h4>
            <p>Трансформер состоит из стеков энкодеров и декодеров (или только декодеров в моделях типа GPT). Каждый слой включает механизм self-attention и полносвязную нейросеть (feed-forward network).</p>
            <p><strong>Компоненты трансформера:</strong></p>
            <ul>
                <li><strong>Positional encoding</strong> — инъекция информации о позиции токена</li>
                <li><strong>Multi-head attention</strong> — параллельные «головы» внимания для разных аспектов</li>
                <li><strong>Layer normalization</strong> — стабилизация обучения</li>
                <li><strong>Residual connections</strong> — обход слоёв для лучшего градиентного потока</li>
            </ul>
        </div>
    </div>
    
    <div class="303">
        <h3 class="303-title">Адаптивность и обучение в реальном времени</h3>
        <div class="303-card">
            <h4>In-context learning (ICL)</h4>
            <p>In-context learning — способность модели адаптироваться к новой задаче на основе примеров, предоставленных прямо в промпте, без изменения весов модели. Это явление — одно из наиболее интригующих свойств LLM.</p>
            <p><strong>Формы in-context learning:</strong></p>
            <ul>
                <li><strong>Zero-shot</strong> — только описание задачи, без примеров</li>
                <li><strong>One-shot</strong> — один пример решения</li>
                <li><strong>Few-shot</strong> — несколько примеров (обычно 3-10)</li>
            </ul>
            <p><strong>Пояснение:</strong> ICL демонстрирует, что LLM неявно «выучивают» мета-алгоритм решения задач, позволяющий обобщать на новые ситуации без дополнительного обучения.</p>
        </div>
    </div>
    
    <div class="kmp11"><strong>Примечание:</strong> Термин «AI-First» заимствован из бизнес-стратегии, где он означает приоритет ИИ-решений при проектировании продуктов. В академическом контексте он обозначает методологическую установку на использование ИИ как цента (ядра) исследовательского инструментария.</div>
</section>

<section id="4" class="section">
    <h2 class="section-title">Научная методология</h2>
    
    <div class="401">
        <h3 class="401-title">Жизненный цикл лингвистической модели</h3>
        <div class="401-card">
            <h4>Этапы создания модели</h4>
            <p>Разработка лингвистической модели — итеративный процесс, включающий теоретическую подготовку, инженерную реализацию и эмпирическую валидацию. Каждый этап требует специфических компетенций.</p>
            <p><strong>Фазы жизненного цикла:</strong></p>
            <ul>
                <li><strong>Концептуализация</strong> — определение задачи, целевых показателей и ограничений</li>
                <li><strong>Проектирование</strong> — выбор архитектуры, гиперпараметров, схемы обучения</li>
                <li><strong>Сбор данных</strong> — составление или адаптация корпуса</li>
                <li><strong>Предобработка</strong> — очистка, токенизация, аннотирование</li>
                <li><strong>Обучение (тренировка)</strong> — оптимизация параметров модели</li>
                <li><strong>Валидация</strong> — проверка на отложенных данных</li>
                <li><strong>Тестирование</strong> — финальная оценка качества</li>
                <li><strong>Развёртывание</strong> — интеграция в прикладные системы</li>
                <li><strong>Мониторинг</strong> — отслеживание качества в эксплуатации</li>
            </ul>
        </div>
        
        <div class="401-card">
            <h4>Требования к корпусу</h4>
            <p>Качество модели критически зависит от качества обучающих данных. Принцип «garbage in — garbage out» в NLP проявляется особенно остро.</p>
            <p><strong>Характеристики качественного корпуса:</strong></p>
            <ul>
                <li><strong>Репрезентативность</strong> — отражение целевой области применения</li>
                <li><strong>Сбалансированность</strong> — пропорциональное представление классов/жанров</li>
                <li><strong>Чистота</strong> — отсутствие артефактов, ошибок, нерелевантных данных</li>
                <li><strong>Размер</strong> — достаточный объём для обучения выбранной архитектуры</li>
                <li><strong>Документированность</strong> — метаданные о составе, происхождении, обработке</li>
            </ul>
        </div>
    </div>
    
    <div class="402">
        <h3 class="402-title">Метрики и валидация</h3>
        <div class="402-card">
            <h4>Измерение качества модели</h4>
            <p>Вопрос «Насколько хороша модель?» требует операционализации через количественные метрики. Выбор метрики зависит от типа задачи и критериев успешности.</p>
        </div>
        
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr>
                        <th>Задача</th>
                        <th>Метрики</th>
                        <th>Интерпретация</th>
                    </tr>
                </thead>
                <tr>
                    <td><strong>Языковое моделирование</strong></td>
                    <td>Perplexity</td>
                    <td>Чем ниже, тем лучше модель предсказывает текст</td>
                </tr>
                <tr>
                    <td><strong>Классификация</strong></td>
                    <td>Accuracy, Precision, Recall, F1</td>
                    <td>Баланс между полнотой и точностью</td>
                </tr>
                <tr>
                    <td><strong>Машинный перевод</strong></td>
                    <td>BLEU, METEOR, COMET</td>
                    <td>Сходство с эталонными переводами</td>
                </tr>
                <tr>
                    <td><strong>Суммаризация</strong></td>
                    <td>ROUGE</td>
                    <td>Перекрытие N-грамм с эталоном</td>
                </tr>
                <tr>
                    <td><strong>Генерация</strong></td>
                    <td>Human evaluation, GPT-eval</td>
                    <td>Субъективная оценка качества</td>
                </tr>
                <tr>
                    <td><strong>Вопросно-ответные системы</strong></td>
                    <td>Exact Match, F1 (token-level)</td>
                    <td>Совпадение с правильным ответом</td>
                </tr>
            </table>
        </div>
        
        <div class="402-card">
            <h4>Перплексия: математика и интуиция</h4>
            <p>Перплексия (perplexity) — ключевая метрика для оценки языковых моделей. Она измеряет, насколько модель «удивлена» тестовыми данными.</p>
            <p><strong>Формула:</strong></p>
            <p style="font-family: monospace; background: #f5f5f5; padding: 10px; border-radius: 4px;">
            PPL = exp(-1/N · Σ log P(w<sub>i</sub> | w<sub>1</sub>...w<sub>i-1</sub>))
            </p>
            <p><strong>Интуиция:</strong> Перплексия показывает «эффективный размер словаря» — сколько равновероятных вариантов следующего слова в среднем рассматривает модель. Перплексия 10 означает: модель в среднем «выбирает» из 10 равновероятных продолжений.</p>
        </div>
        
        <div class="402-card">
            <h4>Проблема интерпретируемости</h4>
            <p>Современные LLM часто называют «чёрными ящиками»: они выдают результаты, но не объясняют их. Для научной лингвистики это серьёзная проблема.</p>
            <p><strong>Методы повышения интерпретируемости:</strong></p>
            <ul>
                <li><strong>Probing classifiers</strong> — анализ, какую информацию кодируют скрытые слои</li>
                <li><strong>Attention visualization</strong> — визуализация паттернов внимания</li>
                <li><strong>Feature attribution</strong> — определение влияния входных признаков</li>
                <li><strong>Mechanistic interpretability</strong> — выявление «схем» внутри модели</li>
            </ul>
        </div>
    </div>
    
    <div class="403">
        <h3 class="403-title">Агентное моделирование и симуляции</h3>
        <div class="403-card">
            <h4>Язык в искусственных сообществах</h4>
            <p>Агентное моделирование (Agent-Based Modeling) — методология изучения сложных систем через симуляцию взаимодействия автономных агентов. В лингвистике этот подход позволяет исследовать эмергентные свойства языка.</p>
            <p><strong>Направления агентного моделирования языка:</strong></p>
            <ul>
                <li><strong>Эволюция языка</strong> — как возникают языковые конвенции?</li>
                <li><strong>Языковые изменения</strong> — механизмы распространения инноваций</li>
                <li><strong>Многоагентная коммуникация</strong> — эмергентные протоколы в AI-системах</li>
                <li><strong>Социолингвистические симуляции</strong> — моделирование диалектного варьирования</li>
            </ul>
        </div>
        
        <div class="403-card">
            <h4>Языковые игры ИИ-агентов</h4>
            <p>Современные исследования показывают, что ИИ-агенты, поставленные в ситуацию коммуникативного взаимодействия, способны спонтанно развивать протоязыковые системы.</p>
            <p><strong>Классические эксперименты:</strong></p>
            <ul>
                <li>OpenAI — агенты, развивающие «язык» для координации</li>
                <li>Facebook AI — эмерджентная коммуникация в играх с референцией</li>
                <li>DeepMind — обучение коммуникации в многоагентных средах</li>
            </ul>
            <p><strong>Пояснение:</strong> Эти эксперименты имеют непосредственное отношение к теоретической лингвистике: они позволяют тестировать гипотезы о происхождении и эволюции языка в контролируемых условиях.</p>
        </div>
    </div>
    
    <div class="kmp12"><strong>Важно:</strong> Валидация модели должна включать не только автоматические метрики, но и экспертную лингвистическую оценку, анализ ошибок и тестирование на «adversarial examples» — специально сконструированных сложных случаях.</div>
</section>

<section id="5" class="section">
    <h2 class="section-title">Прикладные аспекты</h2>
    
    <div class="501">
        <h3 class="501-title">Digital Humanities и культуромика</h3>
        <div class="501-card">
            <h4>Лингвистическое моделирование для гуманитарных наук</h4>
            <p>Digital Humanities (цифровые гуманитарные науки) — междисциплинарная область, применяющая вычислительные методы к гуманитарным материалам. Лингвистические модели играют в ней центральную роль.</p>
            <p><strong>Области применения:</strong></p>
            <ul>
                <li><strong>Историческая лингвистика</strong> — реконструкция праязыков, датировка текстов</li>
                <li><strong>Стилометрия</strong> — атрибуция авторства, выявление стилистических паттернов</li>
                <li><strong>Дискурс-анализ</strong> — автоматическое выявление тематик и идеологий</li>
                <li><strong>Культуромика (culturomics)</strong> — анализ культурных трендов через большие текстовые корпуса</li>
            </ul>
        </div>
        
        <div class="501-card">
            <h4>Анализ исторических текстов</h4>
            <p>Работа с историческими текстами создаёт особые вызовы для NLP: орфографическая вариативность, устаревшая лексика, отсутствие стандартизации.</p>
            <p><strong>Специфические задачи:</strong></p>
            <ul>
                <li><strong>OCR коррекция</strong> — исправление ошибок распознавания</li>
                <li><strong>Нормализация орфографии</strong> — приведение к современным стандартам</li>
                <li><strong>Историческое NER</strong> — распознавание исторических имён и топонимов</li>
                <li><strong>Семантический сдвиг</strong> — отслеживание изменения значений слов</li>
            </ul>
            <p><strong>Пример:</strong> Google Ngram Viewer позволяет отслеживать частотность слов и фраз в книгах за несколько столетий, выявляя культурные и лингвистические тренды.</p>
        </div>
    </div>
    
    <div class="502">
        <h3 class="502-title">Этика и предвзятость (Bias)</h3>
        <div class="502-card">
            <h4>Источники предвзятости в языковых моделях</h4>
            <p>Языковые модели обучаются на текстах, созданных людьми, и неизбежно воспроизводят социальные предубеждения, содержащиеся в этих текстах. Bias в NLP — это систематическое искажение, приводящее к дискриминационным результатам.</p>
            <p><strong>Типы bias в NLP:</strong></p>
            <ul>
                <li><strong>Гендерный</strong> — ассоциация профессий с полом («doctor» → he)</li>
                <li><strong>Расовый/этнический</strong> — стереотипы о группах населения</li>
                <li><strong>Социоэкономический</strong> — недопредставленность маргинализированных групп</li>
                <li><strong>Лингвистический</strong> — дискриминация нестандартных вариантов языка</li>
                <li><strong>Географический</strong> — англоцентризм большинства моделей</li>
            </ul>
        </div>
        
        <div class="502-card">
            <h4>Механизмы воспроизводства bias</h4>
            <p>Bias проникает в модели на разных этапах: от сбора данных до развёртывания системы. Понимание этих механизмов — первый шаг к их митигации.</p>
            <p><strong>Этапы, где возникает bias:</strong></p>
            <ul>
                <li><strong>Корпус</strong> — нерепрезентативность, исторические предубеждения</li>
                <li><strong>Аннотация</strong> — субъективность аннотаторов</li>
                <li><strong>Обучение</strong> — усиление корреляций из данных</li>
                <li><strong>Оценка</strong> — неадекватные бенчмарки</li>
                <li><strong>Развёртывание</strong> — использование в контекстах с разным распределением</li>
            </ul>
        </div>
        
        <div class="502-card">
            <h4>Стратегии деbiasing</h4>
            <p>Полное устранение bias невозможно, однако существуют методы его обнаружения и смягчения.</p>
            <p><strong>Подходы к митигации bias:</strong></p>
            <ul>
                <li><strong>Data augmentation</strong> — расширение данных для балансировки</li>
                <li><strong>Counterfactual debiasing</strong> — обучение на контрфактических примерах</li>
                <li><strong>Adversarial debiasing</strong> — состязательное обучение против bias</li>
                <li><strong>Post-hoc correction</strong> — коррекция выходов модели</li>
                <li><strong>Audit и мониторинг</strong> — систематическая проверка на fairness</li>
            </ul>
        </div>
        
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr>
                        <th>Этический принцип</th>
                        <th>Применение в NLP</th>
                        <th>Вопросы для рефлексии</th>
                    </tr>
                </thead>
                <tr>
                    <td><strong>Справедливость (Fairness)</strong></td>
                    <td>Равное качество для разных групп пользователей</td>
                    <td>Для кого модель работает хуже?</td>
                </tr>
                <tr>
                    <td><strong>Прозрачность (Transparency)</strong></td>
                    <td>Объяснимость решений модели</td>
                    <td>Может ли пользователь понять, почему модель дала такой ответ?</td>
                </tr>
                <tr>
                    <td><strong>Подотчётность (Accountability)</strong></td>
                    <td>Ответственность за ошибки модели</td>
                    <td>Кто несёт ответственность за вред, причинённый моделью?</td>
                </tr>
                <tr>
                    <td><strong>Приватность (Privacy)</strong></td>
                    <td>Защита данных в корпусах и при инференсе</td>
                    <td>Какие персональные данные модель «запоминает»?</td>
                </tr>
            </table>
        </div>
    </div>
    
    <div class="503">
        <h3 class="503-title">Лингвист в AI-First среде</h3>
        <div class="503-card">
            <h4>Трансформация профессии</h4>
            <p>Парадигма AI-First не устраняет потребность в лингвистах, но радикально меняет содержание их работы. Лингвист становится не столько «описателем языка», сколько <em>архитектором смыслов</em> и <em>куратором моделей</em>.</p>
            <p><strong>Новые роли лингвиста:</strong></p>
            <ul>
                <li><strong>Prompt engineer</strong> — проектирование эффективных промптов</li>
                <li><strong>Data curator</strong> — отбор и подготовка обучающих данных</li>
                <li><strong>Evaluation specialist</strong> — разработка методов оценки качества</li>
                <li><strong>Bias auditor</strong> — выявление и анализ предвзятостей</li>
                <li><strong>Domain expert</strong> — адаптация моделей к специфическим областям</li>
                <li><strong>AI trainer</strong> — обучение моделей через RLHF (reinforcement learning from human feedback)</li>
            </ul>
        </div>
        
        <div class="503-card">
            <h4>Компетенции лингвиста будущего</h4>
            <p>Эффективная работа в AI-First среде требует сочетания традиционных лингвистических знаний с новыми техническими и методологическими компетенциями.</p>
            <p><strong>Ключевые компетенции:</strong></p>
            <ul>
                <li><strong>Программирование</strong> — Python, работа с NLP-библиотеками</li>
                <li><strong>Статистика и ML</strong> — понимание принципов машинного обучения</li>
                <li><strong>Корпусная лингвистика</strong> — работа с большими текстовыми данными</li>
                <li><strong>Критическое мышление</strong> — оценка ограничений и рисков моделей</li>
                <li><strong>Междисциплинарность</strong> — коммуникация со специалистами из других областей</li>
                <li><strong>Этическая рефлексия</strong> — осознание социальных последствий технологий</li>
            </ul>
        </div>
        
        <div class="503-card">
            <h4>Лингвист как архитектор смыслов</h4>
            <p>В мире, где машины генерируют тексты, человеческая экспертиза в области смысла становится особенно ценной. Лингвист понимает нюансы, которые модель может упустить: культурный контекст, коммуникативные намерения, имплицитные значения.</p>
            <p><strong>Функции «архитектора смыслов»:</strong></p>
            <ul>
                <li>Постановка задач для моделей в терминах коммуникативных целей</li>
                <li>Интерпретация результатов работы моделей</li>
                <li>Мосты между машинным и человеческим «пониманием»</li>
                <li>Проектирование систем взаимодействия человек-ИИ</li>
            </ul>
        </div>
    </div>
    
    <div class="kmp11"><strong>Примечание:</strong> Модельная лингвистика в парадигме AI-First — это не отказ от традиционной лингвистики, а её эволюция. Теоретические знания о языке остаются необходимыми для понимания того, что делают модели, и для проектирования более совершенных систем.</div>
    
    <div class="kmp12"><strong>Важно:</strong> Этическая ответственность лингвиста в AI-First среде возрастает: модели, разработанные с его участием, влияют на миллионы пользователей. Осознание этой ответственности — неотъемлемая часть профессиональной компетенции.</div>
</section>

<section id="6" class="section">
    <h2 class="section-title">Интеграция перспектив</h2>
    
    <div class="601">
        <h3 class="601-title">Ключевые тезисы модельной лингвистики</h3>
        <div class="601-card">
            <h4>Синтез подходов</h4>
            <p>Модельная лингвистика в эпоху AI-First представляет собой синтез теоретической лингвистики, компьютерных наук и философии науки. Её центральный тезис: язык наиболее продуктивно изучать через построение работающих моделей.</p>
            <p><strong>Фундаментальные положения:</strong></p>
            <ul>
                <li>Язык — динамическая, вероятностная, контекстно-зависимая система</li>
                <li>Модель — не «истина», а операциональный инструмент с измеримым качеством</li>
                <li>Большие языковые модели — эмпирическое доказательство мощи дистрибутивного подхода</li>
                <li>Лингвист будущего — куратор моделей и архитектор смыслов</li>
                <li>Этическая рефлексия — неотъемлемая часть лингвистической практики</li>
            </ul>
        </div>
    </div>
    
    <div class="kmp14"><strong>Пояснение:</strong> Данный материал представляет собой концептуальную рамку для изучения языка в контексте современных технологий искусственного интеллекта. Он не заменяет традиционное лингвистическое понимание, а дополняет его, открывая новые перспективы исследования и применения лингвистических знаний.</div>
</section>

<footer class="footer">
<div class="container">
<p>© 2025 | kmp | CC BY-NC-SA 4.0<br>
Разработано для студентов БрГУ имени А.С. Пушкина</p>
</div>
</footer>
<div style="position: fixed; bottom: 10px; color: #777777; right: 30px; opacity: 0.3; font-size: 14px;">kmp+</div>
    <script>
        // Функция для плавной прокрутки к разделу
        function scrollToSection(sectionId) {
            const section = document.getElementById(sectionId);
            const menuHeight = document.querySelector('.menu').offsetHeight;
            
            window.scrollTo({
                top: section.offsetTop - menuHeight - 20,
                behavior: 'smooth'
            });
        }

        // Функция для переключения темы
        document.getElementById('themeToggle').addEventListener('click', function() {
            const currentTheme = document.documentElement.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            
            document.documentElement.setAttribute('data-theme', newTheme);
            this.textContent = newTheme === 'dark' ? '🌙' : '☀️';
        });

        // Анимация появления секций при прокрутке
        document.addEventListener('DOMContentLoaded', function() {
            const sections = document.querySelectorAll('.section');
            
            const observerOptions = {
                root: null,
                rootMargin: '0px',
                threshold: 0.1
            };
            
            const observer = new IntersectionObserver(function(entries, observer) {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('fade-in');
                        observer.unobserve(entry.target);
                    }
                });
            }, observerOptions);
            
            sections.forEach(section => {
                section.classList.remove('fade-in');
                observer.observe(section);
            });
        });
    </script>
</body>
</html>