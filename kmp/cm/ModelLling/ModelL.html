<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>kmp+</title>
    <style>
        /* Основные стили */
        :root {
            --primary-color: #325980;
            --secondary-color: #4CAF50;
            --background-color: #f5f5f5;
            --content-bg: #ffffff;
            --text-color: #333333;
            --header-text-color: #ffffff;
            --menu-bg: #ffffff;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            --border-radius: 8px;
			  --accent11: #4caf50;
		--accent12: #4cafff;
		--accent13: #ffaf50;
		--accent14: #821978;
        }

        /* Темная тема */
        [data-theme="dark"] {
            --primary-color: #3e76ad;
            --secondary-color: #388e3c;
            --background-color: #000000;
            --content-bg: #1e1e1e;
            --text-color: #e0e0e0;
            --header-text-color: #ffffff;
            --menu-bg: #000000;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            transition: background-color 0.3s, color 0.3s;
        }

        body {
            font-family: 'Roboto', 'Arial', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--background-color);
            padding-top: 2px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        header {
            background-color: var(--primary-color);
            color: var(--header-text-color);
            padding: 20px 0;
            text-align: center;
            border-radius: var(--border-radius);
            margin-bottom: 2px;
        }

        h1 {
            font-size: 2.2rem;
            margin-bottom: 10px;
        }

        h2 {
            color: var(--primary-color);
            margin: 25px 0 15px;
            padding-bottom: 8px;
            border-bottom: 2px solid var(--secondary-color);
        }

        h3 {
            color: var(--primary-color);
            margin: 20px 0 10px;
        }

        p {
            margin-bottom: 15px;
        }

        /* Меню навигации */
        .menu {
            background-color: var(--menu-bg);
            padding: 15px 20px;
            border-radius: var(--border-radius);
            margin-bottom: 30px;
            box-shadow: var(--menu-shadow);
            display: flex;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .menu-buttons {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }
		
		
		.menu-buttons {
    display: flex;
    flex-wrap: wrap;
    gap: 10px;
    justify-content: center; /* Изменено на center */
}

        .menu-btn {
            background-color: var(--primary-color);
            color: white;
            border: none;
            padding: 8px 15px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: background-color 0.3s;
        }

        .menu-btn:hover {
            background-color: var(--secondary-color);
        }

        .theme-toggle {
            background: none;
            border: 10px solid transparent;
            font-size: 1.5rem;
            cursor: pointer;
            color: var(--primary-color);
        }

        /* Секции контента */
        .section {
            background-color: var(--content-bg);
            border-radius: var(--border-radius);
            padding: 25px;
            margin-bottom: 30px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        /* Выделение важного */
        .important {
            background-color: rgba(76, 175, 80, 0.1);
            border-left: 4px solid var(--secondary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        .note {
            background-color: rgba(50, 89, 128, 0.1);
            border-left: 4px solid var(--primary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        /* Таблицы */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            padding: 12px 15px;
            border: 1px solid #ddd;
            text-align: left;
        }

        th {
            background-color: var(--primary-color);
            color: white;
        }

        tr:nth-child(even) {
            background-color: rgba(0, 0, 0, 0.03);
        }

        /* Списки */
        ul, ol {
            padding-left: 25px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
        }

        /* Адаптивный дизайн */
        @media (max-width: 768px) {
            h1 {
                font-size: 1.8rem;
            }
            
            h2 {
                font-size: 1.5rem;
            }
            
            .menu {
                flex-direction: column;
                gap: 15px;
            }
            
            .menu-buttons {
                width: 100%;
                justify-content: center;
            }
            
            .theme-toggle {
                margin-top: 10px;
            }
            
            .section {
                padding: 15px;
            }
        }

        /* Анимации */
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .fade-in {
            animation: fadeIn 0.5s ease-in;
        }

        footer {
            text-align: center;
            padding: 20px 0;
            margin-top: 40px;
            font-size: 0.9rem;
        }
		
					.kmp11, .example {
      background: rgba(76, 175, 80, 0.1);
      border-left: 4px solid var(--accent11);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp12, .example {
      background: rgba(95, 182, 237, 0.1);
      border-left: 4px solid var(--accent12);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
 
		.kmp13, .example {
      background: rgba(205, 170, 110, 0.1);
      border-left: 4px solid var(--accent13);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp14, .example {
      background: rgba(205, 110, 200, 0.1);
      border-left: 4px solid var(--accent14);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
	
	
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Модельная лингвистика</h1>
            <p>методы и предметы моделирования в историческом развитии</p>
        </header>

        <nav class="menu">
            <div class="menu-buttons">
                <button class="menu-btn" onclick="scrollToSection('1')">Моделирование</button>
                <button class="menu-btn" onclick="scrollToSection('2')">Формальное</button>
                <button class="menu-btn" onclick="scrollToSection('3')">Вероятностное</button>
                <button class="menu-btn" onclick="scrollToSection('4')">Корпусное</button>
				<button class="menu-btn" onclick="scrollToSection('5')">Синтаксическое</button>
				<button class="menu-btn" onclick="scrollToSection('6')">Семантическое</button>
				<button class="menu-btn" onclick="scrollToSection('7')">Прагматическое</button>
                                    </div>
            <button class="theme-toggle" id="themeToggle" title="Переключить тему">☀️</button>
        </nav>


<section id="1" class="section">
    <h2 class="section-title">Введение в модельную лингвистику</h2>
    
    <div class="101">
        <h3 class="101-title">Модель и моделирование</h3>
        <div class="101-card">
        <p><strong>Модель</strong> — упрощённое представление объекта, явления или процесса, сохраняющее существенные для исследования свойства оригинала. В лингвистике модель описывает структуру языка, механизмы порождения и понимания речи.</p>
            <p><strong>Моделирование</strong> — процесс построения, изучения и применения моделей для анализа, прогнозирования или имитации языковых явлений.</p>
            <p><strong>Ключевые характеристики модели:</strong></p>
            <ul>
                <li>Адекватность — соответствие моделируемому объекту</li>
                <li>Полнота — охват существенных свойств</li>
                <li>Простота — минимизация избыточных элементов</li>
                <li>Продуктивность — способность порождать новые знания</li>
            </ul>
        </div>
        
        <div class="101-card">
            <h4>Формализация и абстрагирование</h4>
            <p><strong>Формализация</strong> — представление языковых явлений в виде строгой системы символов, правил и отношений. Позволяет применять математические и вычислительные методы к лингвистическим данным.</p>
            <p><strong>Абстрагирование</strong> — выделение существенных признаков объекта при отвлечении от несущественных. В лингвистике: переход от конкретных высказываний к языковым структурам и правилам.</p>
            <p><strong>Уровни абстракции в лингвистике:</strong></p>
            <ul>
                <li>Конкретные речевые акты → типы высказываний</li>
                <li>Словоформы → лексемы → части речи</li>
                <li>Конкретные предложения → синтаксические структуры</li>
            </ul>
            <p><strong>Пояснение:</strong> Формализация делает интуитивные лингвистические знания эксплицитными, проверяемыми и реализуемыми в компьютерных системах.</p>
        </div>
    </div>
    
    <div class="102">
        <h3 class="102-title">История и становление модельной лингвистики</h3>
        <div class="102-card">
            <h4>Этапы развития</h4>
            <p>Модельная лингвистика формировалась на пересечении лингвистики, математики и информатики. Её развитие тесно связано с появлением компьютеров и потребностью в автоматической обработке языка.</p>
            <p><strong>Ключевые этапы:</strong></p>
            <ul>
                <li><strong>1950-е годы:</strong> работы Н. Хомского по порождающим грамматикам; первые опыты машинного перевода</li>
                <li><strong>1960–70-е:</strong> развитие формальных грамматик, создание первых парсеров и морфологических анализаторов</li>
                <li><strong>1980–90-е:</strong> расцвет статистических методов, корпусной лингвистики, скрытых марковских моделей</li>
                <li><strong>2000-е:</strong> машинное обучение, word embeddings, начало эры нейросетей</li>
                <li><strong>2010-е — настоящее время:</strong> глубокое обучение, трансформеры, большие языковые модели (LLM)</li>
            </ul>
        </div>
        
        <div class="102-card">
            <h4>Связь с математической и вычислительной лингвистикой</h4>
            <p><strong>Математическая лингвистика</strong> — область, изучающая язык с помощью математических методов: теории множеств, алгебры, логики, теории вероятностей. Фокус на теоретических моделях.</p>
            <p><strong>Вычислительная лингвистика (Computational Linguistics)</strong> — междисциплинарная область, направленная на создание компьютерных систем обработки естественного языка (NLP). Фокус на практических приложениях.</p>
            <p><strong>Модельная лингвистика</strong> занимает промежуточное положение: использует математический аппарат для построения моделей, которые реализуются и проверяются вычислительными средствами.</p>
        </div>
        
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr>
                        <th>Направление</th>
                        <th>Основной фокус</th>
                        <th>Типичные задачи</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Математическая лингвистика</td>
                        <td>Формальные структуры языка</td>
                        <td>Доказательство свойств грамматик, теоремы о разрешимости</td>
                    </tr>
                    <tr>
                        <td>Вычислительная лингвистика</td>
                        <td>Алгоритмы и системы NLP</td>
                        <td>Машинный перевод, информационный поиск, диалоговые системы</td>
                    </tr>
                    <tr>
                        <td>Модельная лингвистика</td>
                        <td>Моделирование языковых явлений</td>
                        <td>Построение и валидация моделей грамматики, семантики, прагматики</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
    
    <div class="103">
        <h3 class="103-title">Классификация моделей</h3>
        <div class="103-card">
            <h4>Формальные vs эмпирические модели</h4>
            <p><strong>Формальные модели</strong> строятся дедуктивно на основе теоретических постулатов. Правила задаются априорно, модель порождает или анализирует языковые структуры согласно этим правилам.</p>
            <p><strong>Примеры:</strong> порождающие грамматики Хомского, логические исчисления для семантики.</p>
            <p><strong>Эмпирические модели</strong> строятся индуктивно на основе наблюдаемых данных (корпусов). Параметры модели извлекаются из реальных текстов.</p>
            <p><strong>Примеры:</strong> статистические языковые модели, word embeddings, обученные на корпусах.</p>
        </div>
        
        <div class="103-card">
            <h4>Детерминистские vs стохастические модели</h4>
            <p><strong>Детерминистские модели</strong> дают однозначный результат при заданных входных данных. Для одного входа всегда получается один и тот же выход.</p>
            <p><strong>Пример:</strong> контекстно-свободная грамматика, возвращающая все допустимые разборы предложения.</p>
            <p><strong>Стохастические (вероятностные) модели</strong> оперируют вероятностями и могут давать разные результаты или ранжировать альтернативы по вероятности.</p>
            <p><strong>Пример:</strong> вероятностная грамматика (PCFG), присваивающая каждому разбору вероятность.</p>
        </div>
        
        <div class="103-card">
            <h4>Символические vs субсимволические модели</h4>
            <p><strong>Символические модели</strong> работают с дискретными символами и явно заданными правилами. Знания представлены в интерпретируемом виде (правила, графы, логические формулы).</p>
            <p><strong>Примеры:</strong> конечные автоматы, формальные грамматики, семантические сети.</p>
            <p><strong>Субсимволические модели</strong> оперируют числовыми векторами и матрицами. Знания распределены по параметрам модели и часто не интерпретируемы напрямую.</p>
            <p><strong>Примеры:</strong> нейронные сети, word embeddings, трансформеры.</p>
        </div>
        
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr>
                        <th>Критерий</th>
                        <th>Тип 1</th>
                        <th>Тип 2</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Источник знаний</td>
                        <td>Формальные (теория)</td>
                        <td>Эмпирические (данные)</td>
                    </tr>
                    <tr>
                        <td>Характер результата</td>
                        <td>Детерминистские (однозначные)</td>
                        <td>Стохастические (вероятностные)</td>
                    </tr>
                    <tr>
                        <td>Представление знаний</td>
                        <td>Символические (правила)</td>
                        <td>Субсимволические (векторы)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="kmp12"><strong>Важно:</strong> Современные системы NLP часто являются гибридными, сочетая элементы разных типов моделей. Например, нейросетевой парсер (субсимволический) может использовать формальную грамматику для ограничения пространства решений.</div>
    </div>
</section>


<section id="2" class="section">
    <h2 class="section-title">Формальное моделирование</h2>
    
    <div class="201">
        <h3 class="201-title">Формальные грамматики и формальные языки</h3>
        <div class="201-card">
            <h4>Понятие формальной грамматики</h4>
            <p>Формальная грамматика — математическая система, определяющая множество цепочек символов (язык) посредством конечного набора правил.</p>
            <p><strong>Формальное определение:</strong> Грамматика G = (N, Σ, P, S), где:</p>
            <ul>
                <li><strong>N</strong> — конечное множество нетерминальных символов</li>
                <li><strong>Σ</strong> — конечное множество терминальных символов (алфавит)</li>
                <li><strong>P</strong> — конечное множество правил вывода (продукций)</li>
                <li><strong>S</strong> — начальный символ (S ∈ N)</li>
            </ul>
            <p><strong>Пояснение:</strong> Грамматика порождает язык L(G) — множество всех терминальных цепочек, выводимых из начального символа S.</p>
        </div>
        
        <div class="201-card">
            <h4>Иерархия Хомского</h4>
            <p>Ноам Хомский (1956) предложил классификацию формальных грамматик по ограничениям на вид правил:</p>
            <p><strong>Тип 3 — Регулярные грамматики:</strong></p>
            <ul>
                <li>Правила вида: A → aB или A → a (правосторонние) либо A → Ba или A → a (левосторонние)</li>
                <li>Порождают регулярные языки</li>
                <li>Распознаются конечными автоматами</li>
                <li><strong>Применение в лингвистике:</strong> морфология (модели словоизменения), простейшие паттерны</li>
            </ul>
            <p><strong>Тип 2 — Контекстно-свободные грамматики (КС, CFG):</strong></p>
            <ul>
                <li>Правила вида: A → γ, где A ∈ N, γ ∈ (N ∪ Σ)*</li>
                <li>Порождают контекстно-свободные языки</li>
                <li>Распознаются стековыми (магазинными) автоматами</li>
                <li><strong>Применение:</strong> синтаксис естественных языков, грамматики программирования</li>
            </ul>
            <p><strong>Тип 1 — Контекстно-зависимые грамматики (КЗ, CSG):</strong></p>
            <ul>
                <li>Правила вида: αAβ → αγβ, где |γ| ≥ 1</li>
                <li>Замена зависит от контекста (окружающих символов)</li>
                <li>Распознаются линейно ограниченными автоматами</li>
                <li><strong>Применение:</strong> моделирование согласования, дальних зависимостей</li>
            </ul>
            <p><strong>Тип 0 — Неограниченные грамматики:</strong></p>
            <ul>
                <li>Любые правила вида: α → β</li>
                <li>Эквивалентны машинам Тьюринга</li>
                <li>На практике в лингвистике почти не используются</li>
            </ul>
        </div>
        
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr>
                        <th>Тип</th>
                        <th>Название</th>
                        <th>Распознаватель</th>
                        <th>Лингвистическое применение</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>3</td>
                        <td>Регулярные</td>
                        <td>Конечный автомат</td>
                        <td>Морфология, токенизация</td>
                    </tr>
                    <tr>
                        <td>2</td>
                        <td>Контекстно-свободные</td>
                        <td>Стековый автомат</td>
                        <td>Синтаксис (основа)</td>
                    </tr>
                    <tr>
                        <td>1</td>
                        <td>Контекстно-зависимые</td>
                        <td>Линейно ограниченный автомат</td>
                        <td>Согласование, scrambling</td>
                    </tr>
                    <tr>
                        <td>0</td>
                        <td>Неограниченные</td>
                        <td>Машина Тьюринга</td>
                        <td>Теоретический интерес</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="kmp11"><strong>Примечание:</strong> Естественные языки не являются строго контекстно-свободными: явления вроде швейцарского немецкого cross-serial dependencies требуют более мощных формализмов (mildly context-sensitive languages).</div>
    </div>
    
    <div class="202">
        <h3 class="202-title">Автоматы</h3>
        <div class="202-card">
            <h4>Конечные автоматы (Finite Automata)</h4>
            <p>Конечный автомат (КА) — абстрактная вычислительная модель с конечным числом состояний, переходами между ними по входным символам.</p>
            <p><strong>Формальное определение:</strong> КА = (Q, Σ, δ, q₀, F), где:</p>
            <ul>
                <li><strong>Q</strong> — конечное множество состояний</li>
                <li><strong>Σ</strong> — входной алфавит</li>
                <li><strong>δ</strong> — функция переходов: Q × Σ → Q (ДКА) или Q × Σ → 2^Q (НКА)</li>
                <li><strong>q₀</strong> — начальное состояние</li>
                <li><strong>F</strong> — множество допускающих (финальных) состояний</li>
            </ul>
            <p><strong>Применение в лингвистике:</strong></p>
            <ul>
                <li>Морфологический анализ и синтез</li>
                <li>Распознавание паттернов в тексте</li>
                <li>Модели произношения в системах распознавания речи</li>
            </ul>
        </div>
        
        <div class="202-card">
            <h4>Стековые (магазинные) автоматы (Pushdown Automata)</h4>
            <p>Стековый автомат — конечный автомат, дополненный бесконечным стеком (магазином). Может распознавать контекстно-свободные языки.</p>
            <p><strong>Ключевое отличие:</strong> переходы зависят не только от текущего состояния и входного символа, но и от символа на вершине стека. Автомат может добавлять и удалять символы из стека.</p>
            <p><strong>Применение:</strong> синтаксический анализ вложенных структур (скобки, придаточные предложения).</p>
        </div>
        
        <div class="202-card">
            <h4>Трансдукторы (Finite-State Transducers)</h4>
            <p>Трансдуктор — конечный автомат с двумя лентами: входной и выходной. Осуществляет преобразование входной цепочки в выходную.</p>
            <p><strong>Формально:</strong> переходы имеют вид (q, a, b, q'), где a — входной символ, b — выходной символ.</p>
            <p><strong>Применение в лингвистике:</strong></p>
            <ul>
                <li>Морфологический анализ: словоформа → лемма + граммемы</li>
                <li>Транслитерация и нормализация текста</li>
                <li>Фонологические правила (преобразование фонем)</li>
            </ul>
            <p><strong>Пояснение:</strong> Композиция трансдукторов позволяет строить сложные конвейеры обработки текста, где выход одного трансдуктора подаётся на вход другого.</p>
        </div>
        
        <div class="kmp12"><strong>Важно:</strong> Конечные автоматы и трансдукторы — основа классических систем морфологического анализа (Xerox Finite-State Tools, OpenFST). Несмотря на рост нейросетевых методов, они остаются стандартом для языков с богатой морфологией.</div>
    </div>
    
    <div class="203">
        <h3 class="203-title">Логические формулы и описание семантики</h3>
        <div class="203-card">
            <h4>Логика предикатов первого порядка</h4>
            <p>Логика предикатов первого порядка (First-Order Logic, FOL) — формальная система для представления утверждений о сущностях и их свойствах.</p>
            <p><strong>Основные компоненты:</strong></p>
            <ul>
                <li><strong>Константы:</strong> обозначают конкретные сущности (john, moscow)</li>
                <li><strong>Переменные:</strong> обозначают произвольные сущности (x, y)</li>
                <li><strong>Предикаты:</strong> выражают свойства и отношения (Student(x), Loves(x, y))</li>
                <li><strong>Функции:</strong> отображают сущности в сущности (mother(x))</li>
                <li><strong>Кванторы:</strong> ∀ (для всех), ∃ (существует)</li>
                <li><strong>Связки:</strong> ¬ (не), ∧ (и), ∨ (или), → (если... то), ↔ (тогда и только тогда)</li>
            </ul>
            <p><strong>Пример перевода предложения в FOL:</strong></p>
            <p>"Каждый студент читает какую-то книгу" → ∀x(Student(x) → ∃y(Book(y) ∧ Reads(x, y)))</p>
        </div>
        
        <div class="203-card">
            <h4>Модальная логика в лингвистике</h4>
            <p>Модальная логика расширяет логику предикатов операторами, выражающими модальности: возможность, необходимость, знание, убеждение, время.</p>
            <p><strong>Основные модальные операторы:</strong></p>
            <ul>
                <li><strong>◇φ</strong> — возможно, что φ</li>
                <li><strong>□φ</strong> — необходимо, что φ</li>
                <li><strong>Kₐφ</strong> — агент a знает, что φ (эпистемическая логика)</li>
                <li><strong>Bₐφ</strong> — агент a верит, что φ (доксастическая логика)</li>
                <li><strong>Fφ, Pφ, Gφ, Hφ</strong> — темпоральные операторы (будущее, прошлое)</li>
            </ul>
            <p><strong>Применение:</strong></p>
            <ul>
                <li>Анализ модальных глаголов (может, должен, хочет)</li>
                <li>Моделирование косвенной речи и пропозициональных установок</li>
                <li>Формализация временны́х отношений в нарративе</li>
            </ul>
        </div>
        
        <div class="203-card">
            <h4>Лямбда-исчисление и композициональность</h4>
            <p>Лямбда-исчисление — формализм для представления функций и их применения. Используется для построения семантики из значений частей.</p>
            <p><strong>Принцип композициональности (Фреге):</strong> значение сложного выражения определяется значениями его частей и способом их комбинации.</p>
            <p><strong>Пример:</strong></p>
            <ul>
                <li>"читает" → λy.λx.Reads(x, y)</li>
                <li>"книгу" → λP.∃y(Book(y) ∧ P(y))</li>
                <li>"читает книгу" → λx.∃y(Book(y) ∧ Reads(x, y))</li>
            </ul>
        </div>
        
        <div class="kmp14"><strong>Пояснение:</strong> Логико-формальный подход к семантике лежит в основе систем вопросно-ответного поиска, семантического парсинга (преобразование естественного языка в логические формулы или SQL-запросы).</div>
    </div>
</section>

<section id="3" class="section">
    <h2 class="section-title">Вероятностное и статистическое моделирование</h2>
    
    <div class="301">
        <h3 class="301-title">Базовые понятия теории вероятностей</h3>
        <div class="301-card">
            <h4>Случайные величины и распределения</h4>
            <p><strong>Случайная величина</strong> — переменная, значение которой определяется исходом случайного эксперимента. В лингвистике: следующее слово в тексте, часть речи токена, тема документа.</p>
            <p><strong>Дискретные распределения</strong> задаются функцией вероятности P(X = x) для каждого возможного значения x.</p>
            <p><strong>Основные распределения в NLP:</strong></p>
            <ul>
                <li><strong>Равномерное:</strong> все исходы равновероятны</li>
                <li><strong>Мультиномиальное:</strong> обобщение биномиального для нескольких категорий</li>
                <li><strong>Закон Ципфа:</strong> частота слова обратно пропорциональна его рангу</li>
            </ul>
        </div>
        
        <div class="301-card">
            <h4>Условная вероятность</h4>
            <p><strong>Условная вероятность</strong> P(A|B) — вероятность события A при условии, что событие B произошло:</p>
            <p><strong>P(A|B) = P(A ∩ B) / P(B)</strong></p>
            <p><strong>Цепное правило:</strong> P(w₁, w₂, ..., wₙ) = P(w₁) · P(w₂|w₁) · P(w₃|w₁,w₂) · ... · P(wₙ|w₁,...,wₙ₋₁)</p>
            <p>Это правило лежит в основе языкового моделирования: вероятность текста раскладывается в произведение условных вероятностей слов.</p>
        </div>
        
        <div class="301-card">
            <h4>Оценка вероятности из данных</h4>
            <p><strong>Метод максимального правдоподобия (MLE):</strong> оценка вероятности по частоте в обучающей выборке.</p>
            <p>P̂(w) = count(w) / N, где N — общее число слов.</p>
            <p><strong>Проблема разреженности данных:</strong> многие возможные события никогда не встречаются в корпусе, получают нулевую вероятность. Решение — методы сглаживания.</p>
        </div>
        
        <div class="kmp11"><strong>Примечание:</strong> Понимание базовых понятий вероятности критично для работы с любыми статистическими моделями NLP, от n-грамм до современных языковых моделей.</div>
    </div>
    
    <div class="302">
        <h3 class="302-title">N-граммные модели</h3>
        <div class="302-card">
            <h4>Определение и принцип работы</h4>
            <p><strong>N-граммная модель</strong> — языковая модель, основанная на марковском предположении: вероятность слова зависит только от (n-1) предшествующих слов.</p>
            <p><strong>Марковское предположение порядка (n-1):</strong></p>
            <p>P(wᵢ | w₁, ..., wᵢ₋₁) ≈ P(wᵢ | wᵢ₋ₙ₊₁, ..., wᵢ₋₁)</p>
            <p><strong>Типы n-грамм:</strong></p>
            <ul>
                <li><strong>Униграмм (n=1):</strong> P(w) — слова независимы</li>
                <li><strong>Биграмм (n=2):</strong> P(wᵢ | wᵢ₋₁)</li>
                <li><strong>Триграмм (n=3):</strong> P(wᵢ | wᵢ₋₂, wᵢ₋₁)</li>
            </ul>
            <p><strong>Оценка MLE для биграмм:</strong></p>
            <p>P(wᵢ | wᵢ₋₁) = count(wᵢ₋₁, wᵢ) / count(wᵢ₋₁)</p>
        </div>
        
        <div class="302-card">
            <h4>Методы сглаживания</h4>
            <p>Сглаживание перераспределяет вероятностную массу, чтобы ненаблюдавшиеся n-граммы получили ненулевую вероятность.</p>
            <p><strong>Сглаживание Лапласа (add-one):</strong></p>
            <p>P(wᵢ | wᵢ₋₁) = (count(wᵢ₋₁, wᵢ) + 1) / (count(wᵢ₋₁) + V), где V — размер словаря</p>
            <p><strong>Add-k сглаживание:</strong> вместо 1 добавляется k < 1</p>
            <p><strong>Сглаживание Кнезера-Нея (Kneser-Ney):</strong></p>
            <ul>
                <li>Одно из лучших классических методов сглаживания</li>
                <li>Использует абсолютное дисконтирование и распределение продолжения</li>
                <li>Учитывает, в скольких контекстах встречалось слово, а не только его частоту</li>
            </ul>
        </div>
        
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr>
                        <th>Метод сглаживания</th>
                        <th>Принцип</th>
                        <th>Качество</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Laplace (add-one)</td>
                        <td>Добавить 1 ко всем счётчикам</td>
                        <td>Простой, но грубый</td>
                    </tr>
                    <tr>
                        <td>Add-k</td>
                        <td>Добавить k < 1</td>
                        <td>Лучше Лапласа</td>
                    </tr>
                    <tr>
                        <td>Good-Turing</td>
                        <td>Перераспределение по частотам частот</td>
                        <td>Хорошо для редких событий</td>
                    </tr>
                    <tr>
                        <td>Kneser-Ney</td>
                        <td>Абсолютное дисконтирование + continuation probability</td>
                        <td>Один из лучших классических</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="302-card">
            <h4>Преимущества и ограничения n-грамм</h4>
            <p><strong>Преимущества:</strong></p>
            <ul>
                <li>Простота реализации и интерпретации</li>
                <li>Быстрое обучение и применение</li>
                <li>Не требуют GPU</li>
            </ul>
            <p><strong>Ограничения:</strong></p>
            <ul>
                <li>Ограниченный контекст (обычно n ≤ 5)</li>
                <li>Экспоненциальный рост числа параметров с увеличением n</li>
                <li>Не улавливают долгосрочные зависимости</li>
                <li>Проблема разреженности даже с сглаживанием</li>
            </ul>
        </div>
    </div>
    
    <div class="303">
        <h3 class="303-title">Марковские модели и HMM</h3>
        <div class="303-card">
            <h4>Марковские цепи</h4>
            <p><strong>Марковская цепь</strong> — последовательность случайных величин, где вероятность каждого состояния зависит только от предыдущего состояния (свойство Маркова).</p>
            <p>P(Xₜ | X₁, ..., Xₜ₋₁) = P(Xₜ | Xₜ₋₁)</p>
            <p><strong>Матрица переходов A:</strong> aᵢⱼ = P(Xₜ = j | Xₜ₋₁ = i)</p>
        </div>
        
        <div class="303-card">
            <h4>Скрытые марковские модели (HMM)</h4>
            <p><strong>HMM</strong> — модель, в которой наблюдаемая последовательность порождается скрытой марковской цепью состояний.</p>
            <p><strong>Компоненты HMM:</strong></p>
            <ul>
                <li><strong>Q</strong> — множество скрытых состояний (например, части речи)</li>
                <li><strong>O</strong> — множество наблюдаемых символов (слова)</li>
                <li><strong>A</strong> — матрица переходов между состояниями</li>
                <li><strong>B</strong> — матрица эмиссий (вероятности наблюдений при каждом состоянии)</li>
                <li><strong>π</strong> — вектор начальных вероятностей состояний</li>
            </ul>
            <p><strong>Три основные задачи HMM:</strong></p>
            <ul>
                <li><strong>Оценка (Evaluation):</strong> P(O | λ) — алгоритм Forward</li>
                <li><strong>Декодирование (Decoding):</strong> argmax P(Q | O, λ) — алгоритм Viterbi</li>
                <li><strong>Обучение (Learning):</strong> argmax P(O | λ) по λ — алгоритм Baum-Welch (EM)</li>
            </ul>
        </div>
        
        <div class="303-card">
            <h4>Применение HMM в лингвистике</h4>
            <p><strong>POS-tagging (морфологическая разметка):</strong></p>
            <ul>
                <li>Скрытые состояния — части речи</li>
                <li>Наблюдения — словоформы</li>
                <li>Задача декодирования: найти наиболее вероятную последовательность тегов</li>
            </ul>
            <p><strong>Другие применения:</strong></p>
            <ul>
                <li>Распознавание именованных сущностей (NER)</li>
                <li>Сегментация речи и текста</li>
                <li>Распознавание речи (акустическое моделирование)</li>
            </ul>
        </div>
        
        <div class="kmp12"><strong>Важно:</strong> Хотя HMM уступили позиции нейросетевым моделям (CRF, BiLSTM, BERT) для задач разметки последовательностей, они остаются важной концептуальной основой и применяются в системах с ограниченными ресурсами.</div>
    </div>
    
    <div class="304">
        <h3 class="304-title">Байесовские подходы</h3>
        <div class="304-card">
            <h4>Теорема Байеса</h4>
            <p><strong>Теорема Байеса:</strong> P(A | B) = P(B | A) · P(A) / P(B)</p>
            <p><strong>В терминах классификации:</strong></p>
            <p>P(класс | документ) ∝ P(документ | класс) · P(класс)</p>
            <ul>
                <li><strong>P(класс)</strong> — априорная вероятность класса</li>
                <li><strong>P(документ | класс)</strong> — правдоподобие</li>
                <li><strong>P(класс | документ)</strong> — апостериорная вероятность</li>
            </ul>
        </div>
        
        <div class="304-card">
            <h4>Наивный байесовский классификатор</h4>
            <p><strong>Наивное предположение:</strong> признаки (слова) условно независимы при данном классе.</p>
            <p>P(d | c) = ∏ᵢ P(wᵢ | c)</p>
            <p><strong>Классификация:</strong> c* = argmax P(c) · ∏ᵢ P(wᵢ | c)</p>
            <p><strong>Типичные применения:</strong></p>
            <ul>
                <li>Классификация текстов по темам</li>
                <li>Фильтрация спама</li>
                <li>Сентимент-анализ</li>
                <li>Определение языка</li>
            </ul>
            <p><strong>Преимущества:</strong></p>
            <ul>
                <li>Простота и скорость обучения</li>
                <li>Работает с небольшими объёмами данных</li>
                <li>Интерпретируемость</li>
            </ul>
            <p><strong>Ограничения:</strong></p>
            <ul>
                <li>Предположение о независимости нереалистично</li>
                <li>Не учитывает порядок слов</li>
                <li>Чувствителен к несбалансированным классам</li>
            </ul>
        </div>
        
        <div class="kmp14"><strong>Пояснение:</strong> Несмотря на "наивность" предположения, наивный Байес часто показывает удивительно хорошие результаты для текстовой классификации и служит важным baseline.</div>
    </div>
</section>

<section id="4" class="section">
    <h2 class="section-title">Корпусное моделирование</h2>
    
    <div class="401">
        <h3 class="401-title">Типы корпусов и аннотирование</h3>
        <div class="401-card">
            <h4>Понятие корпуса</h4>
            <p><strong>Языковой корпус</strong> — большое собрание текстов в электронной форме, представительное для данного языка, жанра, периода или предметной области, обычно снабжённое лингвистической разметкой.</p>
            <p><strong>Ключевые характеристики корпуса:</strong></p>
            <ul>
                <li><strong>Представительность:</strong> отражение разнообразия языка</li>
                <li><strong>Сбалансированность:</strong> пропорциональное представление жанров/регистров</li>
                <li><strong>Размер:</strong> достаточный объём для статистических обобщений</li>
                <li><strong>Разметка:</strong> наличие и тип лингвистической аннотации</li>
            </ul>
        </div>
        
        <div class="401-card">
            <h4>Типология корпусов</h4>
            <p><strong>По наличию разметки:</strong></p>
            <ul>
                <li><strong>Сырые (raw):</strong> только тексты без аннотации</li>
                <li><strong>Размеченные (annotated):</strong> с лингвистической разметкой</li>
            </ul>
            <p><strong>По языковому составу:</strong></p>
            <ul>
                <li><strong>Одноязычные (monolingual)</strong></li>
                <li><strong>Параллельные (parallel):</strong> тексты с переводами</li>
                <li><strong>Сопоставимые (comparable):</strong> тексты на разных языках на одну тему</li>
            </ul>
            <p><strong>По временно́му охвату:</strong></p>
            <ul>
                <li><strong>Синхронные:</strong> тексты одного периода</li>
                <li><strong>Диахронные:</strong> тексты разных эпох для изучения изменений</li>
            </ul>
            <p><strong>По специализации:</strong></p>
            <ul>
                <li><strong>Общие (reference):</strong> BNC, НКРЯ, Russian National Corpus</li>
                <li><strong>Специализированные:</strong> медицинские, юридические, устной речи, learner corpora</li>
            </ul>
        </div>
        
        <div class="401-card">
            <h4>Уровни аннотирования</h4>
            <p><strong>Морфологическая разметка:</strong> лемма, часть речи, грамматические категории</p>
            <p><strong>Синтаксическая разметка:</strong> деревья составляющих или зависимостей</p>
            <p><strong>Семантическая разметка:</strong> семантические роли, сущности, кореференция</p>
            <p><strong>Дискурсивная разметка:</strong> риторические отношения, тема-рема</p>
        </div>
        
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr>
                        <th>Стандарт</th>
                        <th>Описание</th>
                        <th>Применение</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>TEI (Text Encoding Initiative)</td>
                        <td>XML-стандарт для гуманитарных текстов</td>
                        <td>Разметка метаданных, структуры документа</td>
                    </tr>
                    <tr>
                        <td>CoNLL</td>
                        <td>Табличный формат (token per line)</td>
                        <td>POS, NER, синтаксические зависимости</td>
                    </tr>
                    <tr>
                        <td>Universal Dependencies</td>
                        <td>Унифицированная схема синтаксических зависимостей</td>
                        <td>Кросс-лингвистические исследования</td>
                    </tr>
                    <tr>
                        <td>BRAT / WebAnno</td>
                        <td>Форматы инструментов аннотации</td>
                        <td>Ручная разметка, NER, отношения</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
    
    <div class="402">
        <h3 class="402-title">Метрики и методы корпусного анализа</h3>
        <div class="402-card">
            <h4>Частотный анализ</h4>
            <p><strong>Абсолютная частота:</strong> количество вхождений единицы в корпусе</p>
            <p><strong>Относительная частота:</strong> доля единицы от общего числа токенов (ipm — instances per million)</p>
            <p><strong>Ранг частоты:</strong> позиция в отсортированном списке (для закона Ципфа)</p>
            <p><strong>Закон Ципфа:</strong> f · r ≈ const, где f — частота, r — ранг</p>
        </div>
        
        <div class="402-card">
            <h4>Коллокации</h4>
            <p><strong>Коллокация</strong> — устойчивое сочетание слов, встречающееся чаще, чем ожидалось бы при случайном сочетании.</p>
            <p><strong>Меры ассоциации:</strong></p>
            <ul>
                <li><strong>MI (Mutual Information):</strong> log₂(P(x,y) / (P(x)·P(y))). Выделяет редкие, но устойчивые сочетания</li>
                <li><strong>t-score:</strong> (O - E) / √O. Предпочитает частотные сочетания</li>
                <li><strong>Log-likelihood:</strong> статистический тест на значимость ассоциации</li>
                <li><strong>Dice coefficient:</strong> 2·P(x,y) / (P(x) + P(y))</li>
            </ul>
        </div>
        
        <div class="402-card">
            <h4>Ключевые слова (Keywords)</h4>
            <p><strong>Ключевые слова</strong> — слова, частота которых в целевом корпусе значимо выше (или ниже), чем в референсном корпусе.</p>
            <p><strong>Метрики keyness:</strong></p>
            <ul>
                <li>Log-likelihood ratio</li>
                <li>Chi-square test</li>
                <li>Simple maths (Kilgarriff)</li>
            </ul>
            <p><strong>Применение:</strong> определение специфики текста, жанра, автора; анализ дискурса.</p>
        </div>
        
        <div class="kmp11"><strong>Примечание:</strong> Инструменты корпусного анализа: AntConc, Sketch Engine, NoSketch Engine, НКРЯ, корпус-менеджеры на Python (NLTK, spaCy).</div>
    </div>
    
    <div class="403">
        <h3 class="403-title">Методы предобработки текста</h3>
        <div class="403-card">
            <h4>Токенизация</h4>
            <p><strong>Токенизация</strong> — разбиение текста на минимальные единицы обработки (токены): слова, знаки препинания, числа.</p>
            <p><strong>Проблемы токенизации:</strong></p>
            <ul>
                <li>Сокращения и аббревиатуры (и т.д., США)</li>
                <li>Дефисные написания (по-моему, когда-нибудь)</li>
                <li>Числа и даты (12.03.2024, $100,000)</li>
                <li>URL, email, хештеги, эмодзи</li>
                <li>Агглютинативные и полисинтетические языки</li>
            </ul>
            <p><strong>Подходы:</strong></p>
            <ul>
                <li>Правила на основе регулярных выражений</li>
                <li>Статистические модели (MaxEnt, CRF)</li>
                <li>Субсловная токенизация (BPE, WordPiece, SentencePiece)</li>
            </ul>
        </div>
        
        <div class="403-card">
            <h4>Лемматизация и стемминг</h4>
            <p><strong>Лемматизация</strong> — приведение словоформы к канонической форме (лемме) с учётом морфологического анализа.</p>
            <p>Примеры: бежал → бежать, лучший → хороший, mice → mouse</p>
            <p><strong>Стемминг</strong> — отсечение окончаний и суффиксов по простым правилам без учёта морфологии.</p>
            <p>Примеры: running → run, studies → studi (ошибки возможны)</p>
            <p><strong>Сравнение:</strong></p>
            <ul>
                <li>Лемматизация точнее, но требует словаря и морфоанализатора</li>
                <li>Стемминг быстрее и проще, но менее точен</li>
            </ul>
        </div>
        
        <div class="403-card">
            <h4>POS-tagging (частеречная разметка)</h4>
            <p><strong>POS-tagging</strong> — присвоение каждому токену метки части речи (иногда с грамматическими признаками).</p>
            <p><strong>Наборы тегов:</strong></p>
            <ul>
                <li><strong>Penn Treebank:</strong> 36-45 тегов (английский)</li>
                <li><strong>Universal POS tags:</strong> 17 тегов (кросс-лингвистически)</li>
                <li><strong>OpenCorpora:</strong> русский, детальная морфология</li>
            </ul>
            <p><strong>Методы:</strong></p>
            <ul>
                <li>Правиловые (rule-based)</li>
                <li>Статистические: HMM, CRF</li>
                <li>Нейросетевые: BiLSTM-CRF, BERT-based</li>
            </ul>
        </div>
        
        <div class="kmp12"><strong>Важно:</strong> Качество предобработки критически влияет на результаты всех последующих этапов анализа. Выбор методов зависит от языка, задачи и доступных ресурсов.</div>
    </div>
</section>

<section id="5" class="section">
    <h2 class="section-title">Синтаксическое моделирование</h2>
    
    <div class="501">
        <h3 class="501-title">Контекстно-свободные грамматики и расширения</h3>
        <div class="501-card">
            <h4>Контекстно-свободные грамматики (CFG)</h4>
            <p><strong>CFG (Context-Free Grammar)</strong> — формальная грамматика, в которой каждое правило имеет вид A → α, где A — нетерминал, α — цепочка терминалов и нетерминалов.</p>
            <p><strong>Пример грамматики для простого предложения:</strong></p>
            <ul>
                <li>S → NP VP</li>
                <li>NP → Det N | Det N PP | Pron</li>
                <li>VP → V NP | V NP PP</li>
                <li>PP → P NP</li>
            </ul>
            <p><strong>Преимущества CFG:</strong></p>
            <ul>
                <li>Естественное представление иерархической структуры</li>
                <li>Эффективные алгоритмы парсинга (O(n³))</li>
                <li>Широкая поддержка инструментами</li>
            </ul>
            <p><strong>Ограничения CFG:</strong></p>
            <ul>
                <li>Не выражают согласование (между подлежащим и сказуемым)</li>
                <li>Не моделируют дальние зависимости напрямую</li>
                <li>Неоднозначность: одному предложению может соответствовать много деревьев</li>
            </ul>
        </div>
        
        <div class="501-card">
            <h4>Вероятностные CFG (PCFG)</h4>
            <p><strong>PCFG</strong> — CFG, в которой каждому правилу приписана вероятность. Сумма вероятностей правил с одинаковой левой частью равна 1.</p>
            <p><strong>Вероятность дерева:</strong> произведение вероятностей всех использованных правил.</p>
            <p><strong>Применение:</strong></p>
            <ul>
                <li>Разрешение структурной неоднозначности (выбор наиболее вероятного разбора)</li>
                <li>Языковое моделирование на уровне синтаксиса</li>
            </ul>
            <p><strong>Обучение PCFG:</strong> оценка вероятностей правил по размеченному корпусу (treebank).</p>
        </div>
        
        <div class="501-card">
            <h4>Расширенные формализации</h4>
            <p><strong>TAG (Tree-Adjoining Grammar):</strong></p>
            <ul>
                <li>Оперирует элементарными деревьями, а не правилами</li>
                <li>Операции: подстановка (substitution) и вставка (adjunction)</li>
                <li>Mildly context-sensitive — выражает больше, чем CFG</li>
                <li>Применение: локализация лексической информации</li>
            </ul>
            <p><strong>HPSG (Head-Driven Phrase Structure Grammar):</strong></p>
            <ul>
                <li>Основана на типизированных признаковых структурах</li>
                <li>Унификация как основной механизм</li>
                <li>Богатое представление синтаксиса и семантики</li>
            </ul>
            <p><strong>LFG (Lexical-Functional Grammar):</strong></p>
            <ul>
                <li>Разделение c-структуры (деревья составляющих) и f-структуры (функциональные отношения)</li>
                <li>Хорошо подходит для языков со свободным порядком слов</li>
            </ul>
        </div>
    </div>
    
    <div class="502">
        <h3 class="502-title">Подходы к синтаксическому представлению</h3>
        <div class="502-card">
            <h4>Грамматики составляющих (Constituency)</h4>
            <p>Предложение разбивается на вложенные составляющие (фразы). Результат — дерево, где листья — слова, внутренние узлы — категории фраз (NP, VP, PP).</p>
            <p><strong>Характеристики:</strong></p>
            <ul>
                <li>Естественно выражают фразовую структуру</li>
                <li>Связаны с традицией генеративной грамматики</li>
                <li>Используются в Penn Treebank и подобных корпусах</li>
            </ul>
        </div>
        
        <div class="502-card">
            <h4>Грамматики зависимостей (Dependency)</h4>
            <p>Синтаксическая структура представляется как набор направленных связей между словами. Каждое слово (кроме корня) зависит от одного главного слова.</p>
            <p><strong>Характеристики:</strong></p>
            <ul>
                <li>Нет узлов фраз — только слова и связи</li>
                <li>Каждая связь помечена типом отношения (nsubj, dobj, amod, ...)</li>
                <li>Естественно для языков со свободным порядком слов</li>
                <li>Используются в Universal Dependencies</li>
            </ul>
            <p><strong>Пример:</strong> "Студент читает книгу"</p>
            <ul>
                <li>читает ← nsubj ← студент</li>
                <li>читает → dobj → книгу</li>
            </ul>
        </div>
        
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr>
                        <th>Критерий</th>
                        <th>Constituency</th>
                        <th>Dependency</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Структура</td>
                        <td>Дерево с фразовыми узлами</td>
                        <td>Граф зависимостей между словами</td>
                    </tr>
                    <tr>
                        <td>Лингвистическая традиция</td>
                        <td>Хомский, генеративная грамматика</td>
                        <td>Теньер, функциональная грамматика</td>
                    </tr>
                    <tr>
                        <td>Языковая универсальность</td>
                        <td>Лучше для SVO-языков</td>
                        <td>Универсальнее (UD)</td>
                    </tr>
                    <tr>
                        <td>Применение</td>
                        <td>Генерация, семантика</td>
                        <td>Извлечение отношений, QA</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
    
    <div class="503">
        <h3 class="503-title">Алгоритмы синтаксического анализа (парсинга)</h3>
        <div class="503-card">
            <h4>Алгоритм CYK (Cocke–Younger–Kasami)</h4>
            <p><strong>CYK</strong> — алгоритм динамического программирования для парсинга CFG в нормальной форме Хомского.</p>
            <p><strong>Сложность:</strong> O(n³ · |G|), где n — длина предложения, |G| — размер грамматики</p>
            <p><strong>Принцип:</strong> заполнение таблицы снизу вверх. Ячейка [i,j] содержит нетерминалы, порождающие подстроку от i до j.</p>
            <p><strong>Расширение для PCFG:</strong> вместо множеств хранятся вероятности, выбирается максимальная.</p>
        </div>
        
        <div class="503-card">
            <h4>Алгоритм Эрли (Earley)</h4>
            <p><strong>Earley</strong> — универсальный алгоритм для любых CFG (не требует приведения к нормальной форме).</p>
            <p><strong>Принцип:</strong> три операции — predictor, scanner, completer — для построения множеств состояний (items).</p>
            <p><strong>Сложность:</strong> O(n³) в общем случае, O(n²) для однозначных грамматик, O(n) для детерминированных.</p>
            <p><strong>Преимущества:</strong> работает с любой CFG, хорошо обрабатывает левую рекурсию.</p>
        </div>
        
        <div class="503-card">
            <h4>Парсинг зависимостей</h4>
            <p><strong>Transition-based parsing:</strong></p>
            <ul>
                <li>Построение дерева через последовательность действий (shift, left-arc, right-arc)</li>
                <li>Классификатор предсказывает следующее действие</li>
                <li><strong>Сложность:</strong> O(n) — линейная</li>
                <li>Примеры: Arc-Standard, Arc-Eager, модель Мальта</li>
            </ul>
            <p><strong>Graph-based parsing:</strong></p>
            <ul>
                <li>Оценка всех возможных рёбер, затем поиск оптимального дерева</li>
                <li>Алгоритмы: Eisner (проективные), Chu-Liu-Edmonds (непроективные)</li>
                <li><strong>Сложность:</strong> O(n²) — O(n³)</li>
                <li>Глобально оптимальное решение</li>
            </ul>
        </div>
        
        <div class="kmp14"><strong>Пояснение:</strong> Современные парсеры (например, на основе BERT) часто используют гибридные подходы: нейросеть для скоринга и классический алгоритм для построения структуры.</div>
    </div>
</section>


<section id="6" class="section">
    <h2 class="section-title">Семантические модели</h2>
    
    <div class="601">
        <h3 class="601-title">Формальная семантика</h3>
        <div class="601-card">
            <h4>Логико-формальные представления</h4>
            <p><strong>Формальная семантика</strong> изучает отношение между языковыми выражениями и их значениями с помощью логико-математических инструментов.</p>
            <p><strong>Семантика Монтегю:</strong></p>
            <ul>
                <li>Ричард Монтегю (1970-е) показал, что естественный язык можно анализировать с той же строгостью, что и формальные языки</li>
                <li>Использует интенсиональную логику и типизированное лямбда-исчисление</li>
                <li>Значение выражения — функция из возможных миров в экстенсионалы</li>
            </ul>
        </div>
        
        <div class="601-card">
            <h4>Принцип композициональности</h4>
            <p><strong>Принцип Фреге:</strong> значение сложного выражения определяется значениями его частей и способом их синтаксической комбинации.</p>
            <p><strong>Механизм:</strong> синтаксические правила сопровождаются семантическими правилами, которые комбинируют значения.</p>
            <p><strong>Типы (типовая логика):</strong></p>
            <ul>
                <li><strong>e</strong> — тип сущностей (individuals)</li>
                <li><strong>t</strong> — тип истинностных значений (truth values)</li>
                <li><strong>⟨e, t⟩</strong> — функция от сущностей к истинностным значениям (свойства)</li>
                <li><strong>⟨⟨e, t⟩, t⟩</strong> — обобщённые кванторы (типа "каждый студент")</li>
            </ul>
        </div>
        
        <div class="601-card">
            <h4>Применение в NLP</h4>
            <p><strong>Semantic parsing:</strong> преобразование предложений естественного языка в формальные представления (логические формулы, SQL, исполняемый код).</p>
            <p><strong>Примеры систем:</strong></p>
            <ul>
                <li>Вопросно-ответные системы над базами знаний</li>
                <li>Интерфейсы на естественном языке к базам данных</li>
                <li>Диалоговые агенты с формальным пониманием намерений</li>
            </ul>
        </div>
    </div>
    
    <div class="602">
        <h3 class="602-title">Распределённые представления слов (Word Embeddings)</h3>
        <div class="602-card">
            <h4>Дистрибутивная гипотеза</h4>
            <p><strong>Гипотеза Дж. Фёрса:</strong> "You shall know a word by the company it keeps" — слова, встречающиеся в похожих контекстах, имеют похожие значения.</p>
            <p><strong>Дистрибутивная семантика:</strong> значение слова представляется через статистику его употреблений в корпусе.</p>
        </div>
        
        <div class="602-card">
            <h4>Модели word embeddings</h4>
            <p><strong>Word2Vec (Mikolov et al., 2013):</strong></p>
            <ul>
                <li><strong>Skip-gram:</strong> по центральному слову предсказываются окружающие</li>
                <li><strong>CBOW:</strong> по окружающим словам предсказывается центральное</li>
                <li>Negative sampling для эффективного обучения</li>
            </ul>
            <p><strong>GloVe (Pennington et al., 2014):</strong></p>
            <ul>
                <li>Основан на матрице совстречаемости слов</li>
                <li>Оптимизирует логарифм отношения вероятностей</li>
                <li>Объединяет глобальную статистику и локальные контексты</li>
            </ul>
            <p><strong>FastText (Bojanowski et al., 2017):</strong></p>
            <ul>
                <li>Расширяет word2vec на субсловные единицы (n-граммы символов)</li>
                <li>Позволяет получать векторы для OOV (out-of-vocabulary) слов</li>
                <li>Особенно полезно для морфологически богатых языков</li>
            </ul>
        </div>
        
        <div class="602-card">
            <h4>Свойства и операции с векторами</h4>
            <p><strong>Семантическая близость:</strong> косинусное расстояние между векторами</p>
            <p>sim(w₁, w₂) = cos(θ) = (v₁ · v₂) / (||v₁|| · ||v₂||)</p>
            <p><strong>Аналогии:</strong> знаменитый пример: king − man + woman ≈ queen</p>
            <p><strong>Ограничения:</strong></p>
            <ul>
                <li>Статические векторы — один вектор на слово, не учитывает контекст</li>
                <li>Проблема многозначности (полисемии): "банк" (финансовый) vs "банк" (реки)</li>
                <li>Bias: векторы отражают предубеждения в обучающих данных</li>
            </ul>
        </div>
        
        <div class="kmp11"><strong>Примечание:</strong> Современные модели (ELMo, BERT) создают контекстуализированные embeddings — разные векторы для одного слова в разных контекстах.</div>
    </div>
    
    <div class="603">
        <h3 class="603-title">Семантические ресурсы и структурированные представления</h3>
        <div class="603-card">
            <h4>Лексические базы данных</h4>
            <p><strong>WordNet (Princeton):</strong></p>
            <ul>
                <li>Лексическая база для английского языка</li>
                <li>Организована в synsets — группы синонимов</li>
                <li><strong>Отношения:</strong> гиперонимия, гипонимия, меронимия, антонимия</li>
                <li>Русские аналоги: RuWordNet, RuThes</li>
            </ul>
            <p><strong>FrameNet:</strong></p>
            <ul>
                <li>Фреймовая семантика (Fillmore)</li>
                <li>Фрейм — сценарий с типичными участниками (ролями)</li>
                <li>Пример: фрейм COMMERCE — roles: Buyer, Seller, Goods, Money</li>
            </ul>
        </div>
        
        <div class="603-card">
            <h4>Онтологии</h4>
            <p><strong>Онтология</strong> — формальное представление знаний о предметной области: концепты, их свойства и отношения.</p>
            <p><strong>Компоненты онтологии:</strong></p>
            <ul>
                <li><strong>Классы:</strong> категории сущностей</li>
                <li><strong>Экземпляры:</strong> конкретные объекты</li>
                <li><strong>Свойства:</strong> атрибуты и отношения</li>
                <li><strong>Аксиомы:</strong> ограничения и правила вывода</li>
            </ul>
            <p><strong>Примеры:</strong></p>
            <ul>
                <li><strong>DBpedia, Wikidata:</strong> структурированные знания из Википедии</li>
                <li><strong>YAGO:</strong> объединение WordNet и Wikipedia</li>
                <li><strong>Schema.org:</strong> разметка веб-страниц для поисковых систем</li>
            </ul>
        </div>
        
        <div class="603-card">
            <h4>Семантические сети и графы знаний</h4>
            <p><strong>Семантическая сеть</strong> — граф, где узлы — концепты, рёбра — отношения между ними.</p>
            <p><strong>Граф знаний (Knowledge Graph):</strong></p>
            <ul>
                <li>Хранит факты в виде троек (субъект, предикат, объект)</li>
                <li>Пример: (Москва, столица, Россия)</li>
                <li>Позволяет делать логический вывод и отвечать на вопросы</li>
            </ul>
            <p><strong>Применение:</strong> поисковые системы (Google Knowledge Graph), виртуальные ассистенты, рекомендательные системы.</p>
        </div>
        
        <div class="kmp12"><strong>Важно:</strong> Гибридные подходы, сочетающие нейросетевые embeddings и структурированные знания (knowledge-enhanced embeddings), — актуальное направление исследований.</div>
    </div>
</section>


<section id="7" class="section">
  <h2 class="section-title">Прагматическое моделирование</h2>

  <!-- 7.1 Введение -->
  <div class="061">
    <h3 class="061-title">Прагматика в компьютерном моделировании</h3>
    <div class="061-card">
      <h4>Что изучает вычислительная прагматика</h4>
      <p>Прагматика исследует, как контекст влияет на интерпретацию языковых выражений. В отличие от семантики, изучающей буквальное значение, прагматика моделирует <em>подразумеваемые</em> значения, намерения говорящего и выводы слушающего.</p>
      <p>Компьютерное моделирование прагматики необходимо для создания систем, способных понимать не только <em>что</em> сказано, но и <em>почему</em> и <em>зачем</em>.</p>
      <p><strong>Ключевые задачи вычислительной прагматики:</strong></p>
      <ul>
        <li>Распознавание намерений пользователя (intent detection)</li>
        <li>Разрешение референции и анафоры в контексте</li>
        <li>Моделирование импликатур и пресуппозиций</li>
        <li>Управление диалогом и поддержание когерентности</li>
      </ul>
      <p><strong>Пояснение:</strong> Прагматические модели лежат в основе чат-ботов, голосовых ассистентов и систем автоматического ответа на вопросы.</p>
    </div>
  </div>

  <!-- 7.2 Речевые акты -->
  <div class="062">
    <h3 class="062-title">Модели актора/адресата и теория речевых актов</h3>
    <div class="062-card">
      <h4>Теория речевых актов Остина и Сёрля</h4>
      <p>Теория речевых актов (Speech Act Theory) рассматривает высказывание как действие. Дж. Остин выделил три уровня речевого акта, которые необходимо моделировать в диалоговых системах:</p>
      <p><strong>Три компонента речевого акта:</strong></p>
      <ul>
        <li><strong>Локуция</strong> — произнесение высказывания с определённым значением</li>
        <li><strong>Иллокуция</strong> — намерение говорящего (просьба, обещание, вопрос)</li>
        <li><strong>Перлокуция</strong> — эффект, произведённый на адресата</li>
      </ul>
    </div>
    <div class="062-card">
      <h4>Таксономия иллокутивных актов Сёрля</h4>
      <p>Дж. Сёрль классифицировал речевые акты по их иллокутивной силе. Эта классификация используется в системах распознавания намерений:</p>
    </div>
    <div class="table-kmp">
      <table class="table">
        <thead>
          <tr>
            <th>Тип акта</th>
            <th>Описание</th>
            <th>Примеры</th>
          </tr>
        </thead>
        <tr>
          <td>Репрезентативы (ассертивы)</td>
          <td>Описание положения дел</td>
          <td>утверждение, сообщение, констатация</td>
        </tr>
        <tr>
          <td>Директивы</td>
          <td>Побуждение к действию</td>
          <td>просьба, приказ, вопрос</td>
        </tr>
        <tr>
          <td>Комиссивы</td>
          <td>Принятие обязательства</td>
          <td>обещание, клятва, угроза</td>
        </tr>
        <tr>
          <td>Экспрессивы</td>
          <td>Выражение эмоций</td>
          <td>благодарность, извинение, поздравление</td>
        </tr>
        <tr>
          <td>Декларативы</td>
          <td>Изменение статуса реальности</td>
          <td>объявление, назначение, увольнение</td>
        </tr>
      </table>
    </div>
    <div class="062-card">
      <h4>Вычислительные модели речевых актов</h4>
      <p>В современных NLP-системах распознавание речевых актов реализуется как задача классификации намерений (intent classification):</p>
      <p><strong>Подходы к моделированию:</strong></p>
      <ul>
        <li><strong>Правиловые системы</strong> — на основе ключевых слов и паттернов</li>
        <li><strong>Статистические классификаторы</strong> — SVM, наивный Байес</li>
        <li><strong>Нейросетевые модели</strong> — BERT, RoBERTa для intent detection</li>
        <li><strong>Slot filling</strong> — заполнение слотов для параметров намерения</li>
      </ul>
    </div>
    <div class="kmp11"><strong>Примечание:</strong> В промышленных системах (Dialogflow, RASA, Amazon Lex) intent detection и slot filling объединяются в единый pipeline обработки пользовательского ввода.</div>
  </div>

  <!-- 7.3 Кооперация и инференция -->
  <div class="063">
    <h3 class="063-title">Модели кооперации и инференции</h3>
    <div class="063-card">
      <h4>Принцип кооперации и максимы Грайса</h4>
      <p>Г.П. Грайс сформулировал принцип кооперации: участники коммуникации стремятся к эффективному обмену информацией. Этот принцип конкретизируется в четырёх максимах:</p>
    </div>
    <div class="table-kmp">
      <table class="table">
        <thead>
          <tr>
            <th>Максима</th>
            <th>Требование</th>
            <th>Нарушение → импликатура</th>
          </tr>
        </thead>
        <tr>
          <td><strong>Количества</strong></td>
          <td>Сообщай ровно столько, сколько нужно</td>
          <td>Избыточность/недосказанность сигнализирует скрытый смысл</td>
        </tr>
        <tr>
          <td><strong>Качества</strong></td>
          <td>Говори только то, что считаешь истинным</td>
          <td>Ирония, метафора, гипербола</td>
        </tr>
        <tr>
          <td><strong>Отношения (релевантности)</strong></td>
          <td>Говори по существу</td>
          <td>Уклонение от темы как намёк</td>
        </tr>
        <tr>
          <td><strong>Способа</strong></td>
          <td>Выражайся ясно, избегай двусмысленности</td>
          <td>Намеренная неясность как стратегия</td>
        </tr>
      </table>
    </div>
    <div class="063-card">
      <h4>Вычислительное моделирование импликатур</h4>
      <p>Моделирование грайсианских рассуждений — сложная задача, требующая учёта контекста и здравого смысла. Современные подходы включают:</p>
      <p><strong>Методы моделирования:</strong></p>
      <ul>
        <li><strong>Rational Speech Acts (RSA)</strong> — байесовская модель прагматического вывода</li>
        <li><strong>Game-theoretic pragmatics</strong> — теоретико-игровые модели коммуникации</li>
        <li><strong>Large Language Models</strong> — неявное моделирование прагматики в GPT, LLaMA</li>
      </ul>
    </div>
    <div class="063-card">
      <h4>Модель Rational Speech Acts (RSA)</h4>
      <p>RSA моделирует коммуникацию как рекурсивное рассуждение о намерениях:</p>
      <ul>
        <li><strong>Literal listener (L₀)</strong> — интерпретирует буквально</li>
        <li><strong>Pragmatic speaker (S₁)</strong> — выбирает высказывание с учётом L₀</li>
        <li><strong>Pragmatic listener (L₁)</strong> — интерпретирует с учётом S₁</li>
      </ul>
      <p>Формально: P(m|u) ∝ P(u|m) · P(m), где m — значение, u — высказывание.</p>
    </div>
    <div class="063-card">
      <h4>BDI-архитектура: Belief–Desire–Intention</h4>
      <p>Модель BDI используется для моделирования рациональных агентов в диалоговых системах:</p>
      <ul>
        <li><strong>Beliefs (убеждения)</strong> — знания агента о мире</li>
        <li><strong>Desires (желания)</strong> — цели, к которым агент стремится</li>
        <li><strong>Intentions (намерения)</strong> — выбранный план действий</li>
      </ul>
      <p><strong>Пояснение:</strong> BDI-агенты способны планировать действия, пересматривать намерения и объяснять своё поведение.</p>
    </div>
    <div class="kmp12"><strong>Важно:</strong> Теория релевантности Спербера и Уилсон развивает идеи Грайса, сводя все максимы к одному принципу релевантности: высказывание должно производить максимальный когнитивный эффект при минимальных усилиях обработки.</div>
  </div>

  <!-- 7.4 Модели диалога -->
  <div class="064">
    <h3 class="064-title">Вычислительные модели диалога</h3>
    <div class="064-card">
      <h4>Эволюция диалоговых систем</h4>
      <p>Диалоговые системы прошли путь от простых правиловых систем до нейросетевых моделей. Основные архитектуры различаются по способу управления состоянием диалога:</p>
    </div>
    <div class="table-kmp">
      <table class="table">
        <thead>
          <tr>
            <th>Архитектура</th>
            <th>Принцип</th>
            <th>Преимущества</th>
            <th>Недостатки</th>
          </tr>
        </thead>
        <tr>
          <td>FSM</td>
          <td>Конечный автомат с фиксированными переходами</td>
          <td>Простота, предсказуемость</td>
          <td>Негибкость, комбинаторный взрыв</td>
        </tr>
        <tr>
          <td>Frame-based</td>
          <td>Заполнение слотов фрейма</td>
          <td>Гибкий порядок вопросов</td>
          <td>Ограничен доменом</td>
        </tr>
        <tr>
          <td>POMDP</td>
          <td>Марковский процесс с неполной наблюдаемостью</td>
          <td>Учёт неопределённости</td>
          <td>Вычислительная сложность</td>
        </tr>
        <tr>
          <td>End-to-end neural</td>
          <td>Обучение на данных без явных правил</td>
          <td>Масштабируемость</td>
          <td>Нет гарантий, требует много данных</td>
        </tr>
      </table>
    </div>
    <div class="064-card">
      <h4>FSM — Finite State Machine (конечный автомат)</h4>
      <p>Диалог представляется как граф, где узлы — состояния системы, а рёбра — переходы по условиям (реплики пользователя):</p>
      <ul>
        <li>Состояния: S = {s₀, s₁, ..., sₙ}</li>
        <li>Входной алфавит: пользовательские интенты</li>
        <li>Функция переходов: δ(s, intent) → s'</li>
        <li>Выходы: реплики системы</li>
      </ul>
      <p><strong>Применение:</strong> IVR-системы, простые FAQ-боты, голосовые меню.</p>
    </div>
    <div class="064-card">
      <h4>POMDP — Partially Observable Markov Decision Process</h4>
      <p>POMDP моделирует диалог как процесс принятия решений в условиях неопределённости:</p>
      <p><strong>Компоненты POMDP:</strong></p>
      <ul>
        <li><strong>S</strong> — множество скрытых состояний (намерения пользователя)</li>
        <li><strong>A</strong> — множество действий системы</li>
        <li><strong>O</strong> — наблюдения (ASR-выход, распознанные интенты)</li>
        <li><strong>T(s'|s,a)</strong> — вероятность перехода</li>
        <li><strong>Ω(o|s',a)</strong> — модель наблюдений</li>
        <li><strong>R(s,a)</strong> — функция вознаграждения</li>
        <li><strong>b(s)</strong> — belief state (распределение вероятностей состояний)</li>
      </ul>
      <p><strong>Пояснение:</strong> POMDP позволяет системе принимать решения даже при ошибках распознавания речи, поддерживая вероятностное представление о состоянии диалога.</p>
    </div>
    <div class="064-card">
      <h4>Нейронные диалоговые модели</h4>
      <p>Современные диалоговые системы основаны на архитектурах глубокого обучения:</p>
      <p><strong>Основные архитектуры:</strong></p>
      <ul>
        <li><strong>Seq2Seq с вниманием</strong> — encoder-decoder для генерации ответов</li>
        <li><strong>Memory Networks</strong> — внешняя память для хранения контекста</li>
        <li><strong>Transformer-based</strong> — GPT, DialoGPT, BlenderBot</li>
        <li><strong>Retrieval-augmented</strong> — RAG для task-oriented диалогов</li>
      </ul>
      <p><strong>Компоненты task-oriented системы:</strong></p>
      <ul>
        <li>NLU (Natural Language Understanding) — понимание интента и слотов</li>
        <li>DST (Dialogue State Tracking) — отслеживание состояния</li>
        <li>Policy — выбор следующего действия</li>
        <li>NLG (Natural Language Generation) — генерация ответа</li>
      </ul>
    </div>
    <div class="kmp14"><strong>Пояснение:</strong> В гибридных системах нейросетевые компоненты (NLU, NLG) сочетаются с символическими (правила, база знаний) для обеспечения контролируемости и интерпретируемости.</div>
  </div>

  <!-- 7.5 Когерентность и RST -->
  <div class="065">
    <h3 class="065-title">Когерентность и сегментация дискурса</h3>
    <div class="065-card">
      <h4>Понятие когерентности дискурса</h4>
      <p>Когерентность — свойство текста, при котором его части связаны логически и тематически. Компьютерные модели когерентности необходимы для:</p>
      <ul>
        <li>Оценки качества генерируемого текста</li>
        <li>Автоматического реферирования</li>
        <li>Анализа аргументации</li>
        <li>Понимания многоходовых диалогов</li>
      </ul>
    </div>
    <div class="065-card">
      <h4>RST — Rhetorical Structure Theory</h4>
      <p>Теория риторической структуры (Манн и Томпсон, 1988) описывает текст как иерархию связанных сегментов. Каждая связь — риторическое отношение между ядром (nucleus) и сателлитом:</p>
    </div>
    <div class="table-kmp">
      <table class="table">
        <thead>
          <tr>
            <th>Тип отношения</th>
            <th>Примеры</th>
            <th>Описание</th>
          </tr>
        </thead>
        <tr>
          <td>Презентационные</td>
          <td>Motivation, Antithesis, Background</td>
          <td>Воздействуют на адресата</td>
        </tr>
        <tr>
          <td>Субъектно-материальные</td>
          <td>Elaboration, Cause, Condition</td>
          <td>Организуют содержание</td>
        </tr>
        <tr>
          <td>Многоядерные</td>
          <td>Contrast, Sequence, Joint</td>
          <td>Равноправные сегменты</td>
        </tr>
      </table>
    </div>
    <div class="065-card">
      <h4>Вычислительные модели RST-парсинга</h4>
      <p>RST-парсинг включает два этапа: сегментацию текста на элементарные дискурсивные единицы (EDU) и построение дерева отношений:</p>
      <p><strong>Подходы к RST-парсингу:</strong></p>
      <ul>
        <li><strong>Transition-based</strong> — shift-reduce парсеры (DPLP)</li>
        <li><strong>Chart-based</strong> — CKY-подобные алгоритмы</li>
        <li><strong>Neural parsers</strong> — BERT для сегментации и классификации</li>
        <li><strong>Top-down</strong> — рекурсивное разбиение (Wang et al., 2017)</li>
      </ul>
      <p><strong>Ресурсы:</strong> RST Discourse Treebank (RST-DT), PDTB (Penn Discourse Treebank), Ru-RSTreebank для русского языка.</p>
    </div>
    <div class="065-card">
      <h4>Centering Theory — теория центрации</h4>
      <p>Теория центрации (Grosz, Joshi, Weinstein) моделирует локальную когерентность через отслеживание фокуса внимания:</p>
      <p><strong>Основные понятия:</strong></p>
      <ul>
        <li><strong>Cf (forward-looking centers)</strong> — сущности, упомянутые в высказывании</li>
        <li><strong>Cb (backward-looking center)</strong> — наиболее значимая сущность, связывающая с предыдущим высказыванием</li>
        <li><strong>Cp (preferred center)</strong> — наиболее вероятный Cb следующего высказывания</li>
      </ul>
      <p><strong>Типы переходов:</strong></p>
      <ul>
        <li><strong>Continue</strong> — Cb = Cp, Cb сохраняется</li>
        <li><strong>Retain</strong> — Cb сохраняется, но Cb ≠ Cp</li>
        <li><strong>Smooth Shift</strong> — Cb меняется, новый Cb = Cp</li>
        <li><strong>Rough Shift</strong> — Cb меняется, новый Cb ≠ Cp</li>
      </ul>
    </div>
    <div class="065-card">
      <h4>Применение моделей когерентности</h4>
      <p><strong>Практические задачи:</strong></p>
      <ul>
        <li>Оценка эссе и автоматическая проверка сочинений</li>
        <li>Упорядочивание предложений (sentence ordering)</li>
        <li>Обнаружение разрывов в диалоге</li>
        <li>Выделение тематических сегментов в длинных текстах</li>
        <li>Улучшение генерации текста в LLM</li>
      </ul>
    </div>
    <div class="kmp11"><strong>Примечание:</strong> Модели когерентности часто используются как дополнительные сигналы при обучении языковых моделей, улучшая связность генерируемых текстов.</div>
  </div>

  <!-- Итоговая таблица -->
  <div class="066">
    <h3 class="066-title">Сводная таблица подходов</h3>
    <div class="table-kmp">
      <table class="table">
        <thead>
          <tr>
            <th>Область</th>
            <th>Ключевые теории</th>
            <th>Вычислительные модели</th>
            <th>Инструменты/ресурсы</th>
          </tr>
        </thead>
        <tr>
          <td>Речевые акты</td>
          <td>Остин, Сёрль</td>
          <td>Intent classification, slot filling</td>
          <td>RASA, Dialogflow, Snips NLU</td>
        </tr>
        <tr>
          <td>Импликатуры</td>
          <td>Грайс, Sperber & Wilson</td>
          <td>RSA, game-theoretic models</td>
          <td>WebPPL, probabilistic programs</td>
        </tr>
        <tr>
          <td>Намерения агента</td>
          <td>BDI (Bratman)</td>
          <td>BDI-агенты, planning</td>
          <td>Jason, JACK, AgentSpeak</td>
        </tr>
        <tr>
          <td>Управление диалогом</td>
          <td>—</td>
          <td>FSM, POMDP, RL</td>
          <td>PyDial, ConvLab</td>
        </tr>
        <tr>
          <td>Когерентность</td>
          <td>RST, Centering</td>
          <td>RST-парсеры, neural coherence</td>
          <td>RST-DT, PDTB, DMRST</td>
        </tr>
      </table>
    </div>
  </div>

  <div class="kmp12"><strong>Важно:</strong> Современные LLM неявно моделируют многие прагматические явления, однако явные модели прагматики остаются важными для интерпретируемости, контроля и систем с ограниченными данными.</div>

</section>
				
	
<footer class="footer">
<div class="container">
<p>© 2025 | kmp | CC BY-NC-SA 4.0<br>
Разработано для студентов БрГУ имени А.С. Пушкина</p>
</div>
</footer>
<div style="position: fixed; bottom: 10px; color: #777777; right: 30px; opacity: 0.3; font-size: 14px;">kmp+</div>
    <script>
        // Функция для плавной прокрутки к разделу
        function scrollToSection(sectionId) {
            const section = document.getElementById(sectionId);
            const menuHeight = document.querySelector('.menu').offsetHeight;
            
            window.scrollTo({
                top: section.offsetTop - menuHeight - 20,
                behavior: 'smooth'
            });
        }

        // Функция для переключения темы
        document.getElementById('themeToggle').addEventListener('click', function() {
            const currentTheme = document.documentElement.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            
            document.documentElement.setAttribute('data-theme', newTheme);
            this.textContent = newTheme === 'dark' ? '🌙' : '☀️';
        });

        // Анимация появления секций при прокрутке
        document.addEventListener('DOMContentLoaded', function() {
            const sections = document.querySelectorAll('.section');
            
            const observerOptions = {
                root: null,
                rootMargin: '0px',
                threshold: 0.1
            };
            
            const observer = new IntersectionObserver(function(entries, observer) {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('fade-in');
                        observer.unobserve(entry.target);
                    }
                });
            }, observerOptions);
            
            sections.forEach(section => {
                section.classList.remove('fade-in');
                observer.observe(section);
            });
        });
    </script>
</body>
</html>