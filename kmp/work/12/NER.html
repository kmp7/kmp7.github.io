<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>kmp+</title>
    <style>
        /* Основные стили */
        :root {
            --primary-color: #325980;
            --secondary-color: #4CAF50;
            --background-color: #f5f5f5;
            --content-bg: #ffffff;
            --text-color: #333333;
            --header-text-color: #ffffff;
            --menu-bg: #ffffff;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            --border-radius: 8px;
			  --accent11: #4caf50;
		--accent12: #4cafff;
		--accent13: #ffaf50;
		--accent14: #821978;
        }

        /* Темная тема */
        [data-theme="dark"] {
            --primary-color: #3e76ad;
            --secondary-color: #388e3c;
            --background-color: #000000;
            --content-bg: #1e1e1e;
            --text-color: #e0e0e0;
            --header-text-color: #ffffff;
            --menu-bg: #000000;
            --menu-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            transition: background-color 0.3s, color 0.3s;
        }

        body {
            font-family: 'Roboto', 'Arial', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--background-color);
            padding-top: 2px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        header {
            background-color: var(--primary-color);
            color: var(--header-text-color);
            padding: 20px 0;
            text-align: center;
            border-radius: var(--border-radius);
            margin-bottom: 2px;
        }

        h1 {
            font-size: 2.2rem;
            margin-bottom: 10px;
        }

        h2 {
            color: var(--primary-color);
            margin: 25px 0 15px;
            padding-bottom: 8px;
            border-bottom: 2px solid var(--secondary-color);
        }

        h3 {
            color: var(--primary-color);
            margin: 20px 0 10px;
        }

        p {
            margin-bottom: 15px;
        }

        /* Меню навигации */
        .menu {
            background-color: var(--menu-bg);
            padding: 15px 20px;
            border-radius: var(--border-radius);
            margin-bottom: 30px;
            box-shadow: var(--menu-shadow);
            display: flex;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .menu-buttons {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }
		
		
		.menu-buttons {
    display: flex;
    flex-wrap: wrap;
    gap: 10px;
    justify-content: center; /* Изменено на center */
}

        .menu-btn {
            background-color: var(--primary-color);
            color: white;
            border: none;
            padding: 8px 15px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: background-color 0.3s;
        }

        .menu-btn:hover {
            background-color: var(--secondary-color);
        }

        .theme-toggle {
            background: none;
            border: 10px solid transparent;
            font-size: 1.5rem;
            cursor: pointer;
            color: var(--primary-color);
        }

        /* Секции контента */
        .section {
            background-color: var(--content-bg);
            border-radius: var(--border-radius);
            padding: 25px;
            margin-bottom: 30px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        /* Выделение важного */
        .important {
            background-color: rgba(76, 175, 80, 0.1);
            border-left: 4px solid var(--secondary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        .note {
            background-color: rgba(50, 89, 128, 0.1);
            border-left: 4px solid var(--primary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        /* Таблицы */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            padding: 12px 15px;
            border: 1px solid #ddd;
            text-align: left;
        }

        th {
            background-color: var(--primary-color);
            color: white;
        }

        tr:nth-child(even) {
            background-color: rgba(0, 0, 0, 0.03);
        }

        /* Списки */
        ul, ol {
            padding-left: 25px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
        }

        /* Адаптивный дизайн */
        @media (max-width: 768px) {
            h1 {
                font-size: 1.8rem;
            }
            
            h2 {
                font-size: 1.5rem;
            }
            
            .menu {
                flex-direction: column;
                gap: 15px;
            }
            
            .menu-buttons {
                width: 100%;
                justify-content: center;
            }
            
            .theme-toggle {
                margin-top: 10px;
            }
            
            .section {
                padding: 15px;
            }
        }

        /* Анимации */
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .fade-in {
            animation: fadeIn 0.5s ease-in;
        }

        footer {
            text-align: center;
            padding: 20px 0;
            margin-top: 40px;
            font-size: 0.9rem;
        }
		
					.kmp11, .example {
      background: rgba(76, 175, 80, 0.1);
      border-left: 4px solid var(--accent11);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp12, .example {
      background: rgba(95, 182, 237, 0.1);
      border-left: 4px solid var(--accent12);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
 
		.kmp13, .example {
      background: rgba(205, 170, 110, 0.1);
      border-left: 4px solid var(--accent13);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }

		.kmp14, .example {
      background: rgba(205, 110, 200, 0.1);
      border-left: 4px solid var(--accent14);
      padding: 10px 15px;
      margin: 15px 0;
      border-radius: 4px;
    }
	
	
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Named Entity Recognition</h1>
            <p>автоматическое извлечения именованных сущностей в тексте</p>
        </header>

        <nav class="menu">
            <div class="menu-buttons">
                <button class="menu-btn" onclick="scrollToSection('1')">NER</button>
                <button class="menu-btn" onclick="scrollToSection('2')">LA</button>
                <button class="menu-btn" onclick="scrollToSection('3')">ML</button>
                <button class="menu-btn" onclick="scrollToSection('4')">DL</button>
				<button class="menu-btn" onclick="scrollToSection('5')">LLM</button>
				<button class="menu-btn" onclick="scrollToSection('6')">Practice</button>
				<button class="menu-btn" onclick="scrollToSection('7')">Summary</button>
                                    </div>
            <button class="theme-toggle" id="themeToggle" title="Переключить тему">☀️</button>
        </nav>


<section id="introduction" class="section">
    <h2 class="section-title">Понятие NER</h2>
    
    <div class="001">
               <div class="001-card">
            <p><strong>Named Entity Recognition (NER)</strong> — это задача автоматического извлечения и классификации именованных сущностей в тексте. Именованная сущность — это слово или словосочетание, которое обозначает уникальный объект реального мира: человека, организацию, место, дату и т.д.</p>
            <p>NER является одной из фундаментальных задач обработки естественного языка (NLP) и служит базой для множества прикладных задач.</p>
            <p><strong>Где применяется NER:</strong></p>
            <ul>
                <li>Информационный поиск (найти все документы про конкретную компанию)</li>
                <li>Машинный перевод (имена собственные требуют особой обработки)</li>
                <li>Вопросно-ответные системы (кто? где? когда?)</li>
                <li>Анализ социальных сетей (отслеживание упоминаний брендов)</li>
                <li>Медицинская информатика (извлечение названий лекарств, симптомов)</li>
                <li>Юридические технологии (извлечение имён сторон, дат, сумм)</li>
            </ul>
        </div>
    </div>

    <div class="002">
        <h3 class="002-title">Типология сущностей</h3>
        <div class="002-card">
            <p>Классический набор типов сущностей был предложен в рамках конференций MUC (Message Understanding Conference) в 1990-х годах. Со временем типология расширялась для решения специфических задач.</p>
        </div>
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr>
                        <th>Тип</th>
                        <th>Код</th>
                        <th>Примеры</th>
                    </tr>
                </thead>
                <tr><td>Персона</td><td>PER</td><td>Александр Пушкин, Меркель, доктор Хаус</td></tr>
                <tr><td>Организация</td><td>ORG</td><td>Газпром, ООН, МГУ, партия «Яблоко»</td></tr>
                <tr><td>Локация</td><td>LOC</td><td>Москва, река Волга, Альпы, улица Ленина</td></tr>
                <tr><td>Геополитическая единица</td><td>GPE</td><td>Россия, штат Техас, Евросоюз</td></tr>
                <tr><td>Время/Дата</td><td>TIME/DATE</td><td>15 марта 2024, вчера, XIX век</td></tr>
                <tr><td>Денежные суммы</td><td>MONEY</td><td>500 рублей, $1.5 млн</td></tr>
                <tr><td>Процент</td><td>PERCENT</td><td>15%, три четверти</td></tr>
                <tr><td>Объект/Сооружение</td><td>FAC (Facility)</td><td>Эйфелева башня, аэропорт Шереметьево</td></tr>
                <tr><td>Продукт</td><td>PRODUCT</td><td>iPhone 15, Toyota Camry</td></tr>
                <tr><td>Событие</td><td>EVENT</td><td>Олимпиада-2024, Вторая мировая война</td></tr>
                <tr><td>Произведение</td><td>WORK_OF_ART</td><td>«Война и мир», «Мона Лиза»</td></tr>
                <tr><td>Прочее</td><td>MISC</td><td>Всё, что не попадает в другие категории</td></tr>
            </table>
        </div>
        <div class="kmp14"><strong>Пояснение:</strong> Выбор типологии зависит от задачи. Для новостной аналитики достаточно PER, ORG, LOC. Для медицинских текстов нужны DISEASE, DRUG, SYMPTOM. Для юридических — CONTRACT, COURT, LAW.</div>
    </div>

    <div class="003">
        <h3 class="003-title">Сложные случаи: вложенные и прерывистые сущности</h3>
        <div class="003-card">
            <p>Реальные тексты содержат сущности, которые сложно обрабатывать стандартными методами.</p>
            <p><strong>Вложенные сущности (Nested Entities):</strong></p>
            <ul>
                <li>«<u>Московский</u> государственный университет» — внутри ORG находится отсылка к LOC (Москва)</li>
                <li>«Банк <u>России</u>» — ORG содержит GPE</li>
                <li>«Премия имени <u>Нобеля</u>» — EVENT содержит PER</li>
            </ul>
            <p><strong>Прерывистые сущности (Discontinuous Entities):</strong></p>
            <ul>
                <li>«Министерство иностранных дел и торговли» — две организации с общим началом</li>
                <li>«Президенты России и США» — два GPE в одной конструкции</li>
            </ul>
            <p><strong>Метонимия:</strong></p>
            <ul>
                <li>«Москва приняла решение» — LOC или ORG (правительство)?</li>
                <li>«Кремль заявил» — FAC или ORG?</li>
            </ul>
        </div>
        <div class="kmp12"><strong>Важно:</strong> Большинство стандартных NER-систем не обрабатывают вложенные сущности. Для таких случаев используются специальные архитектуры (span-based models) или многослойная разметка.</div>
    </div>

    <div class="004">
        <h3 class="004-title">Форматы разметки: BIO и BILOU</h3>
        <div class="004-card">
            <p>Для обучения NER-моделей текст размечается на уровне токенов (слов). Каждому токену присваивается метка, указывающая, является ли он частью сущности и какой именно.</p>
            <p><strong>BIO-схема (IOB2):</strong></p>
            <ul>
                <li><strong>B</strong> (Begin) — первый токен сущности</li>
                <li><strong>I</strong> (Inside) — продолжение сущности</li>
                <li><strong>O</strong> (Outside) — не сущность</li>
            </ul>
        </div>
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr><th>Токен</th><th>BIO-метка</th><th>BILOU-метка</th></tr>
                </thead>
                <tr><td>Александр</td><td>B-PER</td><td>B-PER</td></tr>
                <tr><td>Сергеевич</td><td>I-PER</td><td>I-PER</td></tr>
                <tr><td>Пушкин</td><td>I-PER</td><td>L-PER</td></tr>
                <tr><td>родился</td><td>O</td><td>O</td></tr>
                <tr><td>в</td><td>O</td><td>O</td></tr>
                <tr><td>Москве</td><td>B-LOC</td><td>U-LOC</td></tr>
            </table>
        </div>
        <div class="004-card">
            <p><strong>BILOU-схема (BIOES):</strong></p>
            <ul>
                <li><strong>B</strong> (Begin) — начало многотокенной сущности</li>
                <li><strong>I</strong> (Inside) — середина сущности</li>
                <li><strong>L</strong> (Last) — последний токен сущности</li>
                <li><strong>O</strong> (Outside) — не сущность</li>
                <li><strong>U</strong> (Unit) — однотокенная сущность</li>
            </ul>
        </div>
        <div class="kmp14"><strong>Пояснение:</strong> BILOU предоставляет больше информации о границах сущностей. Исследования показывают, что для нейросетевых моделей BILOU даёт прирост точности на 0.5–1.5% по сравнению с BIO, особенно для коротких сущностей.</div>
    </div>

    <div class="005">
        <h3 class="005-title">Метрики оценки качества</h3>
        <div class="005-card">
            <p>Качество NER оценивается на уровне сущностей (не токенов). Основные метрики — Precision, Recall и F1-score.</p>
            <p><strong>Формулы:</strong></p>
            <ul>
                <li><strong>Precision</strong> = TP / (TP + FP) — доля правильных среди найденных</li>
                <li><strong>Recall</strong> = TP / (TP + FN) — доля найденных среди всех правильных</li>
                <li><strong>F1</strong> = 2 × (Precision × Recall) / (Precision + Recall)</li>
            </ul>
        </div>
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr><th>Тип оценки</th><th>Описание</th><th>Пример</th></tr>
                </thead>
                <tr>
                    <td>Exact Match (строгое)</td>
                    <td>Границы И тип должны совпадать полностью</td>
                    <td>«Александр Пушкин»[PER] = «Александр Пушкин»[PER] ✓</td>
                </tr>
                <tr>
                    <td>Partial Match (частичное)</td>
                    <td>Учитывается частичное совпадение границ</td>
                    <td>«Александр»[PER] ≈ «Александр Пушкин»[PER] — частично верно</td>
                </tr>
                <tr>
                    <td>Type Match</td>
                    <td>Границы совпадают, но тип может отличаться</td>
                    <td>«Москва»[LOC] vs «Москва»[GPE]</td>
                </tr>
            </table>
        </div>
        <div class="kmp11"><strong>Примечание:</strong> Для большинства задач используется Exact Match. Частичное совпадение полезно при анализе ошибок и для задач, где границы сущности неоднозначны (например, включать ли титул «г-н» в имя).</div>
    </div>
</section>

<section id="2" class="section">
    <h2 class="section-title">Лингвистический анализ NER</h2>
    
    <div class="006">
        <h3 class="006-title">Газеттиры: словари сущностей</h3>
        <div class="006-card">
            <p><strong>Газеттир (Gazetteer)</strong> — это структурированный список именованных сущностей определённого типа. Это один из старейших и надёжных методов NER.</p>
            <p><strong>Источники газеттиров:</strong></p>
            <ul>
                <li><strong>Wikidata</strong> — структурированная база знаний с миллионами сущностей и их свойствами</li>
                <li><strong>GeoNames</strong> — база географических названий (11+ млн объектов)</li>
                <li><strong>ORCID</strong> — идентификаторы исследователей</li>
                <li><strong>Национальные реестры</strong> — ЕГРЮЛ (организации РФ), ФИАС (адреса)</li>
                <li><strong>Специализированные базы</strong> — DrugBank (лекарства), UMLS (медицина)</li>
            </ul>
            <p><strong>Преимущества газеттиров:</strong></p>
            <ul>
                <li>100% точность для известных сущностей</li>
                <li>Не требуют обучения</li>
                <li>Легко обновляются</li>
                <li>Интерпретируемость результатов</li>
            </ul>
            <p><strong>Недостатки:</strong></p>
            <ul>
                <li>Не распознают новые сущности (OOV — Out of Vocabulary)</li>
                <li>Проблемы с вариативностью написания (Газпром / ПАО «Газпром» / Gazprom)</li>
                <li>Омонимия (Пушкин — поэт, город, станция метро)</li>
            </ul>
        </div>
        <div class="kmp12"><strong>Важно:</strong> В современных системах газеттиры используются не как единственный метод, а как дополнительный признак (feature) для ML-моделей или для постобработки результатов нейросети.</div>
    </div>

    <div class="007">
        <h3 class="007-title">Морфологическая нормализация</h3>
        <div class="007-card">
            <p>Флективные языки (русский, немецкий, финский и др.) представляют особую сложность для NER из-за богатого словоизменения.</p>
            <p><strong>Проблема падежей в русском языке:</strong></p>
        </div>
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr><th>Падеж</th><th>Форма</th><th>Пример в контексте</th></tr>
                </thead>
                <tr><td>Именительный</td><td>Москва</td><td>Москва — столица России</td></tr>
                <tr><td>Родительный</td><td>Москвы</td><td>улицы Москвы</td></tr>
                <tr><td>Дательный</td><td>Москве</td><td>приехал к Москве</td></tr>
                <tr><td>Винительный</td><td>Москву</td><td>посетил Москву</td></tr>
                <tr><td>Творительный</td><td>Москвой</td><td>любуюсь Москвой</td></tr>
                <tr><td>Предложный</td><td>о Москве</td><td>живу в Москве</td></tr>
            </table>
        </div>
        <div class="007-card">
            <p><strong>Методы нормализации:</strong></p>
            <ul>
                <li><strong>Лемматизация</strong> — приведение к начальной форме с учётом части речи («Москвой» → «Москва»)</li>
                <li><strong>Стемминг</strong> — отсечение окончаний по правилам («Москвой» → «Москв»)</li>
                <li><strong>Fuzzy matching</strong> — нечёткое сравнение строк (расстояние Левенштейна)</li>
            </ul>
        </div>
        <div class="kmp14"><strong>Пояснение:</strong> Для классических ML-методов нормализация критически важна. Для нейросетей с контекстуализированными эмбеддингами (BERT) она менее критична, так как модель учитывает контекст. Однако даже для BERT нормализация улучшает работу с редкими формами слов.</div>
    </div>

    <div class="008">
        <h3 class="008-title">Синтаксические правила и контекстные маркеры</h3>
        <div class="008-card">
            <p>Многие сущности можно распознать по характерному контексту — словам-индикаторам, которые часто появляются рядом с сущностями определённого типа.</p>
            <p><strong>Левый контекст (триггеры):</strong></p>
        </div>
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr><th>Тип сущности</th><th>Триггеры слева</th></tr>
                </thead>
                <tr><td>PER</td><td>г-н, г-жа, господин, доктор, профессор, президент, директор</td></tr>
                <tr><td>ORG</td><td>компания, корпорация, банк, ООО, ПАО, АО, фонд, партия</td></tr>
                <tr><td>LOC</td><td>город, село, река, озеро, гора, улица, проспект, страна</td></tr>
                <tr><td>DATE</td><td>в, до, после, начиная с, к, году, месяце</td></tr>
            </table>
        </div>
        <div class="008-card">
            <p><strong>Правый контекст:</strong></p>
            <ul>
                <li>«... <em>Inc.</em>», «... <em>Ltd.</em>», «... <em>GmbH</em>» → ORG</li>
                <li>«... <em>-ович</em>», «... <em>-евна</em>» → PER (отчество)</li>
                <li>«... <em>область</em>», «... <em>край</em>» → LOC</li>
            </ul>
            <p><strong>Паттерны регулярных выражений:</strong></p>
            <ul>
                <li>Даты: <code>\d{1,2}[./]\d{1,2}[./]\d{2,4}</code></li>
                <li>Деньги: <code>\d+[\s]*(руб|долл|евро|\$|€|₽)</code></li>
                <li>Email, URL, телефоны — стандартные регулярки</li>
            </ul>
        </div>
        <div class="kmp11"><strong>Примечание:</strong> Правила особенно эффективны для «регулярных» сущностей (даты, суммы, email). Для имён людей и организаций правила дают высокую точность (precision), но низкую полноту (recall).</div>
    </div>

    <div class="009">
        <h3 class="009-title">Орфографические признаки</h3>
        <div class="009-card">
            <p>Написание слова само по себе несёт информацию о его природе.</p>
            <p><strong>Полезные орфографические признаки:</strong></p>
        </div>
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr><th>Признак</th><th>Паттерн</th><th>Типичный тип</th></tr>
                </thead>
                <tr><td>Начинается с заглавной</td><td>Москва</td><td>PER, ORG, LOC</td></tr>
                <tr><td>Все заглавные</td><td>NASA, МГУ</td><td>ORG (аббревиатура)</td></tr>
                <tr><td>CamelCase</td><td>McDonald's, iPhone</td><td>ORG, PRODUCT</td></tr>
                <tr><td>Содержит цифры</td><td>COVID-19, Boeing 747</td><td>MISC, PRODUCT</td></tr>
                <tr><td>Содержит дефис</td><td>Нью-Йорк, Ростов-на-Дону</td><td>LOC</td></tr>
                <tr><td>В кавычках</td><td>«Газпром», "Apple"</td><td>ORG, WORK_OF_ART</td></tr>
            </table>
        </div>
        <div class="kmp12"><strong>Важно:</strong> Заглавная буква в начале предложения — ложный сигнал. Классические методы используют признак «является ли слово первым в предложении» для корректировки.</div>
    </div>
</section>

<section id="3" class="section">
    <h2 class="section-title">Машинное обучение для NER</h2>
    
    <div class="010">
        <h3 class="010-title">Почему NER — задача sequence labeling</h3>
        <div class="010-card">
            <p>NER — это задача <strong>последовательной разметки (sequence labeling)</strong>: каждому элементу входной последовательности (токену) нужно присвоить метку из заданного набора.</p>
            <p>Ключевая особенность: метки соседних токенов <strong>зависят друг от друга</strong>.</p>
            <p><strong>Примеры зависимостей:</strong></p>
            <ul>
                <li>После <code>B-PER</code> может идти только <code>I-PER</code> или <code>O</code> (не <code>I-LOC</code>!)</li>
                <li>Метка <code>I-ORG</code> не может быть первой в последовательности</li>
                <li>Если текущее слово — отчество (на <em>-ович</em>), предыдущее, скорее всего, имя (PER)</li>
            </ul>
            <p>Это означает, что классификация каждого токена <em>независимо</em> от других (как в обычной классификации) приведёт к ошибкам.</p>
        </div>
    </div>

    <div class="011">
        <h3 class="011-title">Conditional Random Fields (CRF)</h3>
        <div class="011-card">
            <p><strong>CRF (Условные случайные поля)</strong> — вероятностная графическая модель, специально разработанная для задач sequence labeling.</p>
            <p><strong>Идея «на пальцах»:</strong></p>
            <p>Представьте, что вы угадываете слово в кроссворде. Вы учитываете:</p>
            <ol>
                <li>Определение (= признаки текущего токена)</li>
                <li>Уже вписанные буквы от пересекающихся слов (= метки соседних токенов)</li>
            </ol>
            <p>CRF делает то же самое: учитывает и признаки токена, и «совместимость» соседних меток.</p>
            <p><strong>Математически:</strong></p>
            <p>CRF моделирует условную вероятность последовательности меток <em>y</em> при данной последовательности токенов <em>x</em>:</p>
            <p><code>P(y|x) = (1/Z) × exp(Σ λ_i × f_i(y_{t-1}, y_t, x, t))</code></p>
            <p>где <code>f_i</code> — функции признаков, <code>λ_i</code> — обучаемые веса.</p>
        </div>
        <div class="kmp14"><strong>Пояснение:</strong> Z — нормализующая константа. Функции признаков могут учитывать: (1) само слово, (2) метку текущего токена, (3) метку предыдущего токена, (4) любые комбинации.</div>
    </div>

    <div class="012">
        <h3 class="012-title">Инженерия признаков (Feature Engineering)</h3>
        <div class="012-card">
            <p>Качество классического ML напрямую зависит от качества признаков. Это область, где <strong>лингвист незаменим</strong>.</p>
            <p><strong>Категории признаков для NER:</strong></p>
        </div>
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr><th>Категория</th><th>Примеры признаков</th></tr>
                </thead>
                <tr>
                    <td>Лексические</td>
                    <td>Само слово, лемма, нижний регистр, n-граммы символов</td>
                </tr>
                <tr>
                    <td>Орфографические</td>
                    <td>isUpper, isTitle, isDigit, hasHyphen, длина слова</td>
                </tr>
                <tr>
                    <td>Морфологические</td>
                    <td>Часть речи (POS), падеж, число, род; суффикс (-ов, -ский, -ция)</td>
                </tr>
                <tr>
                    <td>Синтаксические</td>
                    <td>Позиция в предложении, зависимости (dependency parsing)</td>
                </tr>
                <tr>
                    <td>Газеттирные</td>
                    <td>isInCityList, isInPersonNames, isKnownOrganization</td>
                </tr>
                <tr>
                    <td>Контекстные</td>
                    <td>Признаки слов в окне ±2 (предыдущее слово, следующее слово)</td>
                </tr>
            </table>
        </div>
        <div class="012-card">
            <p><strong>Пример признакового вектора для слова «Иванова»:</strong></p>
            <ul>
                <li>word=Иванова</li>
                <li>lemma=Иванов</li>
                <li>suffix_3=ова</li>
                <li>isTitle=True</li>
                <li>POS=NOUN</li>
                <li>case=Gen</li>
                <li>prev_word=директора</li>
                <li>inSurnameList=True</li>
            </ul>
        </div>
        <div class="kmp11"><strong>Примечание:</strong> Суффикс <em>-ович/-евич</em> (отчество), <em>-ов/-ев/-ин</em> (фамилия), <em>-ский/-ская</em> (прилагательное → возможно LOC или ORG) — мощные признаки для русского языка.</div>
    </div>

    <div class="013">
        <h3 class="013-title">Библиотеки и инструменты</h3>
        <div class="013-card">
            <p><strong>Популярные инструменты для классического ML в NER:</strong></p>
            <ul>
                <li><strong>CRFsuite</strong> — быстрая реализация CRF на C++</li>
                <li><strong>sklearn-crfsuite</strong> — Python-обёртка для CRFsuite</li>
                <li><strong>NLTK</strong> — содержит инструменты для разметки и обучения</li>
                <li><strong>Natasha</strong> — библиотека для русского языка с правилами и ML</li>
                <li><strong>Томита-парсер</strong> (Яндекс) — правиловый экстрактор на контекстно-свободных грамматиках</li>
            </ul>
        </div>
        <div class="kmp12"><strong>Важно:</strong> Классические методы до сих пор актуальны для: (1) узких доменов с малым количеством данных, (2) задач с высокими требованиями к скорости, (3) случаев, когда нужна интерпретируемость.</div>
    </div>
</section>

<section id="4" class="section">
    <h2 class="section-title">Глубокое обучение для NER</h2>
    
    <div class="014">
        <h3 class="014-title">Эмбеддинги: от Word2Vec к BERT</h3>
        <div class="014-card">
            <p><strong>Эмбеддинг (embedding)</strong> — это представление слова в виде вектора чисел фиксированной размерности. Близкие по смыслу слова имеют близкие векторы.</p>
            <p><strong>Эволюция эмбеддингов:</strong></p>
        </div>
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr><th>Поколение</th><th>Модель</th><th>Особенность</th></tr>
                </thead>
                <tr>
                    <td>1. Статические</td>
                    <td>Word2Vec, GloVe, FastText</td>
                    <td>Одно слово = один вектор (независимо от контекста)</td>
                </tr>
                <tr>
                    <td>2. Контекстуальные (RNN)</td>
                    <td>ELMo</td>
                    <td>Вектор зависит от предложения (LSTM)</td>
                </tr>
                <tr>
                    <td>3. Контекстуальные (Transformer)</td>
                    <td>BERT, RoBERTa, XLM-R</td>
                    <td>Вектор зависит от контекста (механизм внимания)</td>
                </tr>
            </table>
        </div>
        <div class="014-card">
            <p><strong>Пример: слово «банк»</strong></p>
            <ul>
                <li>Word2Vec: один вектор для всех значений</li>
                <li>BERT в контексте «деньги в банке»: вектор близок к «финансы», «кредит»</li>
                <li>BERT в контексте «сидел на банке реки»: вектор близок к «берег», «река»</li>
            </ul>
        </div>
        <div class="kmp14"><strong>Пояснение:</strong> Для NER контекстуализированные эмбеддинги критически важны, так как позволяют различать омонимы: «Пушкин» (поэт) vs «Пушкин» (город) на основе окружающих слов.</div>
    </div>

    <div class="015">
        <h3 class="015-title">Архитектура BiLSTM-CRF</h3>
        <div class="015-card">
            <p>До появления BERT стандартной архитектурой для NER была связка BiLSTM + CRF.</p>
            <p><strong>Компоненты:</strong></p>
            <ol>
                <li><strong>Embedding Layer</strong> — преобразует токены в векторы (Word2Vec/GloVe + character-level CNN)</li>
                <li><strong>BiLSTM</strong> — двунаправленная LSTM читает предложение слева направо и справа налево, создавая контекстуализированное представление каждого токена</li>
                <li><strong>Dense Layer</strong> — преобразует выход LSTM в «оценки» для каждой метки</li>
                <li><strong>CRF Layer</strong> — учитывает зависимости между метками (как в классическом CRF)</li>
            </ol>
        </div>
        <div class="015-card">
            <p><strong>Почему нельзя просто Dense + Softmax?</strong></p>
            <p>Softmax выбирает метку для каждого токена <em>независимо</em>. Без CRF модель может предсказать нелегальную последовательность:</p>
            <p><code>[O, I-PER, B-LOC, I-PER, O]</code> — I-PER не может идти сразу после O!</p>
            <p>CRF-слой учится штрафовать такие переходы.</p>
        </div>
        <div class="kmp11"><strong>Примечание:</strong> Современные BERT-модели для NER часто обходятся без CRF, так как механизм внимания (attention) неявно учитывает зависимости. Но добавление CRF поверх BERT всё ещё даёт небольшой прирост (0.2–0.5 F1).</div>
    </div>

    <div class="016">
        <h3 class="016-title">BERT для NER: тонкая настройка (Fine-tuning)</h3>
        <div class="016-card">
            <p><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong> — предобученная языковая модель, которую можно дообучить для NER.</p>
            <p><strong>Схема использования:</strong></p>
            <ol>
                <li>Берём предобученный BERT (например, для русского: rubert-base-cased, sbert_large_nlu_ru)</li>
                <li>Добавляем классификационную «голову» (linear layer поверх [CLS] или токенов)</li>
                <li>Дообучаем на размеченных NER-данных (несколько эпох, низкий learning rate)</li>
            </ol>
        </div>
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr><th>Модель</th><th>Язык</th><th>F1 на типовых датасетах</th></tr>
                </thead>
                <tr><td>BERT-base-cased</td><td>Английский</td><td>~92% (CoNLL-2003)</td></tr>
                <tr><td>ruBERT</td><td>Русский</td><td>~90% (Collection3)</td></tr>
                <tr><td>XLM-RoBERTa-large</td><td>Мультиязычный</td><td>~93% (CoNLL-2003)</td></tr>
            </table>
        </div>
        <div class="kmp12"><strong>Важно:</strong> Fine-tuning BERT требует GPU и размеченных данных (минимум 5–10 тысяч примеров). Для маленьких датасетов лучше использовать few-shot или transfer learning с доменно-близкой модели.</div>
    </div>

    <div class="017">
        <h3 class="017-title">Проблема токенизации: WordPiece и BPE</h3>
        <div class="017-card">
            <p>BERT и другие трансформеры используют <strong>субсловную токенизацию</strong>: редкие слова разбиваются на части.</p>
            <p><strong>Пример (WordPiece):</strong></p>
            <ul>
                <li>«Петропавловск» → [«Пет», «##роп», «##авл», «##овск»]</li>
                <li>«Достоевского» → [«Достоев», «##ского»]</li>
                <li>«COVID-19» → [«CO», «##VI», «##D», «-», «19»]</li>
            </ul>
            <p><strong>Проблема для NER:</strong></p>
            <p>Метки присваиваются на уровне слов, а модель работает с субтокенами. Нужна стратегия выравнивания.</p>
        </div>
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr><th>Стратегия</th><th>Описание</th></tr>
                </thead>
                <tr>
                    <td>First token</td>
                    <td>Берём предсказание только для первого субтокена слова</td>
                </tr>
                <tr>
                    <td>All tokens (same label)</td>
                    <td>Присваиваем одну метку всем субтокенам слова при обучении</td>
                </tr>
                <tr>
                    <td>Pooling</td>
                    <td>Усредняем/максимизируем эмбеддинги субтокенов перед классификацией</td>
                </tr>
            </table>
        </div>
        <div class="kmp14"><strong>Пояснение:</strong> Стратегия «first token» — самая распространённая. При инференсе предсказания для ##-токенов игнорируются, метка берётся от первого субтокена.</div>
    </div>

    <div class="018">
        <h3 class="018-title">Датасеты и бенчмарки для NER</h3>
        <div class="018-card">
            <p><strong>Ключевые датасеты:</strong></p>
        </div>
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr><th>Датасет</th><th>Язык</th><th>Типы сущностей</th><th>Объём</th></tr>
                </thead>
                <tr><td>CoNLL-2003</td><td>Английский, немецкий</td><td>PER, ORG, LOC, MISC</td><td>~20K предложений</td></tr>
                <tr><td>OntoNotes 5.0</td><td>Английский, китайский, арабский</td><td>18 типов</td><td>~1.7M токенов</td></tr>
                <tr><td>WikiNER</td><td>Мультиязычный</td><td>PER, ORG, LOC</td><td>Разный объём</td></tr>
                <tr><td>Collection3</td><td>Русский</td><td>PER, ORG, LOC</td><td>~1M токенов</td></tr>
                <tr><td>NEREL</td><td>Русский</td><td>29+ типов, вложенные</td><td>~900 документов</td></tr>
                <tr><td>FactRuEval</td><td>Русский</td><td>PER, ORG, LOC + связи</td><td>~250 документов</td></tr>
            </table>
        </div>
        <div class="kmp11"><strong>Примечание:</strong> Для русского языка датасетов существенно меньше, чем для английского. При обучении моделей для русского часто используют transfer learning с мультиязычных моделей (XLM-RoBERTa).</div>
    </div>
</section>

<section id="5" class="section">
    <h2 class="section-title">NER в эпоху больших языковых моделей (LLM)</h2>
    
    <div class="019">
        <h3 class="019-title">Zero-shot и Few-shot NER</h3>
        <div class="019-card">
            <p>Большие языковые модели (GPT-4, Claude, LLaMA) могут извлекать сущности <strong>без специального обучения</strong> — только на основе инструкции в промпте.</p>
            <p><strong>Zero-shot:</strong> Модель не видит примеров — только описание задачи.</p>
            <p><strong>Few-shot:</strong> В промпте даётся несколько примеров (обычно 3–10).</p>
            <p><strong>Пример zero-shot промпта:</strong></p>
        </div>
        <div class="019-card" style="background-color: #f5f5f5; padding: 15px; font-family: monospace;">
            <p>Извлеки из текста все именованные сущности следующих типов:</p>
            <p>- PERSON: имена людей</p>
            <p>- ORG: организации</p>
            <p>- DRUG: названия лекарств</p>
            <p></p>
            <p>Текст: «Врач Иванова назначила пациенту аспирин производства компании Bayer.»</p>
            <p></p>
            <p>Ответ в формате JSON:</p>
        </div>
        <div class="019-card">
            <p><strong>Преимущества:</strong></p>
            <ul>
                <li>Не нужны размеченные данные</li>
                <li>Легко добавлять новые типы сущностей</li>
                <li>Работает «из коробки» для нестандартных доменов</li>
            </ul>
            <p><strong>Недостатки:</strong></p>
            <ul>
                <li>Качество ниже, чем у fine-tuned BERT (на 3–10% F1)</li>
                <li>Высокая стоимость API (для больших объёмов)</li>
                <li>Нестабильность формата вывода</li>
                <li>Галлюцинации — модель может «выдумать» сущности</li>
            </ul>
        </div>
        <div class="kmp12"><strong>Важно:</strong> Zero-shot LLM эффективен для прототипирования и работы с редкими типами сущностей (архаизмы, термины узкой области). Для production с высокими требованиями к качеству рекомендуется fine-tuning.</div>
    </div>

    <div class="020">
        <h3 class="020-title">Few-shot: выбор примеров</h3>
        <div class="020-card">
            <p>Качество few-shot сильно зависит от того, <strong>какие примеры</strong> вы даёте модели.</p>
            <p><strong>Рекомендации по подбору примеров:</strong></p>
            <ul>
                <li><strong>Разнообразие типов</strong> — покажите примеры каждого нужного типа сущностей</li>
                <li><strong>Граничные случаи</strong> — включите сложные примеры (короткие имена, аббревиатуры)</li>
                <li><strong>Негативные примеры</strong> — покажите, что НЕ является сущностью</li>
                <li><strong>Формат вывода</strong> — будьте единообразны в формате ответа</li>
            </ul>
            <p><strong>Пример few-shot промпта:</strong></p>
        </div>
        <div class="020-card" style="background-color: #f5f5f5; padding: 15px; font-family: monospace;">
            <p>Задача: извлечь лекарства (DRUG) и дозировки (DOSE).</p>
            <p></p>
            <p>Пример 1:</p>
            <p>Текст: "Принимать ибупрофен 400 мг два раза в день"</p>
            <p>Ответ: {"DRUG": ["ибупрофен"], "DOSE": ["400 мг"]}</p>
            <p></p>
            <p>Пример 2:</p>
            <p>Текст: "Витамин D не обнаружен"</p>
            <p>Ответ: {"DRUG": ["Витамин D"], "DOSE": []}</p>
            <p></p>
            <p>Теперь обработай:</p>
            <p>Текст: "Назначен курс амоксициллина 500мг и парацетамол при температуре"</p>
        </div>
        <div class="kmp14"><strong>Пояснение:</strong> Второй пример показывает, что отсутствие дозировки — это нормально (пустой список), а не повод выдумывать. Такие «негативные» примеры снижают галлюцинации.</div>
    </div>

    <div class="021">
        <h3 class="021-title">Structured Output: получение чистого JSON/XML</h3>
        <div class="021-card">
            <p>LLM генерируют текст, а для NER нужен структурированный вывод. Это создаёт проблемы:</p>
            <ul>
                <li>Модель может добавить комментарии («Вот результат:...»)</li>
                <li>JSON может быть невалидным (пропущены кавычки, запятые)</li>
                <li>Формат может меняться от запроса к запросу</li>
            </ul>
            <p><strong>Решения:</strong></p>
        </div>
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr><th>Подход</th><th>Инструменты</th><th>Описание</th></tr>
                </thead>
                <tr>
                    <td>JSON Mode (API)</td>
                    <td>OpenAI API, Anthropic API</td>
                    <td>Параметр response_format={"type": "json"}</td>
                </tr>
                <tr>
                    <td>Pydantic-валидация</td>
                    <td>Instructor, Marvin</td>
                    <td>Определяете схему на Pydantic, библиотека валидирует вывод</td>
                </tr>
                <tr>
                    <td>Грамматики (GBNF)</td>
                    <td>llama.cpp, Outlines</td>
                    <td>Ограничивает генерацию токенов по формальной грамматике</td>
                </tr>
                <tr>
                    <td>Retry + парсинг</td>
                    <td>LangChain</td>
                    <td>Если вывод невалиден — перезапрос с уточнением</td>
                </tr>
            </table>
        </div>
        <div class="021-card">
            <p><strong>Пример с библиотекой Instructor (Python):</strong></p>
        </div>
        <div class="021-card" style="background-color: #f5f5f5; padding: 15px; font-family: monospace;">
            <p>from pydantic import BaseModel</p>
            <p>import instructor</p>
            <p>from openai import OpenAI</p>
            <p></p>
            <p>class Entity(BaseModel):</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;text: str</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;type: str</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;start: int</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;end: int</p>
            <p></p>
            <p>class NERResult(BaseModel):</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;entities: list[Entity]</p>
            <p></p>
            <p>client = instructor.from_openai(OpenAI())</p>
            <p>result = client.chat.completions.create(</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;model="gpt-4",</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;response_model=NERResult,</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;messages=[{"role": "user", "content": prompt}]</p>
            <p>)</p>
        </div>
        <div class="kmp11"><strong>Примечание:</strong> Instructor автоматически добавляет JSON-схему в промпт и валидирует ответ. При ошибке парсинга — автоматически переспрашивает модель.</div>
    </div>

    <div class="022">
        <h3 class="022-title">Инженерия промптов: техники повышения качества</h3>
        <div class="022-card">
            <p><strong>Chain-of-Thought (CoT)</strong> — просим модель «думать вслух» перед ответом.</p>
            <p><strong>Пример без CoT:</strong></p>
            <p>«Извлеки сущности из текста: ...» → прямой ответ</p>
            <p><strong>Пример с CoT:</strong></p>
            <p>«Для каждого потенциального кандидата объясни, почему это сущность или нет, затем дай итоговый список.»</p>
        </div>
        <div class="022-card">
            <p><strong>Другие техники:</strong></p>
            <ul>
                <li><strong>Role prompting</strong>: «Ты — эксперт-лингвист, специализирующийся на извлечении сущностей...»</li>
                <li><strong>Step-by-step</strong>: Разбейте задачу на шаги (1. Найди кандидатов, 2. Классифицируй, 3. Проверь границы)</li>
                <li><strong>Self-verification</strong>: «Проверь свой ответ: все ли сущности найдены? Нет ли ложных?»</li>
                <li><strong>Contrastive examples</strong>: Покажите, чем отличаются похожие типы (LOC vs GPE)</li>
            </ul>
        </div>
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr><th>Техника</th><th>Когда использовать</th><th>Прирост качества</th></tr>
                </thead>
                <tr><td>Few-shot (3–5 примеров)</td><td>Всегда, когда есть примеры</td><td>+5–15% F1</td></tr>
                <tr><td>Chain-of-Thought</td><td>Сложные случаи (вложенные, неоднозначные)</td><td>+2–8% F1</td></tr>
                <tr><td>JSON Mode / Instructor</td><td>Когда нужен стабильный формат</td><td>Снижает ошибки парсинга</td></tr>
                <tr><td>Self-verification</td><td>Высокие требования к recall</td><td>+1–3% recall</td></tr>
            </table>
        </div>
        <div class="kmp12"><strong>Важно:</strong> Prompt engineering — это итеративный процесс. Всегда тестируйте на held-out данных и анализируйте ошибки.</div>
    </div>

    <div class="023">
        <h3 class="023-title">Гибридные подходы: LLM + классические методы</h3>
        <div class="023-card">
            <p>Оптимальная стратегия часто сочетает разные подходы.</p>
            <p><strong>Варианты гибридных архитектур:</strong></p>
            <ul>
                <li><strong>LLM для генерации данных</strong>: GPT-4 создаёт синтетические примеры для обучения BERT</li>
                <li><strong>LLM для сложных случаев</strong>: Быстрая BERT-модель обрабатывает большинство текстов, LLM вызывается для низкоуверенных предсказаний</li>
                <li><strong>Постобработка газеттирами</strong>: Нейросеть извлекает кандидатов, газеттир верифицирует и нормализует</li>
                <li><strong>Entity Linking</strong>: NER находит упоминания, LLM связывает с базой знаний (Wikidata)</li>
            </ul>
        </div>
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr><th>Подход</th><th>Скорость</th><th>Качество</th><th>Стоимость</th></tr>
                </thead>
                <tr><td>Правила + газеттиры</td><td>Очень высокая</td><td>Среднее</td><td>Низкая</td></tr>
                <tr><td>Fine-tuned BERT</td><td>Высокая</td><td>Высокое</td><td>Средняя</td></tr>
                <tr><td>Zero-shot LLM</td><td>Низкая</td><td>Среднее</td><td>Высокая</td></tr>
                <tr><td>Few-shot LLM</td><td>Низкая</td><td>Хорошее</td><td>Высокая</td></tr>
                <tr><td>BERT + LLM (гибрид)</td><td>Средняя</td><td>Очень высокое</td><td>Средняя</td></tr>
            </table>
        </div>
        <div class="kmp14"><strong>Пояснение:</strong> Для production-систем с большим потоком данных оптимально: быстрая модель (BERT или правила) для 90% случаев, LLM — для 10% сложных. Это баланс качества и стоимости.</div>
    </div>

    <div class="024">
        <h3 class="024-title">Доменная адаптация и специфические задачи</h3>
        <div class="024-card">
            <p>NER-модели, обученные на новостях, плохо работают на медицинских или юридических текстах. Требуется <strong>доменная адаптация</strong>.</p>
            <p><strong>Стратегии адаптации:</strong></p>
            <ul>
                <li><strong>Domain-specific pretraining</strong>: Дообучаем языковую модель (BERT) на текстах домена без разметки</li>
                <li><strong>Fine-tuning на доменных данных</strong>: Размечаем 500–2000 примеров вручную</li>
                <li><strong>Active Learning</strong>: Модель выбирает примеры для разметки, где она наименее уверена</li>
                <li><strong>Data Augmentation</strong>: Генерируем вариации обучающих примеров (замена синонимов, перефразирование)</li>
            </ul>
        </div>
        <div class="024-card">
            <p><strong>Специализированные модели по доменам:</strong></p>
            <ul>
                <li><strong>Биомедицина</strong>: BioBERT, PubMedBERT, SciBERT</li>
                <li><strong>Юриспруденция</strong>: LegalBERT</li>
                <li><strong>Финансы</strong>: FinBERT</li>
                <li><strong>Русский язык</strong>: RuBERT, RuRoBERTa, SBERT_large_nlu_ru</li>
            </ul>
        </div>
        <div class="kmp11"><strong>Примечание:</strong> Для лингвиста доменная адаптация — это возможность применить экспертизу: создать гайдлайны для разметки, определить специфические типы сущностей, разрешить спорные случаи.</div>
    </div>
</section>

<section id="6" class="section">
    <h2 class="section-title">Практические инструменты и библиотеки</h2>
    
    <div class="025">
        <h3 class="025-title">Обзор основных библиотек</h3>
        <div class="table-kmp">
            <table class="table">
                <thead>
                    <tr><th>Библиотека</th><th>Язык/Платформа</th><th>Особенности</th><th>Поддержка русского</th></tr>
                </thead>
                <tr>
                    <td><strong>spaCy</strong></td>
                    <td>Python</td>
                    <td>Быстрая, production-ready, pipeline-архитектура</td>
                    <td>Модель ru_core_news_lg</td>
                </tr>
                <tr>
                    <td><strong>Stanza</strong></td>
                    <td>Python</td>
                    <td>Академическая, нейросетевая, 70+ языков</td>
                    <td>Да (Stanford NLP)</td>
                </tr>
                <tr>
                    <td><strong>Natasha</strong></td>
                    <td>Python</td>
                    <td>Специально для русского, правила + ML</td>
                    <td>Только русский</td>
                </tr>
                <tr>
                    <td><strong>DeepPavlov</strong></td>
                    <td>Python</td>
                    <td>BERT-модели для русского, диалоговые системы</td>
                    <td>Да</td>
                </tr>
                <tr>
                    <td><strong>Hugging Face Transformers</strong></td>
                    <td>Python</td>
                    <td>Доступ к тысячам моделей, fine-tuning</td>
                    <td>Множество моделей</td>
                </tr>
                <tr>
                    <td><strong>Flair</strong></td>
                    <td>Python</td>
                    <td>Stacking эмбеддингов, сильные NER-модели</td>
                    <td>Ограниченная</td>
                </tr>
            </table>
        </div>
    </div>

    <div class="026">
        <h3 class="026-title">Пример: NER с использованием Natasha (русский язык)</h3>
        <div class="026-card" style="background-color: #f5f5f5; padding: 15px; font-family: monospace;">
            <p>from natasha import (</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;Segmenter, MorphVocab,</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, NewsNERTagger,</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;Doc</p>
            <p>)</p>
            <p></p>
            <p># Инициализация</p>
            <p>segmenter = Segmenter()</p>
            <p>morph_vocab = MorphVocab()</p>
            <p>emb = NewsEmbedding()</p>
            <p>ner_tagger = NewsNERTagger(emb)</p>
            <p></p>
            <p># Обработка текста</p>
            <p>text = "Президент России Владимир Путин встретился с канцлером Германии в Москве."</p>
            <p>doc = Doc(text)</p>
            <p>doc.segment(segmenter)</p>
            <p>doc.tag_ner(ner_tagger)</p>
            <p></p>
            <p># Вывод результатов</p>
            <p>for span in doc.spans:</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;print(f"{span.text} → {span.type}")</p>
            <p></p>
            <p># Результат:</p>
            <p># России → LOC</p>
            <p># Владимир Путин → PER</p>
            <p># Германии → LOC</p>
            <p># Москве → LOC</p>
        </div>
        <div class="kmp14"><strong>Пояснение:</strong> Natasha использует компактные нейросетевые модели (около 50 МБ), оптимизированные для русского языка. Подходит для быстрой обработки больших объёмов текста.</div>
    </div>

    <div class="027">
        <h3 class="027-title">Пример: Fine-tuning BERT с Hugging Face</h3>
        <div class="027-card" style="background-color: #f5f5f5; padding: 15px; font-family: monospace;">
            <p>from transformers import (</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;AutoTokenizer, AutoModelForTokenClassification,</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;TrainingArguments, Trainer</p>
            <p>)</p>
            <p>from datasets import load_dataset</p>
            <p></p>
            <p># Загрузка модели и токенизатора</p>
            <p>model_name = "DeepPavlov/rubert-base-cased"</p>
            <p>tokenizer = AutoTokenizer.from_pretrained(model_name)</p>
            <p>model = AutoModelForTokenClassification.from_pretrained(</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;model_name, num_labels=9  # число меток NER</p>
            <p>)</p>
            <p></p>
            <p># Загрузка датасета</p>
            <p>dataset = load_dataset("conll2003")  # или ваш датасет</p>
            <p></p>
            <p># Токенизация с выравниванием меток</p>
            <p>def tokenize_and_align_labels(examples):</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;tokenized = tokenizer(examples["tokens"], </p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;truncation=True, is_split_into_words=True)</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;# ... логика выравнивания меток ...</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;return tokenized</p>
            <p></p>
            <p># Обучение</p>
            <p>training_args = TrainingArguments(</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;output_dir="./ner-model",</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;num_train_epochs=3,</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;per_device_train_batch_size=16</p>
            <p>)</p>
            <p>trainer = Trainer(model=model, args=training_args, ...)</p>
            <p>trainer.train()</p>
        </div>
        <div class="kmp11"><strong>Примечание:</strong> Полный код fine-tuning занимает около 50–100 строк. Hugging Face предоставляет готовые примеры в документации (раздел Token Classification).</div>
    </div>
</section>

<section id="7" class="section">
    <h2 class="section-title">Заключение и рекомендации</h2>
    
    <div class="028">
        <h3 class="028-title">Выбор подхода: дерево решений</h3>
        <div class="028-card">
            <p><strong>Как выбрать метод NER для вашей задачи:</strong></p>
            <ul>
                <li><strong>Есть много размеченных данных (>5000 примеров)?</strong> → Fine-tuned BERT</li>
                <li><strong>Мало данных, но есть несколько примеров?</strong> → Few-shot LLM или transfer learning</li>
                <li><strong>Нет данных, нужно быстро?</strong> → Zero-shot LLM</li>
                <li><strong>Стандартные типы (PER, ORG, LOC)?</strong> → Готовые модели (spaCy, Natasha)</li>
                <li><strong>Нужна высокая скорость?</strong> → Правила + газеттиры или оптимизированный BERT</li>
                <li><strong>Критична интерпретируемость?</strong> → CRF с явными признаками</li>
            </ul>
        </div>
    </div>

    <div class="029">
        <h3 class="029-title">Роль лингвиста в NER</h3>
        <div class="029-card">
            <p><strong>Что лингвист может сделать лучше инженера:</strong></p>
            <ul>
                <li><strong>Создание гайдлайнов разметки</strong>: Чёткие правила для аннотаторов снижают шум в данных</li>
                <li><strong>Анализ ошибок</strong>: Классификация ошибок модели (пропуски, ложные срабатывания, ошибки типа)</li>
                <li><strong>Разрешение сложных случаев</strong>: Метонимия, вложенные сущности, граничные случаи</li>
                <li><strong>Feature engineering</strong>: Морфологические признаки, контекстные маркеры для языка</li>
                <li><strong>Prompt engineering</strong>: Формулировка инструкций для LLM на естественном языке</li>
                <li><strong>Качественный контроль</strong>: Оценка адекватности результатов, не только метрик</li>
            </ul>
        </div>
        <div class="kmp12"><strong>Важно:</strong> NER — это задача, где лингвистическая экспертиза критически влияет на результат. Даже лучшая модель не сработает, если неправильно определены типы сущностей или созданы противоречивые гайдлайны.</div>
    </div>

    <div class="030">
        <h3 class="030-title">Перспективы развития</h3>
        <div class="030-card">
            <p><strong>Куда движется NER:</strong></p>
            <ul>
                <li><strong>Multimodal NER</strong>: Извлечение сущностей из текста + изображений (OCR документов, мемы)</li>
                <li><strong>Event Extraction</strong>: Не только «кто/что/где», но и «что произошло» (события, отношения)</li>
                <li><strong>Nested & Discontinuous</strong>: Модели, нативно поддерживающие сложные структуры</li>
                <li><strong>Low-resource languages</strong>: NER для языков с минимальными ресурсами (transfer learning)</li>
                <li><strong>Real-time NER</strong>: Потоковая обработка (социальные сети, новости)</li>
                <li><strong>Privacy-preserving NER</strong>: Обезличивание персональных данных (GDPR compliance)</li>
            </ul>
        </div>
        <div class="kmp14"><strong>Пояснение:</strong> Для будущих преподавателей иностранных языков NER — это инструмент автоматического анализа текстов: извлечение имён собственных для создания упражнений, анализ частотности географических названий в корпусе, поиск культурных реалий.</div>
    </div>
</section>
				
	
<footer class="footer">
<div class="container">
<p>© 2026 | kmp | CC BY-NC-SA 4.0<br>
Разработано для студентов БрГУ имени А.С. Пушкина</p>
</div>
</footer>
<div style="position: fixed; bottom: 10px; color: #777777; right: 30px; opacity: 0.3; font-size: 14px;">kmp+</div>
    <script>
        // Функция для плавной прокрутки к разделу
        function scrollToSection(sectionId) {
            const section = document.getElementById(sectionId);
            const menuHeight = document.querySelector('.menu').offsetHeight;
            
            window.scrollTo({
                top: section.offsetTop - menuHeight - 20,
                behavior: 'smooth'
            });
        }

        // Функция для переключения темы
        document.getElementById('themeToggle').addEventListener('click', function() {
            const currentTheme = document.documentElement.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            
            document.documentElement.setAttribute('data-theme', newTheme);
            this.textContent = newTheme === 'dark' ? '🌙' : '☀️';
        });

        // Анимация появления секций при прокрутке
        document.addEventListener('DOMContentLoaded', function() {
            const sections = document.querySelectorAll('.section');
            
            const observerOptions = {
                root: null,
                rootMargin: '0px',
                threshold: 0.1
            };
            
            const observer = new IntersectionObserver(function(entries, observer) {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('fade-in');
                        observer.unobserve(entry.target);
                    }
                });
            }, observerOptions);
            
            sections.forEach(section => {
                section.classList.remove('fade-in');
                observer.observe(section);
            });
        });
    </script>
</body>
</html>